<!DOCTYPE html>
<html  lang="english" >
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <title>Tukekenulia</title>
    <meta name="description" content="在本作业中完成卷积层和池化层，并且通过加入卷积层和池化层实现三层卷积神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="Tukekenulia">
<meta property="og:url" content="http://yoursite.com/2020/07/06/CS231n_Assignment2_Convolutional_Networks/index.html">
<meta property="og:site_name" content="Tukekenulia">
<meta property="og:description" content="在本作业中完成卷积层和池化层，并且通过加入卷积层和池化层实现三层卷积神经网络">
<meta property="og:locale">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/2.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/3.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/4.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/5.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/6.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/7.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/8.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/9.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/10.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/11.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/12.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/13.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/14.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/15.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/16.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/17.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/18.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/19.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/20.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/1.png">
<meta property="article:published_time" content="2020-07-06T07:08:20.000Z">
<meta property="article:modified_time" content="2020-07-06T07:08:20.000Z">
<meta property="article:author" content="Tukeke">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/CS231n_Assignment2_Convolutional_Networks/2.png">

    
    <link rel="icon" href="/images/icon.png" type="image/x-icon">

    
<link rel="stylesheet" href="/css/common.min.css">



    
    
    
    
        <link href="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/css/lightgallery.min.css" rel="stylesheet">
    
    
    
<link rel="stylesheet" href="/css/iconfont.min.css">

    
<meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <header class="header header-fixture">
    <div class="profile-search-wrap flex sm:block">
        
        
        <div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6">
            <a id="avatar" role="link" href="https://github.com/fengkx" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer" >
                <img src="https://www.gravatar.com/avatar/0bc83cb571cd1c50ba6f3e8a78ef1346?s=128" class="rounded-full" alt="avatar">
            </a>
            <h2 id="name" class="hidden lg:block">fengkx</h2>
            <h3 id="title" class="hidden lg:block">Student &amp; Coder</h3>
            
            <small id="location" class="hidden lg:block">
                <i class="iconfont icon-map-icon"></i>
                Wenzhou11, China
            </small>
            
        </div>
        
        
<div class="search flex-1 flex lg:inline-block sm:hidden lg:px-4 lg:mt-2 lg:mb-4 lg:w-full">
    <form id="search-form" class="my-auto flex-1 lg:border lg:border-solid lg:border-gray-200">
        <div class="input-group table bg-gray-100 lg:bg-white w-full">
            <input id="search-input" type="text" placeholder="Search" class="inline-block w-full bg-gray-100 lg:bg-white p-1">
            <span class="table-cell">
                <button name="search tigger button" disabled>
                    <i class="iconfont icon-search m-2"></i>
                </button>
            </span>
        </div>
    </form>
        
<div id="content-json" data-placeholder="Search" class="invisible hidden">/content.json</div>
<script id="search-teamplate" type="text/html" data-path="/content.json">
    <div>
        <div class="search-header bg-gray-400">
            <input id="actual-search-input" model="keyword" ref="input" class="inline-block w-full h-10 px-2 py-1" placeholder="Search" type="text">
        </div>
        <div class="search-result bg-gray-200">
            {{#each searchPosts}}
            <a href="/{{ path }}" class="result-item block px-2 pb-3 mb-1 pt-1 hover:bg-indigo-100">
                <i class="iconfont icon-file"></i>
                <h1 class="result-title inline font-medium text-lg">{{ title }}</h1>
                <p class="result-content text-gray-600 text-sm">{{{ text }}}</p>
            </a>
            {{/each}}
        </div>
    </div>
</script>

</div>


        <button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false">
            <i class="iconfont icon-hamburger"></i>
        </button>
    </div>
    <nav id="menu-nav" class="hidden sm:flex flex-col">
        
        
            <div class="menu-item menu-home" role="menuitem">
                <a href="/.">
                    <i class="iconfont icon-home" aria-hidden="true"></i>
                    <span class="menu-title">Home</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-archives" role="menuitem">
                <a href="/archives">
                    <i class="iconfont icon-archive" aria-hidden="true"></i>
                    <span class="menu-title">Archives</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-categories" role="menuitem">
                <a href="/categories">
                    <i class="iconfont icon-folder" aria-hidden="true"></i>
                    <span class="menu-title">Categories</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-tags" role="menuitem">
                <a href="/tags">
                    <i class="iconfont icon-tag" aria-hidden="true"></i>
                    <span class="menu-title">Tags</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-repository" role="menuitem">
                <a href="/repository">
                    <i class="iconfont icon-project" aria-hidden="true"></i>
                    <span class="menu-title">Repository</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-links" role="menuitem">
                <a href="/links">
                    <i class="iconfont icon-friend" aria-hidden="true"></i>
                    <span class="menu-title">Links</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-about" role="menuitem">
                <a href="/about">
                    <i class="iconfont icon-cup" aria-hidden="true"></i>
                    <span class="menu-title">About</span>
                </a>
            </div>
        
        
<div class="social-links flex sm:flex-col lg:hidden mt-5">
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://github.com/fengkx">
                <i class="iconfont social-icon icon-github"></i>
                <span class="menu-title hidden lg:inline">menu.github</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://t.me/fengkx">
                <i class="iconfont social-icon icon-telegram"></i>
                <span class="menu-title hidden lg:inline">menu.telegram</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://twitter.com/example">
                <i class="iconfont social-icon icon-twitter"></i>
                <span class="menu-title hidden lg:inline">menu.twitter</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a href="/atom.xml">
                <i class="iconfont social-icon icon-rss"></i>
                <span class="menu-title hidden lg:inline">menu.rss</span>
            </a>
        </span>
    
</div>


    </nav>
</header>

        <section class="main-section">
            
    <main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen">
    

    <article class="content article article-archives article-type-list" itemscope="">
        <header class="article-header">
            


            <p class="article-meta mb-3 text-xs">
                <span class="article-date">
    <i class="iconfont icon-calendar-check"></i>
	<a href="/2020/07/06/CS231n_Assignment2_Convolutional_Networks/" class="article-date">
	  <time datetime="2020-07-06T07:08:20.000Z" itemprop="datePublished">Jul 6</time>
	</a>
</span>

                

                

                <span class="_partial/post-comment"><i class="icon icon-comment"></i>
                    <a href="/2020/07/06/CS231n_Assignment2_Convolutional_Networks/#comments" class="article-comment-link">
                        Comments
                    </a>
                </span>
                

            </p>
        </header>
        <div class="marked-body article-body">
            <p>在本作业中完成卷积层和池化层，并且通过加入卷积层和池化层实现三层卷积神经网络</p>
<span id="more"></span>

<p>使用全连接层网络可以测试不同的优化策略和网络架构，因为计算的效率很高，但是所有最先进的结果都是使用卷积神经网络</p>
<h1 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># As usual, a bit of setup</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from cs231n.classifiers.cnn import *</span><br><span class="line">from cs231n.data_utils import get_CIFAR10_data</span><br><span class="line">from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient</span><br><span class="line">from cs231n.layers import *</span><br><span class="line">from cs231n.fast_layers import *</span><br><span class="line">from cs231n.solver import Solver</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[&#x27;figure.figsize&#x27;] = (10.0, 8.0) # set default size of plots</span><br><span class="line">plt.rcParams[&#x27;image.interpolation&#x27;] = &#x27;nearest&#x27;</span><br><span class="line">plt.rcParams[&#x27;image.cmap&#x27;] = &#x27;gray&#x27;</span><br><span class="line"></span><br><span class="line"># for auto-reloading external modules</span><br><span class="line"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload 2</span><br><span class="line"></span><br><span class="line">def rel_error(x, y):  //相对误差</span><br><span class="line">  &quot;&quot;&quot; returns relative error &quot;&quot;&quot;</span><br><span class="line">  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Load the (preprocessed) CIFAR10 data.</span><br><span class="line"></span><br><span class="line">data = get_CIFAR10_data()</span><br><span class="line">for k, v in data.items():</span><br><span class="line">  print(&#x27;%s: &#x27; % k, v.shape)</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/2.png" alt="2"></p>
<h3 id="Convolution-Naive-forward-pass"><a href="#Convolution-Naive-forward-pass" class="headerlink" title="Convolution: Naive forward pass"></a>Convolution: Naive forward pass</h3><p>朴素的正向卷积传播，在layers.py完成后并应用，并检测误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x_shape = (2, 3, 4, 4)</span><br><span class="line">w_shape = (3, 3, 4, 4)</span><br><span class="line">x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)</span><br><span class="line">w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)</span><br><span class="line">b = np.linspace(-0.1, 0.2, num=3)</span><br><span class="line"></span><br><span class="line">conv_param = &#123;&#x27;stride&#x27;: 2, &#x27;pad&#x27;: 1&#125;</span><br><span class="line">out, _ = conv_forward_naive(x, w, b, conv_param)</span><br><span class="line">correct_out = np.array([[[[-0.08759809, -0.10987781],</span><br><span class="line">                           [-0.18387192, -0.2109216 ]],</span><br><span class="line">                          [[ 0.21027089,  0.21661097],</span><br><span class="line">                           [ 0.22847626,  0.23004637]],</span><br><span class="line">                          [[ 0.50813986,  0.54309974],</span><br><span class="line">                           [ 0.64082444,  0.67101435]]],</span><br><span class="line">                         [[[-0.98053589, -1.03143541],</span><br><span class="line">                           [-1.19128892, -1.24695841]],</span><br><span class="line">                          [[ 0.69108355,  0.66880383],</span><br><span class="line">                           [ 0.59480972,  0.56776003]],</span><br><span class="line">                          [[ 2.36270298,  2.36904306],</span><br><span class="line">                           [ 2.38090835,  2.38247847]]]])</span><br><span class="line"></span><br><span class="line"># Compare your output to ours; difference should be around 2e-8</span><br><span class="line">print(&#x27;Testing conv_forward_naive&#x27;)</span><br><span class="line">print(&#x27;difference: &#x27;, rel_error(out, correct_out))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/3.png" alt="3"></p>
<h3 id="Aside-Image-processing-via-convolutions"><a href="#Aside-Image-processing-via-convolutions" class="headerlink" title="Aside: Image processing via convolutions"></a>Aside: Image processing via convolutions</h3><p>用不同的卷积核：灰度卷积核以及边缘检测卷积核来处理两张图片，最后显示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">from scipy.misc import imread, imresize</span><br><span class="line"></span><br><span class="line">kitten, puppy = imread(&#x27;kitten.jpg&#x27;), imread(&#x27;puppy.jpg&#x27;)</span><br><span class="line"># kitten is wide, and puppy is already square</span><br><span class="line">d = kitten.shape[1] - kitten.shape[0]</span><br><span class="line">kitten_cropped = kitten[:, d//2:-d//2, :]</span><br><span class="line"></span><br><span class="line">img_size = 200   # Make this smaller if it runs too slow</span><br><span class="line">x = np.zeros((2, 3, img_size, img_size))</span><br><span class="line">x[0, :, :, :] = imresize(puppy, (img_size, img_size)).transpose((2, 0, 1))</span><br><span class="line">x[1, :, :, :] = imresize(kitten_cropped, (img_size, img_size)).transpose((2, 0, 1))</span><br><span class="line"></span><br><span class="line"># Set up a convolutional weights holding 2 filters, each 3x3</span><br><span class="line">w = np.zeros((2, 3, 3, 3))</span><br><span class="line"></span><br><span class="line"># The first filter converts the image to grayscale.</span><br><span class="line"># Set up the red, green, and blue channels of the filter.</span><br><span class="line">w[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]</span><br><span class="line">w[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]</span><br><span class="line">w[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]</span><br><span class="line"></span><br><span class="line"># Second filter detects horizontal edges in the blue channel.</span><br><span class="line">w[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]</span><br><span class="line"></span><br><span class="line"># Vector of biases. We don&#x27;t need any bias for the grayscale</span><br><span class="line"># filter, but for the edge detection filter we want to add 128</span><br><span class="line"># to each output so that nothing is negative.</span><br><span class="line">b = np.array([0, 128])</span><br><span class="line"></span><br><span class="line"># Compute the result of convolving each input in x with each filter in w,</span><br><span class="line"># offsetting by b, and storing the results in out.</span><br><span class="line">out, _ = conv_forward_naive(x, w, b, &#123;&#x27;stride&#x27;: 1, &#x27;pad&#x27;: 1&#125;)</span><br><span class="line"></span><br><span class="line">def imshow_noax(img, normalize=True):</span><br><span class="line">    &quot;&quot;&quot; Tiny helper to show images as uint8 and remove axis labels &quot;&quot;&quot;</span><br><span class="line">    if normalize:</span><br><span class="line">        img_max, img_min = np.max(img), np.min(img)</span><br><span class="line">        img = 255.0 * (img - img_min) / (img_max - img_min)</span><br><span class="line">    plt.imshow(img.astype(&#x27;uint8&#x27;))</span><br><span class="line">    plt.gca().axis(&#x27;off&#x27;)</span><br><span class="line"></span><br><span class="line"># Show the original images and the results of the conv operation</span><br><span class="line">plt.subplot(2, 3, 1)</span><br><span class="line">imshow_noax(puppy, normalize=False)</span><br><span class="line">plt.title(&#x27;Original image&#x27;)</span><br><span class="line">plt.subplot(2, 3, 2)</span><br><span class="line">imshow_noax(out[0, 0])</span><br><span class="line">plt.title(&#x27;Grayscale&#x27;)</span><br><span class="line">plt.subplot(2, 3, 3)</span><br><span class="line">imshow_noax(out[0, 1])</span><br><span class="line">plt.title(&#x27;Edges&#x27;)</span><br><span class="line">plt.subplot(2, 3, 4)</span><br><span class="line">imshow_noax(kitten_cropped, normalize=False)</span><br><span class="line">plt.subplot(2, 3, 5)</span><br><span class="line">imshow_noax(out[1, 0])</span><br><span class="line">plt.subplot(2, 3, 6)</span><br><span class="line">imshow_noax(out[1, 1])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/4.png" alt="4"></p>
<h3 id="Convolution-Naive-backward-pass"><a href="#Convolution-Naive-backward-pass" class="headerlink" title="Convolution: Naive backward pass"></a>Convolution: Naive backward pass</h3><p>朴素的反向卷积传播，在layers.py完成后并应用，并检测误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(4, 3, 5, 5)</span><br><span class="line">w = np.random.randn(2, 3, 3, 3)</span><br><span class="line">b = np.random.randn(2,)</span><br><span class="line">dout = np.random.randn(4, 2, 5, 5)</span><br><span class="line">conv_param = &#123;&#x27;stride&#x27;: 1, &#x27;pad&#x27;: 1&#125;</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)</span><br><span class="line">dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)</span><br><span class="line"></span><br><span class="line">out, cache = conv_forward_naive(x, w, b, conv_param)</span><br><span class="line">dx, dw, db = conv_backward_naive(dout, cache)</span><br><span class="line"></span><br><span class="line"># Your errors should be around 1e-8&#x27;</span><br><span class="line">print(&#x27;Testing conv_backward_naive function&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx, dx_num))</span><br><span class="line">print(&#x27;dw error: &#x27;, rel_error(dw, dw_num))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db, db_num))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/5.png" alt="5"></p>
<h3 id="Max-pooling-Naive-forward"><a href="#Max-pooling-Naive-forward" class="headerlink" title="Max pooling: Naive forward"></a>Max pooling: Naive forward</h3><p>在layers.py中完成正向传播的最大池化函数，并测试误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x_shape = (2, 3, 4, 4)</span><br><span class="line">x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)</span><br><span class="line">pool_param = &#123;&#x27;pool_width&#x27;: 2, &#x27;pool_height&#x27;: 2, &#x27;stride&#x27;: 2&#125;</span><br><span class="line"></span><br><span class="line">out, _ = max_pool_forward_naive(x, pool_param)</span><br><span class="line"></span><br><span class="line">correct_out = np.array([[[[-0.26315789, -0.24842105],</span><br><span class="line">                          [-0.20421053, -0.18947368]],</span><br><span class="line">                         [[-0.14526316, -0.13052632],</span><br><span class="line">                          [-0.08631579, -0.07157895]],</span><br><span class="line">                         [[-0.02736842, -0.01263158],</span><br><span class="line">                          [ 0.03157895,  0.04631579]]],</span><br><span class="line">                        [[[ 0.09052632,  0.10526316],</span><br><span class="line">                          [ 0.14947368,  0.16421053]],</span><br><span class="line">                         [[ 0.20842105,  0.22315789],</span><br><span class="line">                          [ 0.26736842,  0.28210526]],</span><br><span class="line">                         [[ 0.32631579,  0.34105263],</span><br><span class="line">                          [ 0.38526316,  0.4       ]]]])</span><br><span class="line"></span><br><span class="line"># Compare your output with ours. Difference should be around 1e-8.</span><br><span class="line">print(&#x27;Testing max_pool_forward_naive function:&#x27;)</span><br><span class="line">print(&#x27;difference: &#x27;, rel_error(out, correct_out))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/6.png" alt="6"></p>
<h2 id="Max-pooling-Naive-backward"><a href="#Max-pooling-Naive-backward" class="headerlink" title="Max pooling: Naive backward"></a>Max pooling: Naive backward</h2><p>在layers.py中完成反向传播的最大池化函数，并测试误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(3, 2, 8, 8)</span><br><span class="line">dout = np.random.randn(3, 2, 4, 4)</span><br><span class="line">pool_param = &#123;&#x27;pool_height&#x27;: 2, &#x27;pool_width&#x27;: 2, &#x27;stride&#x27;: 2&#125;</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)</span><br><span class="line"></span><br><span class="line">out, cache = max_pool_forward_naive(x, pool_param)</span><br><span class="line">dx = max_pool_backward_naive(dout, cache)</span><br><span class="line"></span><br><span class="line"># Your error should be around 1e-12</span><br><span class="line">print(&#x27;Testing max_pool_backward_naive function:&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx, dx_num))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/7.png" alt="7"></p>
<h3 id="Fast-layers"><a href="#Fast-layers" class="headerlink" title="Fast layers"></a>Fast layers</h3><p>在fast_layers.py中已经实现了卷积和池化层更快的实现方案，因为需要Cython扩展包，所以需要在cs231n的目录的命令行方式下执行:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build_ext --inplace</span><br></pre></td></tr></table></figure>

<p>以下代码段是测试朴素的正反向卷积，池化传播和加速版的性能，结果表明性能是原来的10倍。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.fast_layers import conv_forward_fast, conv_backward_fast</span><br><span class="line">from time import time</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(100, 3, 31, 31)</span><br><span class="line">w = np.random.randn(25, 3, 3, 3)</span><br><span class="line">b = np.random.randn(25,)</span><br><span class="line">dout = np.random.randn(100, 25, 16, 16)</span><br><span class="line">conv_param = &#123;&#x27;stride&#x27;: 2, &#x27;pad&#x27;: 1&#125;</span><br><span class="line"></span><br><span class="line">t0 = time()</span><br><span class="line">out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)</span><br><span class="line">t1 = time()</span><br><span class="line">out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing conv_forward_fast:&#x27;)</span><br><span class="line">print(&#x27;Naive: %fs&#x27; % (t1 - t0))</span><br><span class="line">print(&#x27;Fast: %fs&#x27; % (t2 - t1))</span><br><span class="line">print(&#x27;Speedup: %fx&#x27; % ((t1 - t0) / (t2 - t1)))</span><br><span class="line">print(&#x27;Difference: &#x27;, rel_error(out_naive, out_fast))</span><br><span class="line"></span><br><span class="line">t0 = time()</span><br><span class="line">dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)</span><br><span class="line">t1 = time()</span><br><span class="line">dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line">print(&#x27;\nTesting conv_backward_fast:&#x27;)</span><br><span class="line">print(&#x27;Naive: %fs&#x27; % (t1 - t0))</span><br><span class="line">print(&#x27;Fast: %fs&#x27; % (t2 - t1))</span><br><span class="line">print(&#x27;Speedup: %fx&#x27; % ((t1 - t0) / (t2 - t1)))</span><br><span class="line">print(&#x27;dx difference: &#x27;, rel_error(dx_naive, dx_fast))</span><br><span class="line">print(&#x27;dw difference: &#x27;, rel_error(dw_naive, dw_fast))</span><br><span class="line">print(&#x27;db difference: &#x27;, rel_error(db_naive, db_fast))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/8.png" alt="8"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.fast_layers import max_pool_forward_fast, max_pool_backward_fast</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(100, 3, 32, 32)</span><br><span class="line">dout = np.random.randn(100, 3, 16, 16)</span><br><span class="line">pool_param = &#123;&#x27;pool_height&#x27;: 2, &#x27;pool_width&#x27;: 2, &#x27;stride&#x27;: 2&#125;</span><br><span class="line"></span><br><span class="line">t0 = time()</span><br><span class="line">out_naive, cache_naive = max_pool_forward_naive(x, pool_param)</span><br><span class="line">t1 = time()</span><br><span class="line">out_fast, cache_fast = max_pool_forward_fast(x, pool_param)</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing pool_forward_fast:&#x27;)</span><br><span class="line">print(&#x27;Naive: %fs&#x27; % (t1 - t0))</span><br><span class="line">print(&#x27;fast: %fs&#x27; % (t2 - t1))</span><br><span class="line">print(&#x27;speedup: %fx&#x27; % ((t1 - t0) / (t2 - t1)))</span><br><span class="line">print(&#x27;difference: &#x27;, rel_error(out_naive, out_fast))</span><br><span class="line"></span><br><span class="line">t0 = time()</span><br><span class="line">dx_naive = max_pool_backward_naive(dout, cache_naive)</span><br><span class="line">t1 = time()</span><br><span class="line">dx_fast = max_pool_backward_fast(dout, cache_fast)</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line">print(&#x27;\nTesting pool_backward_fast:&#x27;)</span><br><span class="line">print(&#x27;Naive: %fs&#x27; % (t1 - t0))</span><br><span class="line">print(&#x27;speedup: %fx&#x27; % ((t1 - t0) / (t2 - t1)))</span><br><span class="line">print(&#x27;dx difference: &#x27;, rel_error(dx_naive, dx_fast))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/9.png" alt="9"></p>
<h3 id="Convolutional-“sandwich”-layers"><a href="#Convolutional-“sandwich”-layers" class="headerlink" title="Convolutional “sandwich” layers"></a>Convolutional “sandwich” layers</h3><p>本段代码主要就是在layer_utils.py中已经实现了加入卷积层和池化层的混合神经网络，也就是这里的卷积“三明治”层。然后测试加入卷积层和池化层的混合神经网络的误差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.layer_utils import conv_relu_pool_forward, conv_relu_pool_backward</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(2, 3, 16, 16)</span><br><span class="line">w = np.random.randn(3, 3, 3, 3)</span><br><span class="line">b = np.random.randn(3,)</span><br><span class="line">dout = np.random.randn(2, 3, 8, 8)</span><br><span class="line">conv_param = &#123;&#x27;stride&#x27;: 1, &#x27;pad&#x27;: 1&#125;</span><br><span class="line">pool_param = &#123;&#x27;pool_height&#x27;: 2, &#x27;pool_width&#x27;: 2, &#x27;stride&#x27;: 2&#125;</span><br><span class="line"></span><br><span class="line">out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)</span><br><span class="line">dx, dw, db = conv_relu_pool_backward(dout, cache)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)</span><br><span class="line">dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing conv_relu_pool&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dw error: &#x27;, rel_error(dw_num, dw))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/10.png" alt="10"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.layer_utils import conv_relu_forward, conv_relu_backward</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(2, 3, 8, 8)</span><br><span class="line">w = np.random.randn(3, 3, 3, 3)</span><br><span class="line">b = np.random.randn(3,)</span><br><span class="line">dout = np.random.randn(2, 3, 8, 8)</span><br><span class="line">conv_param = &#123;&#x27;stride&#x27;: 1, &#x27;pad&#x27;: 1&#125;</span><br><span class="line"></span><br><span class="line">out, cache = conv_relu_forward(x, w, b, conv_param)</span><br><span class="line">dx, dw, db = conv_relu_backward(dout, cache)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)</span><br><span class="line">dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing conv_relu:&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dw error: &#x27;, rel_error(dw_num, dw))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/11.png" alt="11"></p>
<h3 id="Three-layer-ConvNet"><a href="#Three-layer-ConvNet" class="headerlink" title="Three-layer ConvNet"></a>Three-layer ConvNet</h3><p>本段代码需要在cnn.py中编写代码，具体在下面的函数中，并且在这里检验误差</p>
<h3 id="Sanity-check-loss"><a href="#Sanity-check-loss" class="headerlink" title="Sanity check loss"></a>Sanity check loss</h3><p>检验损失的误差，加入正则化reg后，损失会上升。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = ThreeLayerConvNet()</span><br><span class="line"></span><br><span class="line">N = 50</span><br><span class="line">X = np.random.randn(N, 3, 32, 32)</span><br><span class="line">y = np.random.randint(10, size=N)</span><br><span class="line"></span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">print(&#x27;Initial loss (no regularization): &#x27;, loss)</span><br><span class="line"></span><br><span class="line">model.reg = 0.5</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">print(&#x27;Initial loss (with regularization): &#x27;, loss)</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/12.png" alt="12"></p>
<h3 id="Gradient-check"><a href="#Gradient-check" class="headerlink" title="Gradient check"></a>Gradient check</h3><p>检验dw，db，dx即梯度的误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = 2</span><br><span class="line">input_dim = (3, 16, 16)</span><br><span class="line">reg = 0.0</span><br><span class="line">num_classes = 10</span><br><span class="line">np.random.seed(231)</span><br><span class="line">X = np.random.randn(num_inputs, *input_dim)</span><br><span class="line">y = np.random.randint(num_classes, size=num_inputs)</span><br><span class="line"></span><br><span class="line">model = ThreeLayerConvNet(num_filters=3, filter_size=3,</span><br><span class="line">                          input_dim=input_dim, hidden_dim=7,</span><br><span class="line">                          dtype=np.float64)</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">for param_name in sorted(grads):</span><br><span class="line">    f = lambda _: model.loss(X, y)[0]</span><br><span class="line">    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)</span><br><span class="line">    e = rel_error(param_grad_num, grads[param_name])</span><br><span class="line">    print(&#x27;%s max relative error: %e&#x27; % (param_name, rel_error(param_grad_num, grads[param_name])))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/13.png" alt="13"></p>
<h3 id="Overfit-small-data"><a href="#Overfit-small-data" class="headerlink" title="Overfit small data"></a>Overfit small data</h3><p>在小的数据上训练模型，会过拟合小数据集，因此在训练集上的准确率会比较高，在验证集上的准确率较低，并且可视化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为什么会在训练集上的准确率越来越高，而测试集上的表现差强人意呢？</span><br><span class="line">因为在训练的时候，学习的都是训练集中的特征，测试集中可能会出现训练集中没有识别的特征，从而导致识别不了，导致测试集的准确率很低；而训练集通过训练，训练集的特征都会记下，从而准确率会很高</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">num_train = 100</span><br><span class="line">small_data = &#123;</span><br><span class="line">  &#x27;X_train&#x27;: data[&#x27;X_train&#x27;][:num_train],</span><br><span class="line">  &#x27;y_train&#x27;: data[&#x27;y_train&#x27;][:num_train],</span><br><span class="line">  &#x27;X_val&#x27;: data[&#x27;X_val&#x27;],</span><br><span class="line">  &#x27;y_val&#x27;: data[&#x27;y_val&#x27;],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = ThreeLayerConvNet(weight_scale=1e-2)</span><br><span class="line"></span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                num_epochs=15, batch_size=50,</span><br><span class="line">                update_rule=&#x27;adam&#x27;,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=True, print_every=1)</span><br><span class="line">solver.train()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/14.png" alt="14"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(2, 1, 1)</span><br><span class="line">plt.plot(solver.loss_history, &#x27;o&#x27;)</span><br><span class="line">plt.xlabel(&#x27;iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;loss&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(2, 1, 2)</span><br><span class="line">plt.plot(solver.train_acc_history, &#x27;-o&#x27;)</span><br><span class="line">plt.plot(solver.val_acc_history, &#x27;-o&#x27;)</span><br><span class="line">plt.legend([&#x27;train&#x27;, &#x27;val&#x27;], loc=&#x27;upper left&#x27;)</span><br><span class="line">plt.xlabel(&#x27;epoch&#x27;)</span><br><span class="line">plt.ylabel(&#x27;accuracy&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/15.png" alt="15"></p>
<h3 id="Train-the-net"><a href="#Train-the-net" class="headerlink" title="Train the net"></a>Train the net</h3><p>训练一个3层卷积神经网络，预期在训练集上有40%+的准确率</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)</span><br><span class="line"></span><br><span class="line">solver = Solver(model, data,</span><br><span class="line">                num_epochs=1, batch_size=50,</span><br><span class="line">                update_rule=&#x27;adam&#x27;,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=True, print_every=20)</span><br><span class="line">solver.train()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/16.png" alt="16"></p>
<h3 id="Visualize-Filters"><a href="#Visualize-Filters" class="headerlink" title="Visualize Filters"></a>Visualize Filters</h3><p>通过以下代码可以将第一层卷积神经网络中的卷积核可视化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.vis_utils import visualize_grid</span><br><span class="line"></span><br><span class="line">grid = visualize_grid(model.params[&#x27;W1&#x27;].transpose(0, 2, 3, 1))</span><br><span class="line">plt.imshow(grid.astype(&#x27;uint8&#x27;))  //显示</span><br><span class="line">plt.axis(&#x27;off&#x27;)         //将XY轴坐标值隐藏</span><br><span class="line">plt.gcf().set_size_inches(5, 5) //显示窗口的大小</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/17.png" alt="17"></p>
<h3 id="Spatial-Batch-Normalization"><a href="#Spatial-Batch-Normalization" class="headerlink" title="Spatial Batch Normalization"></a>Spatial Batch Normalization</h3><p>提示如下：</p>
<p>我们已经看到批处理标准化对于训练深度全连接网络是一种非常有用的技术。批处理归一化也可以用于卷积网络，但我们需要稍作调整;修改将称为“空间批处理规范化”。</p>
<p>通常batch-normalization接受输入的形状(N, D)和产生输出的形状(N, D),我们整个minibatch规范化维度数据来自卷积N层,批规范化需要接受输入的形状(N、C, H, W)和产生输出的形状(N、C, H, W) N维度给出了minibatch大小和(H, W)尺寸给的空间大小特征映射。</p>
<p>如果使用卷积生成feature map，那么我们希望每个feature channel的统计数据在不同图像之间以及同一图像中的不同位置都是相对一致的。因此，空间批处理归一化通过计算小批处理维度N和空间维度H和W的统计量来计算每个C特征通道的均值和方差。</p>
<p>因此也就是说对于拥有C个通道的图像而言，计算每个图像（即N个）图像的H,W的平均值，然后每个通道减去平均值除以方差即可。</p>
<h3 id="Spatial-batch-normalization-forward"><a href="#Spatial-batch-normalization-forward" class="headerlink" title="Spatial batch normalization: forward"></a>Spatial batch normalization: forward</h3><p>需要在layers.py中实现spatial_batchnorm_forward，并在以下代码中检验误差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"># Check the training-time forward pass by checking means and variances</span><br><span class="line"># of features both before and after spatial batch normalization</span><br><span class="line"></span><br><span class="line">N, C, H, W = 2, 3, 4, 5</span><br><span class="line">x = 4 * np.random.randn(N, C, H, W) + 10</span><br><span class="line"></span><br><span class="line">print(&#x27;Before spatial batch normalization:&#x27;)</span><br><span class="line">print(&#x27;  Shape: &#x27;, x.shape)</span><br><span class="line">print(&#x27;  Means: &#x27;, x.mean(axis=(0, 2, 3)))</span><br><span class="line">print(&#x27;  Stds: &#x27;, x.std(axis=(0, 2, 3)))</span><br><span class="line"></span><br><span class="line"># Means should be close to zero and stds close to one</span><br><span class="line">gamma, beta = np.ones(C), np.zeros(C)</span><br><span class="line">bn_param = &#123;&#x27;mode&#x27;: &#x27;train&#x27;&#125;</span><br><span class="line">out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)</span><br><span class="line">print(&#x27;After spatial batch normalization:&#x27;)</span><br><span class="line">print(&#x27;  Shape: &#x27;, out.shape)</span><br><span class="line">print(&#x27;  Means: &#x27;, out.mean(axis=(0, 2, 3)))</span><br><span class="line">print(&#x27;  Stds: &#x27;, out.std(axis=(0, 2, 3)))</span><br><span class="line"></span><br><span class="line"># Means should be close to beta and stds close to gamma</span><br><span class="line">gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])</span><br><span class="line">out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)</span><br><span class="line">print(&#x27;After spatial batch normalization (nontrivial gamma, beta):&#x27;)</span><br><span class="line">print(&#x27;  Shape: &#x27;, out.shape)</span><br><span class="line">print(&#x27;  Means: &#x27;, out.mean(axis=(0, 2, 3)))</span><br><span class="line">print(&#x27;  Stds: &#x27;, out.std(axis=(0, 2, 3)))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/18.png" alt="18"></p>
<p>通过在训练集上训练之后获得运行时的平均值和方差，最后在测试集上测试平均值和方差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"># Check the test-time forward pass by running the training-time</span><br><span class="line"># forward pass many times to warm up the running averages, and then</span><br><span class="line"># checking the means and variances of activations after a test-time</span><br><span class="line"># forward pass.</span><br><span class="line">N, C, H, W = 10, 4, 11, 12</span><br><span class="line"></span><br><span class="line">bn_param = &#123;&#x27;mode&#x27;: &#x27;train&#x27;&#125;</span><br><span class="line">gamma = np.ones(C)</span><br><span class="line">beta = np.zeros(C)</span><br><span class="line">for t in range(50):</span><br><span class="line">  x = 2.3 * np.random.randn(N, C, H, W) + 13</span><br><span class="line">  spatial_batchnorm_forward(x, gamma, beta, bn_param)</span><br><span class="line">bn_param[&#x27;mode&#x27;] = &#x27;test&#x27;</span><br><span class="line">x = 2.3 * np.random.randn(N, C, H, W) + 13</span><br><span class="line">a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)</span><br><span class="line"></span><br><span class="line"># Means should be close to zero and stds close to one, but will be</span><br><span class="line"># noisier than training-time forward passes.</span><br><span class="line">print(&#x27;After spatial batch normalization (test-time):&#x27;)</span><br><span class="line">print(&#x27;  means: &#x27;, a_norm.mean(axis=(0, 2, 3)))</span><br><span class="line">print(&#x27;  stds: &#x27;, a_norm.std(axis=(0, 2, 3)))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/19.png" alt="19"></p>
<h3 id="Spatial-batch-normalization-backward"><a href="#Spatial-batch-normalization-backward" class="headerlink" title="Spatial batch normalization: backward"></a>Spatial batch normalization: backward</h3><p>完成之后测试梯度的误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">N, C, H, W = 2, 3, 4, 5</span><br><span class="line">x = 5 * np.random.randn(N, C, H, W) + 12</span><br><span class="line">gamma = np.random.randn(C)</span><br><span class="line">beta = np.random.randn(C)</span><br><span class="line">dout = np.random.randn(N, C, H, W)</span><br><span class="line"></span><br><span class="line">bn_param = &#123;&#x27;mode&#x27;: &#x27;train&#x27;&#125;</span><br><span class="line">fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]</span><br><span class="line">fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]</span><br><span class="line">fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(fx, x, dout)</span><br><span class="line">da_num = eval_numerical_gradient_array(fg, gamma, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(fb, beta, dout)</span><br><span class="line"></span><br><span class="line">_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)</span><br><span class="line">dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dgamma error: &#x27;, rel_error(da_num, dgamma))</span><br><span class="line">print(&#x27;dbeta error: &#x27;, rel_error(db_num, dbeta))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/20.png" alt="20"></p>
<h1 id="layers-py"><a href="#layers-py" class="headerlink" title="layers.py"></a>layers.py</h1><p>在该py代码中加入卷积的正反向传播函数。</p>
<p>N：N个数据</p>
<p>C：图像的通道数</p>
<p>H、W：长宽</p>
<p>F：F个不同的卷积核</p>
<p>w：卷积核[F,C,HH,WW]分别对应卷积核个数，图像的通道数，长宽</p>
<p>b：卷积后加上偏置值[F,]，F个卷积核因此对应F个偏置值，偏置值相当于对不同卷积核的“喜好程度”</p>
<p>stride：步长</p>
<p>pad:以零填充图像的行数</p>
<p>h_coord:能移动到的y坐标位置</p>
<p>w_coord:能移动到的x坐标位置</p>
<p>out_h,out_w:输出的数据长宽</p>
<p>out：[N,F,out_h,out_w] 说明有N个数据集，然后经过F个卷积核提取了F层的特征</p>
<p>通过卷积之后输出图像的长宽变化公式：<br>$$<br>H’ &#x3D; 1 + (H + 2 * pad - HH) &#x2F;&#x2F; stride<br>$$</p>
<p>$$<br>W’ &#x3D; 1 + (W + 2 * pad - WW) &#x2F;&#x2F; stride<br>$$</p>
<p><img src="/images/CS231n_Assignment2_Convolutional_Networks/1.png" alt="1"></p>
<p>在反向传播中，其实就是正向传播的方向求法，但是在正向传播中N和F是一起求的，可以加速求解。</p>
<p>但是在反向传播中，求dw和dx的时候需要将其分开才可求导，如果不将N和F分开的话，会导致函数中同时存在dw，dx，在求解上会非常的复杂。在求解时一定要注意对称关系！！！</p>
<p>在矩阵运算过程中,dout矩阵已经变成（2，，，）类型的矩阵，因此需要对dout进行如下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(dout[i,:,j,k])[:,None,None,None]</span><br></pre></td></tr></table></figure>

<p>使得dout矩阵变回（2，3，3，3）这样才可和w矩阵及逆行点乘</p>
<h3 id="正文段："><a href="#正文段：" class="headerlink" title="正文段："></a>正文段：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">def conv_forward_naive(x, w, b, conv_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A naive implementation of the forward pass for a convolutional layer.</span><br><span class="line"></span><br><span class="line">    The input consists of N data points, each with C channels, height H and</span><br><span class="line">    width W. We convolve each input with F different filters, where each filter</span><br><span class="line">    spans all C channels and has height HH and width HH.</span><br><span class="line"></span><br><span class="line">    Input:</span><br><span class="line">    - x: Input data of shape (N, C, H, W)</span><br><span class="line">    - w: Filter weights of shape (F, C, HH, WW)</span><br><span class="line">    - b: Biases, of shape (F,)</span><br><span class="line">    - conv_param: A dictionary with the following keys:</span><br><span class="line">      - &#x27;stride&#x27;: The number of pixels between adjacent receptive fields in the</span><br><span class="line">        horizontal and vertical directions.</span><br><span class="line">      - &#x27;pad&#x27;: The number of pixels that will be used to zero-pad the input.</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: Output data, of shape (N, F, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span><br><span class="line">      H&#x27; = 1 + (H + 2 * pad - HH) / stride</span><br><span class="line">      W&#x27; = 1 + (W + 2 * pad - WW) / stride</span><br><span class="line">    - cache: (x, w, b, conv_param)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    out = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the convolutional forward pass.                         #</span><br><span class="line">    # Hint: you can use the function np.pad for padding.                      #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    N,C,H,W=x.shape</span><br><span class="line">    F,_,HH,WW=w.shape</span><br><span class="line">    stride=conv_param[&#x27;stride&#x27;]</span><br><span class="line">    pad=conv_param[&#x27;pad&#x27;]</span><br><span class="line">    out_h=1+(H+2*pad-HH)//stride</span><br><span class="line">    out_w=1+(W+2*pad-WW)//stride</span><br><span class="line">    out=np.zeros([N,F,out_h,out_w])</span><br><span class="line">    x_pad=np.pad(x,((0,),(0,),(pad,),(pad,)),&#x27;constant&#x27;)</span><br><span class="line">    for j in range(out_h):</span><br><span class="line">        for k in range(out_w):</span><br><span class="line">            h_coord=min(j*stride,H+2*pad-HH)</span><br><span class="line">            w_coord=min(k*stride,W+2*pad-WW)</span><br><span class="line">            for i in range(F):</span><br><span class="line">                out[:,i,j,k]=np.sum(x_pad[:,:,h_coord:h_coord+HH,w_coord:w_coord+WW]*w[i,:,:,:],axis=(1,2,3))</span><br><span class="line">    out=out+b[None,:,None,None]</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    return out, cache</span><br><span class="line">    </span><br><span class="line">def conv_backward_naive(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A naive implementation of the backward pass for a convolutional layer.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dout: Upstream derivatives.</span><br><span class="line">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient with respect to x</span><br><span class="line">    - dw: Gradient with respect to w</span><br><span class="line">    - db: Gradient with respect to b</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dw, db = None, None, None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the convolutional backward pass.                        #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    db=np.sum(dout,axis=(0,2,3))</span><br><span class="line">    x,w,b,conv_param=cache</span><br><span class="line">    N,C,H,W=x.shape</span><br><span class="line">    F,_,HH,WW=w.shape</span><br><span class="line">    stride=conv_param[&#x27;stride&#x27;]</span><br><span class="line">    pad=conv_param[&#x27;pad&#x27;]</span><br><span class="line">    x_pad=np.pad(x,((0,),(0,),(pad,),(pad,)),&#x27;constant&#x27;)</span><br><span class="line">    out_h=1+(H+2*pad-HH)//stride</span><br><span class="line">    out_w=1+(W+2*pad-WW)//stride</span><br><span class="line">    dx=np.zeros_like(x)</span><br><span class="line">    dw=np.zeros_like(w)</span><br><span class="line">    dx_pad=np.zeros_like(x_pad)</span><br><span class="line">    for j in range(out_h):</span><br><span class="line">        for k in range(out_w):</span><br><span class="line">            h_coord=min(j*stride,H+2*pad-HH)</span><br><span class="line">            w_coord=min(k*stride,W+2*pad-WW)</span><br><span class="line">            for i in range(N):</span><br><span class="line">                dx_pad[i,:,h_coord:h_coord+HH,w_coord:w_coord+WW]+=np.sum((dout[i,:,j,k])[:,None,None,None]*w,axis=0)</span><br><span class="line">                </span><br><span class="line">            for i in range(F):</span><br><span class="line">                dw[i,:,:,:]+=np.sum(x_pad[:,:,h_coord:h_coord+HH,w_coord:w_coord+WW]*(dout[:,i,j,k])[:,None,None,None],axis=0)</span><br><span class="line">    dx=dx_pad[:,:,pad:-pad,pad:-pad]</span><br><span class="line">               </span><br><span class="line">                                                                          </span><br><span class="line">                                                                                                   </span><br><span class="line">                                                                               </span><br><span class="line">                   </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    return dx, dw, db</span><br></pre></td></tr></table></figure>

<p>在该段py代码中加入池化的正反向传播函数。</p>
<p>H:原输入的高</p>
<p>W：原输入的宽</p>
<p>stride：步长</p>
<p>ph:池化核的高</p>
<p>pw：池化核的宽<br>$$<br>out_h&#x3D;1+(H-ph)&#x2F;&#x2F;stride<br>$$</p>
<p>$$<br>out_w&#x3D;1+(W-pw)&#x2F;&#x2F;stide<br>$$</p>
<p>在反向传播中，需要注意的是，在该函数中不需要像卷积核一样对dw和dx求导，也就是该池化的反向与卷积核基数F和数据集N无关，所以直接对应反向求导即可。</p>
<p>另外对np.max（）求导相当于一维中max(3,5)求导类似，只有最大值才保持原值，其他置0.</p>
<p>原理同卷积核的操作一样，正文段如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">def max_pool_forward_naive(x, pool_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A naive implementation of the forward pass for a max pooling layer.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data, of shape (N, C, H, W)</span><br><span class="line">    - pool_param: dictionary with the following keys:</span><br><span class="line">      - &#x27;pool_height&#x27;: The height of each pooling region</span><br><span class="line">      - &#x27;pool_width&#x27;: The width of each pooling region</span><br><span class="line">      - &#x27;stride&#x27;: The distance between adjacent pooling regions</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: Output data</span><br><span class="line">    - cache: (x, pool_param)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    out = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the max pooling forward pass                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    N,C,H,W=x.shape</span><br><span class="line">    ph,pw,stride=pool_param[&#x27;pool_height&#x27;],pool_param[&#x27;pool_width&#x27;],pool_param[&#x27;stride&#x27;]</span><br><span class="line">    out_h=1+(H-ph)//stride</span><br><span class="line">    out_w=1+(W-pw)//stride</span><br><span class="line">    out=np.zeros([N,C,out_h,out_w])</span><br><span class="line">    for i in range(out_h):</span><br><span class="line">        for j in range(out_w):</span><br><span class="line">            h_coord=min(stride*i,H-ph)</span><br><span class="line">            w_coord=min(stride*j,W-pw)</span><br><span class="line">            out[:,:,i,j]=np.max(x[:,:,h_coord:h_coord+ph,w_coord:w_coord+pw],axis=(2,3))</span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_backward_naive(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A naive implementation of the backward pass for a max pooling layer.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dout: Upstream derivatives</span><br><span class="line">    - cache: A tuple of (x, pool_param) as in the forward pass.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    - dx: Gradient with respect to x</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the max pooling backward pass                           #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    x,pool_param=cache</span><br><span class="line">    N,C,H,W=x.shape</span><br><span class="line">    ph,pw,stride=pool_param[&#x27;pool_height&#x27;],pool_param[&#x27;pool_width&#x27;],pool_param[&#x27;stride&#x27;]</span><br><span class="line">    out_h=1+(H-ph)//stride</span><br><span class="line">    out_w=1+(W-pw)//stride</span><br><span class="line">    dx=np.zeros_like(x)</span><br><span class="line">    for i in range(out_h):</span><br><span class="line">        for j in range(out_w):</span><br><span class="line">            h_coord=min(stride*i,H+ph)</span><br><span class="line">            w_coord=min(stride*j,W-pw)</span><br><span class="line">            x_max=np.max(x[:,:,h_coord:h_coord+ph,w_coord:w_coord+pw],axis=(2,3))</span><br><span class="line">            mask=(x[:,:,h_coord:h_coord+ph,w_coord:w_coord+pw]==(x_max)[:,:,None,None])</span><br><span class="line">            dx[:,:,h_coord:h_coord+ph,w_coord:w_coord+pw]=(dout[:,:,i,j])[:,:,None,None]*mask</span><br><span class="line">                       </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    return dx</span><br></pre></td></tr></table></figure>

<p>在该段代码中加入空间批量归一化的代码</p>
<p>因为在batchnorm_forward中已经实现了将X1[N,D]归一化的方法，简单的理解就是将N个图像在每个D的维度上进行归一化。因此映射到空间批量归一化中，也就是在X2[N,C,H,W]对N个图像的H<em>W特征在每个C维度上进行批量归一化。也就是说:<br>$$<br>X2[N</em>H*W,C] ~&#x3D;X1[N,D]<br>$$<br>在使用batchnorm_forward以及batchnorm_backward之后的正文段如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">def spatial_batchnorm_forward(x, gamma, beta, bn_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the forward pass for spatial batch normalization.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data of shape (N, C, H, W)</span><br><span class="line">    - gamma: Scale parameter, of shape (C,)</span><br><span class="line">    - beta: Shift parameter, of shape (C,)</span><br><span class="line">    - bn_param: Dictionary with the following keys:</span><br><span class="line">      - mode: &#x27;train&#x27; or &#x27;test&#x27;; required</span><br><span class="line">      - eps: Constant for numeric stability</span><br><span class="line">      - momentum: Constant for running mean / variance. momentum=0 means that</span><br><span class="line">        old information is discarded completely at every time step, while</span><br><span class="line">        momentum=1 means that new information is never incorporated. The</span><br><span class="line">        default of momentum=0.9 should work well in most situations.</span><br><span class="line">      - running_mean: Array of shape (D,) giving running mean of features</span><br><span class="line">      - running_var Array of shape (D,) giving running variance of features</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: Output data, of shape (N, C, H, W)</span><br><span class="line">    - cache: Values needed for the backward pass</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    out, cache = None, None</span><br><span class="line"></span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the forward pass for spatial batch normalization.       #</span><br><span class="line">    #                                                                         #</span><br><span class="line">    # HINT: You can implement spatial batch normalization using the vanilla   #</span><br><span class="line">    # version of batch normalization defined above. Your implementation should#</span><br><span class="line">    # be very short; ours is less than five lines.                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    N,C,H,W=x.shape</span><br><span class="line">    x_batch=x.transpose(0,2,3,1).reshape((N*H*W),C)</span><br><span class="line">    out,cache=batchnorm_forward(x_batch,gamma,beta,bn_param)</span><br><span class="line">    out=out.reshape(N,H,W,C).transpose(0,3,1,2)</span><br><span class="line">    </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line"></span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def spatial_batchnorm_backward(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the backward pass for spatial batch normalization.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dout: Upstream derivatives, of shape (N, C, H, W)</span><br><span class="line">    - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span><br><span class="line">    - dgamma: Gradient with respect to scale parameter, of shape (C,)</span><br><span class="line">    - dbeta: Gradient with respect to shift parameter, of shape (C,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dgamma, dbeta = None, None, None</span><br><span class="line"></span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the backward pass for spatial batch normalization.      #</span><br><span class="line">    #                                                                         #</span><br><span class="line">    # HINT: You can implement spatial batch normalization using the vanilla   #</span><br><span class="line">    # version of batch normalization defined above. Your implementation should#</span><br><span class="line">    # be very short; ours is less than five lines.                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    N,C,H,W=dout.shape</span><br><span class="line">    dout_batch=dout.transpose(0,2,3,1).reshape((N*H*W),C)</span><br><span class="line">    dx,dgamma,dbeta=batchnorm_backward(dout_batch,cache)</span><br><span class="line">    dx=dx.reshape(N,H,W,C).transpose(0,3,1,2)</span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line"></span><br><span class="line">    return dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>



<h1 id="fast-layers-py"><a href="#fast-layers-py" class="headerlink" title="fast_layers.py"></a>fast_layers.py</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import numpy as np</span><br><span class="line">try:</span><br><span class="line">    from cs231n.im2col_cython import col2im_cython, im2col_cython</span><br><span class="line">    from cs231n.im2col_cython import col2im_6d_cython</span><br><span class="line">except ImportError:</span><br><span class="line">    print(&#x27;run the following from the cs231n directory and try again:&#x27;)</span><br><span class="line">    print(&#x27;python setup.py build_ext --inplace&#x27;)</span><br><span class="line">    print(&#x27;You may also need to restart your iPython kernel&#x27;)</span><br><span class="line"></span><br><span class="line">from cs231n.im2col import *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_forward_im2col(x, w, b, conv_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the forward pass for a convolutional layer</span><br><span class="line">    based on im2col and col2im.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    num_filters, _, filter_height, filter_width = w.shape</span><br><span class="line">    stride, pad = conv_param[&#x27;stride&#x27;], conv_param[&#x27;pad&#x27;]</span><br><span class="line"></span><br><span class="line">    # Check dimensions</span><br><span class="line">    assert (W + 2 * pad - filter_width) % stride == 0, &#x27;width does not work&#x27;</span><br><span class="line">    assert (H + 2 * pad - filter_height) % stride == 0, &#x27;height does not work&#x27;</span><br><span class="line"></span><br><span class="line">    # Create output</span><br><span class="line">    out_height = (H + 2 * pad - filter_height) // stride + 1</span><br><span class="line">    out_width = (W + 2 * pad - filter_width) // stride + 1</span><br><span class="line">    out = np.zeros((N, num_filters, out_height, out_width), dtype=x.dtype)</span><br><span class="line"></span><br><span class="line">    # x_cols = im2col_indices(x, w.shape[2], w.shape[3], pad, stride)</span><br><span class="line">    x_cols = im2col_cython(x, w.shape[2], w.shape[3], pad, stride)</span><br><span class="line">    res = w.reshape((w.shape[0], -1)).dot(x_cols) + b.reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    out = res.reshape(w.shape[0], out.shape[2], out.shape[3], x.shape[0])</span><br><span class="line">    out = out.transpose(3, 0, 1, 2)</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param, x_cols)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_forward_strides(x, w, b, conv_param):</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    stride, pad = conv_param[&#x27;stride&#x27;], conv_param[&#x27;pad&#x27;]</span><br><span class="line"></span><br><span class="line">    # Check dimensions</span><br><span class="line">    #assert (W + 2 * pad - WW) % stride == 0, &#x27;width does not work&#x27;</span><br><span class="line">    #assert (H + 2 * pad - HH) % stride == 0, &#x27;height does not work&#x27;</span><br><span class="line"></span><br><span class="line">    # Pad the input</span><br><span class="line">    p = pad</span><br><span class="line">    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=&#x27;constant&#x27;)</span><br><span class="line"></span><br><span class="line">    # Figure out output dimensions</span><br><span class="line">    H += 2 * pad</span><br><span class="line">    W += 2 * pad</span><br><span class="line">    out_h = (H - HH) // stride + 1</span><br><span class="line">    out_w = (W - WW) // stride + 1</span><br><span class="line"></span><br><span class="line">    # Perform an im2col operation by picking clever strides</span><br><span class="line">    shape = (C, HH, WW, N, out_h, out_w)</span><br><span class="line">    strides = (H * W, W, 1, C * H * W, stride * W, stride)</span><br><span class="line">    strides = x.itemsize * np.array(strides)</span><br><span class="line">    x_stride = np.lib.stride_tricks.as_strided(x_padded,</span><br><span class="line">                  shape=shape, strides=strides)</span><br><span class="line">    x_cols = np.ascontiguousarray(x_stride)</span><br><span class="line">    x_cols.shape = (C * HH * WW, N * out_h * out_w)</span><br><span class="line"></span><br><span class="line">    # Now all our convolutions are a big matrix multiply</span><br><span class="line">    res = w.reshape(F, -1).dot(x_cols) + b.reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    # Reshape the output</span><br><span class="line">    res.shape = (F, N, out_h, out_w)</span><br><span class="line">    out = res.transpose(1, 0, 2, 3)</span><br><span class="line"></span><br><span class="line">    # Be nice and return a contiguous array</span><br><span class="line">    # The old version of conv_forward_fast doesn&#x27;t do this, so for a fair</span><br><span class="line">    # comparison we won&#x27;t either</span><br><span class="line">    out = np.ascontiguousarray(out)</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param, x_cols)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_backward_strides(dout, cache):</span><br><span class="line">    x, w, b, conv_param, x_cols = cache</span><br><span class="line">    stride, pad = conv_param[&#x27;stride&#x27;], conv_param[&#x27;pad&#x27;]</span><br><span class="line"></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    _, _, out_h, out_w = dout.shape</span><br><span class="line"></span><br><span class="line">    db = np.sum(dout, axis=(0, 2, 3))</span><br><span class="line"></span><br><span class="line">    dout_reshaped = dout.transpose(1, 0, 2, 3).reshape(F, -1)</span><br><span class="line">    dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)</span><br><span class="line"></span><br><span class="line">    dx_cols = w.reshape(F, -1).T.dot(dout_reshaped)</span><br><span class="line">    dx_cols.shape = (C, HH, WW, N, out_h, out_w)</span><br><span class="line">    dx = col2im_6d_cython(dx_cols, N, C, H, W, HH, WW, pad, stride)</span><br><span class="line"></span><br><span class="line">    return dx, dw, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_backward_im2col(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the backward pass for a convolutional layer</span><br><span class="line">    based on im2col and col2im.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, w, b, conv_param, x_cols = cache</span><br><span class="line">    stride, pad = conv_param[&#x27;stride&#x27;], conv_param[&#x27;pad&#x27;]</span><br><span class="line"></span><br><span class="line">    db = np.sum(dout, axis=(0, 2, 3))</span><br><span class="line"></span><br><span class="line">    num_filters, _, filter_height, filter_width = w.shape</span><br><span class="line">    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(num_filters, -1)</span><br><span class="line">    dw = dout_reshaped.dot(x_cols.T).reshape(w.shape)</span><br><span class="line"></span><br><span class="line">    dx_cols = w.reshape(num_filters, -1).T.dot(dout_reshaped)</span><br><span class="line">    # dx = col2im_indices(dx_cols, x.shape, filter_height, filter_width, pad, stride)</span><br><span class="line">    dx = col2im_cython(dx_cols, x.shape[0], x.shape[1], x.shape[2], x.shape[3],</span><br><span class="line">                       filter_height, filter_width, pad, stride)</span><br><span class="line"></span><br><span class="line">    return dx, dw, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conv_forward_fast = conv_forward_strides</span><br><span class="line">conv_backward_fast = conv_backward_strides</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_forward_fast(x, pool_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the forward pass for a max pooling layer.</span><br><span class="line"></span><br><span class="line">    This chooses between the reshape method and the im2col method. If the pooling</span><br><span class="line">    regions are square and tile the input image, then we can use the reshape</span><br><span class="line">    method which is very fast. Otherwise we fall back on the im2col method, which</span><br><span class="line">    is not much faster than the naive method.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width = pool_param[&#x27;pool_height&#x27;], pool_param[&#x27;pool_width&#x27;]</span><br><span class="line">    stride = pool_param[&#x27;stride&#x27;]</span><br><span class="line"></span><br><span class="line">    same_size = pool_height == pool_width == stride</span><br><span class="line">    tiles = H % pool_height == 0 and W % pool_width == 0</span><br><span class="line">    if same_size and tiles:</span><br><span class="line">        out, reshape_cache = max_pool_forward_reshape(x, pool_param)</span><br><span class="line">        cache = (&#x27;reshape&#x27;, reshape_cache)</span><br><span class="line">    else:</span><br><span class="line">        out, im2col_cache = max_pool_forward_im2col(x, pool_param)</span><br><span class="line">        cache = (&#x27;im2col&#x27;, im2col_cache)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_backward_fast(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the backward pass for a max pooling layer.</span><br><span class="line"></span><br><span class="line">    This switches between the reshape method an the im2col method depending on</span><br><span class="line">    which method was used to generate the cache.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    method, real_cache = cache</span><br><span class="line">    if method == &#x27;reshape&#x27;:</span><br><span class="line">        return max_pool_backward_reshape(dout, real_cache)</span><br><span class="line">    elif method == &#x27;im2col&#x27;:</span><br><span class="line">        return max_pool_backward_im2col(dout, real_cache)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError(&#x27;Unrecognized method &quot;%s&quot;&#x27; % method)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_forward_reshape(x, pool_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the forward pass for the max pooling layer that uses</span><br><span class="line">    some clever reshaping.</span><br><span class="line"></span><br><span class="line">    This can only be used for square pooling regions that tile the input.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width = pool_param[&#x27;pool_height&#x27;], pool_param[&#x27;pool_width&#x27;]</span><br><span class="line">    stride = pool_param[&#x27;stride&#x27;]</span><br><span class="line">    assert pool_height == pool_width == stride, &#x27;Invalid pool params&#x27;</span><br><span class="line">    assert H % pool_height == 0</span><br><span class="line">    assert W % pool_height == 0</span><br><span class="line">    x_reshaped = x.reshape(N, C, H // pool_height, pool_height,</span><br><span class="line">                           W // pool_width, pool_width)</span><br><span class="line">    out = x_reshaped.max(axis=3).max(axis=4)</span><br><span class="line"></span><br><span class="line">    cache = (x, x_reshaped, out)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_backward_reshape(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fast implementation of the backward pass for the max pooling layer that</span><br><span class="line">    uses some clever broadcasting and reshaping.</span><br><span class="line"></span><br><span class="line">    This can only be used if the forward pass was computed using</span><br><span class="line">    max_pool_forward_reshape.</span><br><span class="line"></span><br><span class="line">    NOTE: If there are multiple argmaxes, this method will assign gradient to</span><br><span class="line">    ALL argmax elements of the input rather than picking one. In this case the</span><br><span class="line">    gradient will actually be incorrect. However this is unlikely to occur in</span><br><span class="line">    practice, so it shouldn&#x27;t matter much. One possible solution is to split the</span><br><span class="line">    upstream gradient equally among all argmax elements; this should result in a</span><br><span class="line">    valid subgradient. You can make this happen by uncommenting the line below;</span><br><span class="line">    however this results in a significant performance penalty (about 40% slower)</span><br><span class="line">    and is unlikely to matter in practice so we don&#x27;t do it.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, x_reshaped, out = cache</span><br><span class="line"></span><br><span class="line">    dx_reshaped = np.zeros_like(x_reshaped)</span><br><span class="line">    out_newaxis = out[:, :, :, np.newaxis, :, np.newaxis]</span><br><span class="line">    mask = (x_reshaped == out_newaxis)</span><br><span class="line">    dout_newaxis = dout[:, :, :, np.newaxis, :, np.newaxis]</span><br><span class="line">    dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)</span><br><span class="line">    dx_reshaped[mask] = dout_broadcast[mask]</span><br><span class="line">    dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)</span><br><span class="line">    dx = dx_reshaped.reshape(x.shape)</span><br><span class="line"></span><br><span class="line">    return dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_forward_im2col(x, pool_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    An implementation of the forward pass for max pooling based on im2col.</span><br><span class="line"></span><br><span class="line">    This isn&#x27;t much faster than the naive version, so it should be avoided if</span><br><span class="line">    possible.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width = pool_param[&#x27;pool_height&#x27;], pool_param[&#x27;pool_width&#x27;]</span><br><span class="line">    stride = pool_param[&#x27;stride&#x27;]</span><br><span class="line"></span><br><span class="line">    assert (H - pool_height) % stride == 0, &#x27;Invalid height&#x27;</span><br><span class="line">    assert (W - pool_width) % stride == 0, &#x27;Invalid width&#x27;</span><br><span class="line"></span><br><span class="line">    out_height = (H - pool_height) // stride + 1</span><br><span class="line">    out_width = (W - pool_width) // stride + 1</span><br><span class="line"></span><br><span class="line">    x_split = x.reshape(N * C, 1, H, W)</span><br><span class="line">    x_cols = im2col(x_split, pool_height, pool_width, padding=0, stride=stride)</span><br><span class="line">    x_cols_argmax = np.argmax(x_cols, axis=0)</span><br><span class="line">    x_cols_max = x_cols[x_cols_argmax, np.arange(x_cols.shape[1])]</span><br><span class="line">    out = x_cols_max.reshape(out_height, out_width, N, C).transpose(2, 3, 0, 1)</span><br><span class="line"></span><br><span class="line">    cache = (x, x_cols, x_cols_argmax, pool_param)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def max_pool_backward_im2col(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    An implementation of the backward pass for max pooling based on im2col.</span><br><span class="line"></span><br><span class="line">    This isn&#x27;t much faster than the naive version, so it should be avoided if</span><br><span class="line">    possible.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, x_cols, x_cols_argmax, pool_param = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width = pool_param[&#x27;pool_height&#x27;], pool_param[&#x27;pool_width&#x27;]</span><br><span class="line">    stride = pool_param[&#x27;stride&#x27;]</span><br><span class="line"></span><br><span class="line">    dout_reshaped = dout.transpose(2, 3, 0, 1).flatten()</span><br><span class="line">    dx_cols = np.zeros_like(x_cols)</span><br><span class="line">    dx_cols[x_cols_argmax, np.arange(dx_cols.shape[1])] = dout_reshaped</span><br><span class="line">    dx = col2im_indices(dx_cols, (N * C, 1, H, W), pool_height, pool_width,</span><br><span class="line">                padding=0, stride=stride)</span><br><span class="line">    dx = dx.reshape(x.shape)</span><br><span class="line"></span><br><span class="line">    return dx</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="layer-utils-py"><a href="#layer-utils-py" class="headerlink" title="layer_utils.py"></a>layer_utils.py</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def conv_relu_pool_forward(x, w, b, conv_param, pool_param):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Convenience layer that performs a convolution, a ReLU, and a pool.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input to the convolutional layer</span><br><span class="line">    - w, b, conv_param: Weights and parameters for the convolutional layer</span><br><span class="line">    - pool_param: Parameters for the pooling layer</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: Output from the pooling layer</span><br><span class="line">    - cache: Object to give to the backward pass</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    a, conv_cache = conv_forward_fast(x, w, b, conv_param)</span><br><span class="line">    s, relu_cache = relu_forward(a)</span><br><span class="line">    out, pool_cache = max_pool_forward_fast(s, pool_param)</span><br><span class="line">    cache = (conv_cache, relu_cache, pool_cache)</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_relu_pool_backward(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Backward pass for the conv-relu-pool convenience layer</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    conv_cache, relu_cache, pool_cache = cache</span><br><span class="line">    ds = max_pool_backward_fast(dout, pool_cache)</span><br><span class="line">    da = relu_backward(ds, relu_cache)</span><br><span class="line">    dx, dw, db = conv_backward_fast(da, conv_cache)</span><br><span class="line">    return dx, dw, db</span><br></pre></td></tr></table></figure>

<h1 id="cnn-py"><a href="#cnn-py" class="headerlink" title="cnn.py"></a>cnn.py</h1><p>三层卷积神经网络的结构如下：</p>
<p>conv - relu - 2x2 max pool - affine - relu - affine - softmax</p>
<p>主要步骤：</p>
<p>1.初始化权重W，b</p>
<p>W1的初始化是卷积核的大小，即<br>$$<br>W1&#x3D;[F,C,HH,WW]<br>$$<br>因此<br>$$<br>W1&#x3D;[num_filters,C,filter_size,filter_size]<br>$$<br>W2是经过卷积，relu，最大池化后的权重矩阵，由于X*w+b</p>
<p>因此W2的大小为X的经过卷积relu池化后的大小<br>$$<br>X&#x3D;[num,C,H,W]<br>$$<br>num为数据集的数量,C为通道，H,W分别代表高宽，因此<br>$$<br>W2&#x3D;[num_filters*(H&#x2F;&#x2F;2)<em>(W&#x2F;&#x2F;2),hidden_dim]<br>$$<br>num_filters是经过卷积核的数量，因为这里是通过2</em>2的最大池化，所以W和H的大小变为原来的1&#x2F;2，hidden_dim表示隐藏层的单位数，这个是可以随你设置的。</p>
<p>W3是经过relu-affine后的权重矩阵</p>
<p>因此<br>$$<br>W3&#x3D;[hidden,num_classes]<br>$$<br>之后的代码类似于fc_net.py中的两层神经网络的构造一样，只不过在这里是加入了卷积层核池化层，方法完全类似，正文段如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">from builtins import object</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from cs231n.layers import *</span><br><span class="line">from cs231n.fast_layers import *</span><br><span class="line">from cs231n.layer_utils import *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ThreeLayerConvNet(object):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A three-layer convolutional network with the following architecture:</span><br><span class="line"></span><br><span class="line">    conv - relu - 2x2 max pool - affine - relu - affine - softmax</span><br><span class="line"></span><br><span class="line">    The network operates on minibatches of data that have shape (N, C, H, W)</span><br><span class="line">    consisting of N images, each with height H and width W and with C input</span><br><span class="line">    channels.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,</span><br><span class="line">                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,</span><br><span class="line">                 dtype=np.float32):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Initialize a new network.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - input_dim: Tuple (C, H, W) giving size of input data</span><br><span class="line">        - num_filters: Number of filters to use in the convolutional layer</span><br><span class="line">        - filter_size: Size of filters to use in the convolutional layer</span><br><span class="line">        - hidden_dim: Number of units to use in the fully-connected hidden layer</span><br><span class="line">        - num_classes: Number of scores to produce from the final affine layer.</span><br><span class="line">        - weight_scale: Scalar giving standard deviation for random initialization</span><br><span class="line">          of weights.</span><br><span class="line">        - reg: Scalar giving L2 regularization strength</span><br><span class="line">        - dtype: numpy datatype to use for computation.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.dtype = dtype</span><br><span class="line"></span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Initialize weights and biases for the three-layer convolutional    #</span><br><span class="line">        # network. Weights should be initialized from a Gaussian with standard     #</span><br><span class="line">        # deviation equal to weight_scale; biases should be initialized to zero.   #</span><br><span class="line">        # All weights and biases should be stored in the dictionary self.params.   #</span><br><span class="line">        # Store weights and biases for the convolutional layer using the keys &#x27;W1&#x27; #</span><br><span class="line">        # and &#x27;b1&#x27;; use keys &#x27;W2&#x27; and &#x27;b2&#x27; for the weights and biases of the       #</span><br><span class="line">        # hidden affine layer, and keys &#x27;W3&#x27; and &#x27;b3&#x27; for the weights and biases   #</span><br><span class="line">        # of the output affine layer.                                              #</span><br><span class="line">        ###########################################################################</span><br><span class="line">        C,H,W=input_dim</span><br><span class="line">        self.params[&#x27;W1&#x27;]=weight_scale*np.random.randn(num_filters,C,filter_size,filter_size)</span><br><span class="line">        self.params[&#x27;b1&#x27;]=np.zeros(num_filters)</span><br><span class="line">        self.params[&#x27;W2&#x27;]=weight_scale*np.random.randn(num_filters*(H//2)*(W//2),hidden_dim)</span><br><span class="line">        self.params[&#x27;b2&#x27;]=np.zeros(hidden_dim)</span><br><span class="line">        self.params[&#x27;W3&#x27;]=weight_scale*np.random.randn(hidden_dim,num_classes)</span><br><span class="line">        self.params[&#x27;b3&#x27;]=np.zeros(num_classes)</span><br><span class="line">                                                                           </span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        for k, v in self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def loss(self, X, y=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Evaluate loss and gradient for the three-layer convolutional network.</span><br><span class="line"></span><br><span class="line">        Input / output: Same API as TwoLayerNet in fc_net.py.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        W1, b1 = self.params[&#x27;W1&#x27;], self.params[&#x27;b1&#x27;]</span><br><span class="line">        W2, b2 = self.params[&#x27;W2&#x27;], self.params[&#x27;b2&#x27;]</span><br><span class="line">        W3, b3 = self.params[&#x27;W3&#x27;], self.params[&#x27;b3&#x27;]</span><br><span class="line"></span><br><span class="line">        # pass conv_param to the forward pass for the convolutional layer</span><br><span class="line">        filter_size = W1.shape[2]</span><br><span class="line">        conv_param = &#123;&#x27;stride&#x27;: 1, &#x27;pad&#x27;: (filter_size - 1) // 2&#125;</span><br><span class="line"></span><br><span class="line">        # pass pool_param to the forward pass for the max-pooling layer</span><br><span class="line">        pool_param = &#123;&#x27;pool_height&#x27;: 2, &#x27;pool_width&#x27;: 2, &#x27;stride&#x27;: 2&#125;</span><br><span class="line"></span><br><span class="line">        scores = None</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the forward pass for the three-layer convolutional net,  #</span><br><span class="line">        # computing the class scores for X and storing them in the scores          #</span><br><span class="line">        # variable.                                                                #</span><br><span class="line">        ############################################################################</span><br><span class="line">        out1,cache1=conv_relu_pool_forward(X,W1,b1,conv_param,pool_param)</span><br><span class="line">        out2,cache2=affine_relu_forward(out1,W2,b2)</span><br><span class="line">        out3,cache3=affine_forward(out2,W3,b3)</span><br><span class="line">        scores=out3</span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        if y is None:</span><br><span class="line">            return scores</span><br><span class="line"></span><br><span class="line">        loss, grads = 0, &#123;&#125;</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the backward pass for the three-layer convolutional net, #</span><br><span class="line">        # storing the loss and gradients in the loss and grads variables. Compute  #</span><br><span class="line">        # data loss using softmax, and make sure that grads[k] holds the gradients #</span><br><span class="line">        # for self.params[k]. Don&#x27;t forget to add L2 regularization!               #</span><br><span class="line">        ############################################################################</span><br><span class="line">        </span><br><span class="line">        #第三层卷积神经网络</span><br><span class="line">        loss,dscores=softmax_loss(scores,y)</span><br><span class="line">        loss+=0.5*self.reg*np.sum(W3**2)</span><br><span class="line">        dx3,dw3,db3=affine_backward(dscores,cache3)</span><br><span class="line">        grads[&#x27;W3&#x27;]=dw3+self.reg*W3</span><br><span class="line">        grads[&#x27;b3&#x27;]=db3</span><br><span class="line">        </span><br><span class="line">        #第二层卷积神经网络</span><br><span class="line">        loss+=0.5*self.reg*np.sum(W2**2)</span><br><span class="line">        dx2,dw2,db2=affine_relu_backward(dx3,cache2)</span><br><span class="line">        grads[&#x27;W2&#x27;]=dw2+self.reg*W2</span><br><span class="line">        grads[&#x27;b2&#x27;]=db2</span><br><span class="line">        </span><br><span class="line">        #第一层卷积神经网络</span><br><span class="line">        loss+=0.5*self.reg*np.sum(W1**2)</span><br><span class="line">        dx1,dw1,db1=conv_relu_pool_backward(dx2,cache1)</span><br><span class="line">        grads[&#x27;W1&#x27;]=dw1+self.reg*W1</span><br><span class="line">        grads[&#x27;b1&#x27;]=db1</span><br><span class="line">        </span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        return loss, grads</span><br><span class="line"></span><br></pre></td></tr></table></figure>




        </div>
        
<blockquote class="copyright">
    <p><strong>Link to this article : </strong><a class="permalink" href="http://yoursite.com/2020/07/06/CS231n_Assignment2_Convolutional_Networks/">http://yoursite.com/2020/07/06/CS231n_Assignment2_Convolutional_Networks/</a></p>
    <p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p>
</blockquote>


    </article>
    
    <section id="comments">
        
    </section>


    

</main>


<aside style="" id="sidebar" class="aside aside-fixture">
    <div class="toc-sidebar">
        <nav id="toc" class="article-toc">
            <h3 class="toc-title">Catalogue</h3>
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolutional-Networks"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-Naive-forward-pass"><span class="toc-number">1.0.1.</span> <span class="toc-text">Convolution: Naive forward pass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Aside-Image-processing-via-convolutions"><span class="toc-number">1.0.2.</span> <span class="toc-text">Aside: Image processing via convolutions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-Naive-backward-pass"><span class="toc-number">1.0.3.</span> <span class="toc-text">Convolution: Naive backward pass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Max-pooling-Naive-forward"><span class="toc-number">1.0.4.</span> <span class="toc-text">Max pooling: Naive forward</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Max-pooling-Naive-backward"><span class="toc-number">1.1.</span> <span class="toc-text">Max pooling: Naive backward</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Fast-layers"><span class="toc-number">1.1.1.</span> <span class="toc-text">Fast layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional-%E2%80%9Csandwich%E2%80%9D-layers"><span class="toc-number">1.1.2.</span> <span class="toc-text">Convolutional “sandwich” layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Three-layer-ConvNet"><span class="toc-number">1.1.3.</span> <span class="toc-text">Three-layer ConvNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sanity-check-loss"><span class="toc-number">1.1.4.</span> <span class="toc-text">Sanity check loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-check"><span class="toc-number">1.1.5.</span> <span class="toc-text">Gradient check</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfit-small-data"><span class="toc-number">1.1.6.</span> <span class="toc-text">Overfit small data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-the-net"><span class="toc-number">1.1.7.</span> <span class="toc-text">Train the net</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Visualize-Filters"><span class="toc-number">1.1.8.</span> <span class="toc-text">Visualize Filters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spatial-Batch-Normalization"><span class="toc-number">1.1.9.</span> <span class="toc-text">Spatial Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spatial-batch-normalization-forward"><span class="toc-number">1.1.10.</span> <span class="toc-text">Spatial batch normalization: forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spatial-batch-normalization-backward"><span class="toc-number">1.1.11.</span> <span class="toc-text">Spatial batch normalization: backward</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#layers-py"><span class="toc-number">2.</span> <span class="toc-text">layers.py</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E6%96%87%E6%AE%B5%EF%BC%9A"><span class="toc-number">2.0.1.</span> <span class="toc-text">正文段：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fast-layers-py"><span class="toc-number">3.</span> <span class="toc-text">fast_layers.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#layer-utils-py"><span class="toc-number">4.</span> <span class="toc-text">layer_utils.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn-py"><span class="toc-number">5.</span> <span class="toc-text">cnn.py</span></a></li></ol>
        </nav>
    </div>
</aside>





        </section>
        <footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40">
    
    <div class="footer-social-links">
        
            <a target="_blank" rel="noopener" href="https://github.com/fengkx">
                <i class="iconfont icon-github"></i>
            </a>
        
            <a target="_blank" rel="noopener" href="https://t.me/fengkx">
                <i class="iconfont icon-telegram"></i>
            </a>
        
            <a target="_blank" rel="noopener" href="https://twitter.com/example">
                <i class="iconfont icon-twitter"></i>
            </a>
        
            <a href="/atom.xml">
                <i class="iconfont icon-rss"></i>
            </a>
        
    </div>
    
    
</footer>

        <div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div>
        <div id="search-view-container" class="hidden shadow-xl"></div>
        
<script src="/js/dom-event.min.js"></script>



<script src="/js/local-search.min.js"></script>



    <script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script>
    
<script src="/js/light-gallery.min.js"></script>






    </body>
</html>
