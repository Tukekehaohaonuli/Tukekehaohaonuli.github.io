<!DOCTYPE html>
<html  lang="english" >
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <title>Tukekenulia</title>
    <meta name="description" content="总结：本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tukekenulia">
<meta property="og:url" content="http://yoursite.com/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/index.html">
<meta property="og:site_name" content="Tukekenulia">
<meta property="og:description" content="总结：本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。">
<meta property="og:locale">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/5.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/6.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/7.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/8.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/9.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/10.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/3.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/2.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/1.png">
<meta property="article:published_time" content="2020-07-11T08:07:42.000Z">
<meta property="article:modified_time" content="2020-07-11T08:07:42.000Z">
<meta property="article:author" content="Tukeke">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png">

    
    <link rel="icon" href="/images/icon.png" type="image/x-icon">

    
<link rel="stylesheet" href="/css/common.min.css">



    
    
    
    
        <link href="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/css/lightgallery.min.css" rel="stylesheet">
    
    
    
<link rel="stylesheet" href="/css/iconfont.min.css">

    
<meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <header class="header header-fixture">
    <div class="profile-search-wrap flex sm:block">
        
        
        <div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6">
            <a id="avatar" role="link" href="https://github.com/fengkx" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer" >
                <img src="https://www.gravatar.com/avatar/0bc83cb571cd1c50ba6f3e8a78ef1346?s=128" class="rounded-full" alt="avatar">
            </a>
            <h2 id="name" class="hidden lg:block">fengkx</h2>
            <h3 id="title" class="hidden lg:block">Student &amp; Coder</h3>
            
            <small id="location" class="hidden lg:block">
                <i class="iconfont icon-map-icon"></i>
                Guangzhou, China
            </small>
            
        </div>
        
        
<div class="search flex-1 flex lg:inline-block sm:hidden lg:px-4 lg:mt-2 lg:mb-4 lg:w-full">
    <form id="search-form" class="my-auto flex-1 lg:border lg:border-solid lg:border-gray-200">
        <div class="input-group table bg-gray-100 lg:bg-white w-full">
            <input id="search-input" type="text" placeholder="Search" class="inline-block w-full bg-gray-100 lg:bg-white p-1">
            <span class="table-cell">
                <button name="search tigger button" disabled>
                    <i class="iconfont icon-search m-2"></i>
                </button>
            </span>
        </div>
    </form>
        
<div id="content-json" data-placeholder="Search" class="invisible hidden">/content.json</div>
<script id="search-teamplate" type="text/html" data-path="/content.json">
    <div>
        <div class="search-header bg-gray-400">
            <input id="actual-search-input" model="keyword" ref="input" class="inline-block w-full h-10 px-2 py-1" placeholder="Search" type="text">
        </div>
        <div class="search-result bg-gray-200">
            {{#each searchPosts}}
            <a href="/{{ path }}" class="result-item block px-2 pb-3 mb-1 pt-1 hover:bg-indigo-100">
                <i class="iconfont icon-file"></i>
                <h1 class="result-title inline font-medium text-lg">{{ title }}</h1>
                <p class="result-content text-gray-600 text-sm">{{{ text }}}</p>
            </a>
            {{/each}}
        </div>
    </div>
</script>

</div>


        <button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false">
            <i class="iconfont icon-hamburger"></i>
        </button>
    </div>
    <nav id="menu-nav" class="hidden sm:flex flex-col">
        
        
            <div class="menu-item menu-home" role="menuitem">
                <a href="/.">
                    <i class="iconfont icon-home" aria-hidden="true"></i>
                    <span class="menu-title">Home</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-archives" role="menuitem">
                <a href="/archives">
                    <i class="iconfont icon-archive" aria-hidden="true"></i>
                    <span class="menu-title">Archives</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-categories" role="menuitem">
                <a href="/categories">
                    <i class="iconfont icon-folder" aria-hidden="true"></i>
                    <span class="menu-title">Categories</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-tags" role="menuitem">
                <a href="/tags">
                    <i class="iconfont icon-tag" aria-hidden="true"></i>
                    <span class="menu-title">Tags</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-repository" role="menuitem">
                <a href="/repository">
                    <i class="iconfont icon-project" aria-hidden="true"></i>
                    <span class="menu-title">Repository</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-links" role="menuitem">
                <a href="/links">
                    <i class="iconfont icon-friend" aria-hidden="true"></i>
                    <span class="menu-title">Links</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-about" role="menuitem">
                <a href="/about">
                    <i class="iconfont icon-cup" aria-hidden="true"></i>
                    <span class="menu-title">About</span>
                </a>
            </div>
        
        
<div class="social-links flex sm:flex-col lg:hidden mt-5">
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://github.com/fengkx">
                <i class="iconfont social-icon icon-github"></i>
                <span class="menu-title hidden lg:inline">menu.github</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://t.me/fengkx">
                <i class="iconfont social-icon icon-telegram"></i>
                <span class="menu-title hidden lg:inline">menu.telegram</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://twitter.com/example">
                <i class="iconfont social-icon icon-twitter"></i>
                <span class="menu-title hidden lg:inline">menu.twitter</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a href="/atom.xml">
                <i class="iconfont social-icon icon-rss"></i>
                <span class="menu-title hidden lg:inline">menu.rss</span>
            </a>
        </span>
    
</div>


    </nav>
</header>

        <section class="main-section">
            
    <main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen">
    

    <article class="content article article-archives article-type-list" itemscope="">
        <header class="article-header">
            


            <p class="article-meta mb-3 text-xs">
                <span class="article-date">
    <i class="iconfont icon-calendar-check"></i>
	<a href="/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/" class="article-date">
	  <time datetime="2020-07-11T08:07:42.000Z" itemprop="datePublished">Jul 11</time>
	</a>
</span>

                

                

                <span class="_partial/post-comment"><i class="icon icon-comment"></i>
                    <a href="/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/#comments" class="article-comment-link">
                        Comments
                    </a>
                </span>
                

            </p>
        </header>
        <div class="marked-body article-body">
            <h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。</p>
<span id="more"></span>

<p>后面3个作业都是要用到TensorFlow或Pytorch框架来写，因此暂且搁置，等学完框架可能还会再回来的哈哈哈，目前CS231n assignment作业到此完结，关于CS231N的视频课程，在lecture 14后看的不是很清晰，以后如果还有机会使用到这个部分的内容，可以再回来深究，CS231N的学习历程暂告一段落，未完待续…</p>
<h1 id="Image-Captioning-with-LSTMs"><a href="#Image-Captioning-with-LSTMs" class="headerlink" title="Image Captioning with LSTMs"></a>Image Captioning with LSTMs</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># As usual, a bit of setup</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import time, os, json</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array</span><br><span class="line">from cs231n.rnn_layers import *</span><br><span class="line">from cs231n.captioning_solver import CaptioningSolver</span><br><span class="line">from cs231n.classifiers.rnn import CaptioningRNN</span><br><span class="line">from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions</span><br><span class="line">from cs231n.image_utils import image_from_url</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[&#x27;figure.figsize&#x27;] = (10.0, 8.0) # set default size of plots</span><br><span class="line">plt.rcParams[&#x27;image.interpolation&#x27;] = &#x27;nearest&#x27;</span><br><span class="line">plt.rcParams[&#x27;image.cmap&#x27;] = &#x27;gray&#x27;</span><br><span class="line"></span><br><span class="line"># for auto-reloading external modules</span><br><span class="line"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload 2</span><br><span class="line"></span><br><span class="line">def rel_error(x, y):</span><br><span class="line">    &quot;&quot;&quot; returns relative error &quot;&quot;&quot;</span><br><span class="line">    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))</span><br></pre></td></tr></table></figure>

<h3 id="Load-MS-COCO-data"><a href="#Load-MS-COCO-data" class="headerlink" title="Load MS-COCO data"></a>Load MS-COCO data</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Load COCO data from disk; this returns a dictionary</span><br><span class="line"># We&#x27;ll work with dimensionality-reduced features for this notebook, but feel</span><br><span class="line"># free to experiment with the original features by changing the flag below.</span><br><span class="line">data = load_coco_data(pca_features=True)</span><br><span class="line"></span><br><span class="line"># Print out all the keys and values from the data dictionary</span><br><span class="line">for k, v in data.items():</span><br><span class="line">    if type(v) == np.ndarray:</span><br><span class="line">        print(k, type(v), v.shape, v.dtype)</span><br><span class="line">    else:</span><br><span class="line">        print(k, type(v), len(v))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png" alt="4"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>在rnn_layers.py中完成lstm_step_forward以及反向LSTM的反向单步传播，并在这里检验误差。</p>
<h3 id="LSTM-step-forward"><a href="#LSTM-step-forward" class="headerlink" title="LSTM: step forward"></a>LSTM: step forward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">N, D, H = 3, 4, 5</span><br><span class="line">x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)</span><br><span class="line">prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)</span><br><span class="line">prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)</span><br><span class="line">Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)</span><br><span class="line">Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)</span><br><span class="line">b = np.linspace(0.3, 0.7, num=4*H)</span><br><span class="line"></span><br><span class="line">next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">expected_next_h = np.asarray([</span><br><span class="line">    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],</span><br><span class="line">    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],</span><br><span class="line">    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])</span><br><span class="line">expected_next_c = np.asarray([</span><br><span class="line">    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],</span><br><span class="line">    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],</span><br><span class="line">    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])</span><br><span class="line"></span><br><span class="line">print(&#x27;next_h error: &#x27;, rel_error(expected_next_h, next_h))</span><br><span class="line">print(&#x27;next_c error: &#x27;, rel_error(expected_next_c, next_c))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/5.png" alt="5"></p>
<h3 id="LSTM-step-backward"><a href="#LSTM-step-backward" class="headerlink" title="LSTM: step backward"></a>LSTM: step backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">N, D, H = 4, 5, 6</span><br><span class="line">x = np.random.randn(N, D)</span><br><span class="line">prev_h = np.random.randn(N, H)</span><br><span class="line">prev_c = np.random.randn(N, H)</span><br><span class="line">Wx = np.random.randn(D, 4 * H)</span><br><span class="line">Wh = np.random.randn(H, 4 * H)</span><br><span class="line">b = np.random.randn(4 * H)</span><br><span class="line"></span><br><span class="line">next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">dnext_h = np.random.randn(*next_h.shape)</span><br><span class="line">dnext_c = np.random.randn(*next_c.shape)</span><br><span class="line"></span><br><span class="line">fx_h = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fh_h = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fc_h = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fWx_h = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fWh_h = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fb_h = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line"></span><br><span class="line">fx_c = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fh_c = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fc_c = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fWx_c = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fWh_c = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fb_c = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line"></span><br><span class="line">num_grad = eval_numerical_gradient_array</span><br><span class="line"></span><br><span class="line">dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)</span><br><span class="line">dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)</span><br><span class="line">dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)</span><br><span class="line">dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)</span><br><span class="line">dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)</span><br><span class="line">db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)</span><br><span class="line"></span><br><span class="line">dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)</span><br><span class="line"></span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dh error: &#x27;, rel_error(dh_num, dh))</span><br><span class="line">print(&#x27;dc error: &#x27;, rel_error(dc_num, dc))</span><br><span class="line">print(&#x27;dWx error: &#x27;, rel_error(dWx_num, dWx))</span><br><span class="line">print(&#x27;dWh error: &#x27;, rel_error(dWh_num, dWh))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/6.png" alt="6"></p>
<h3 id="LSTM-forward"><a href="#LSTM-forward" class="headerlink" title="LSTM: forward"></a>LSTM: forward</h3><p>在rnn_layers.py中完成Lstm的正反向传播，并在此检验误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N, D, H, T = 2, 5, 4, 3</span><br><span class="line">x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)</span><br><span class="line">h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)</span><br><span class="line">Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)</span><br><span class="line">Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)</span><br><span class="line">b = np.linspace(0.2, 0.7, num=4*H)</span><br><span class="line"></span><br><span class="line">h, cache = lstm_forward(x, h0, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">expected_h = np.asarray([</span><br><span class="line"> [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],</span><br><span class="line">  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],</span><br><span class="line">  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],</span><br><span class="line"> [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],</span><br><span class="line">  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],</span><br><span class="line">  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])</span><br><span class="line"></span><br><span class="line">print(&#x27;h error: &#x27;, rel_error(expected_h, h))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/7.png" alt="7"></p>
<h3 id="LSTM-backward"><a href="#LSTM-backward" class="headerlink" title="LSTM: backward"></a>LSTM: backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.rnn_layers import lstm_forward, lstm_backward</span><br><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">N, D, T, H = 2, 3, 10, 6</span><br><span class="line"></span><br><span class="line">x = np.random.randn(N, T, D)</span><br><span class="line">h0 = np.random.randn(N, H)</span><br><span class="line">Wx = np.random.randn(D, 4 * H)</span><br><span class="line">Wh = np.random.randn(H, 4 * H)</span><br><span class="line">b = np.random.randn(4 * H)</span><br><span class="line"></span><br><span class="line">out, cache = lstm_forward(x, h0, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">dout = np.random.randn(*out.shape)</span><br><span class="line"></span><br><span class="line">dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)</span><br><span class="line"></span><br><span class="line">fx = lambda x: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fh0 = lambda h0: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fWh = lambda Wh: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fb = lambda b: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(fx, x, dout)</span><br><span class="line">dh0_num = eval_numerical_gradient_array(fh0, h0, dout)</span><br><span class="line">dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)</span><br><span class="line">dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(fb, b, dout)</span><br><span class="line"></span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dh0 error: &#x27;, rel_error(dh0_num, dh0))</span><br><span class="line">print(&#x27;dWx error: &#x27;, rel_error(dWx_num, dWx))</span><br><span class="line">print(&#x27;dWh error: &#x27;, rel_error(dWh_num, dWh))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/8.png" alt="8"></p>
<h3 id="LSTM-captioning-model"><a href="#LSTM-captioning-model" class="headerlink" title="LSTM captioning model"></a>LSTM captioning model</h3><p>完成lstm的loss函数和sample函数，实际上是与朴素的RNN算法完全一致，只不过就是里面的内部隐藏状态多了一个c_t，但是外部的操作完全一致，内部的操作已经体现在lstm_forward中，因此这边只需要在rnn.py中添加一个判断类型的语句即可。这里就不贴代码了~~~~</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">N, D, W, H = 10, 20, 30, 40</span><br><span class="line">word_to_idx = &#123;&#x27;&lt;NULL&gt;&#x27;: 0, &#x27;cat&#x27;: 2, &#x27;dog&#x27;: 3&#125;</span><br><span class="line">V = len(word_to_idx)</span><br><span class="line">T = 13</span><br><span class="line"></span><br><span class="line">model = CaptioningRNN(word_to_idx,</span><br><span class="line">          input_dim=D,</span><br><span class="line">          wordvec_dim=W,</span><br><span class="line">          hidden_dim=H,</span><br><span class="line">          cell_type=&#x27;lstm&#x27;,</span><br><span class="line">          dtype=np.float64)</span><br><span class="line"></span><br><span class="line"># Set all model parameters to fixed values</span><br><span class="line">for k, v in model.params.items():</span><br><span class="line">  model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)</span><br><span class="line"></span><br><span class="line">features = np.linspace(-0.5, 1.7, num=N*D).reshape(N, D)</span><br><span class="line">captions = (np.arange(N * T) % V).reshape(N, T)</span><br><span class="line"></span><br><span class="line">loss, grads = model.loss(features, captions)</span><br><span class="line">expected_loss = 9.82445935443</span><br><span class="line"></span><br><span class="line">print(&#x27;loss: &#x27;, loss)</span><br><span class="line">print(&#x27;expected loss: &#x27;, expected_loss)</span><br><span class="line">print(&#x27;difference: &#x27;, abs(loss - expected_loss))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/9.png" alt="9"></p>
<h3 id="Overfit-LSTM-captioning-model"><a href="#Overfit-LSTM-captioning-model" class="headerlink" title="Overfit LSTM captioning model"></a>Overfit LSTM captioning model</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">small_data = load_coco_data(max_train=50)</span><br><span class="line"></span><br><span class="line">small_lstm_model = CaptioningRNN(</span><br><span class="line">          cell_type=&#x27;lstm&#x27;,</span><br><span class="line">          word_to_idx=data[&#x27;word_to_idx&#x27;],</span><br><span class="line">          input_dim=data[&#x27;train_features&#x27;].shape[1],</span><br><span class="line">          hidden_dim=512,</span><br><span class="line">          wordvec_dim=256,</span><br><span class="line">          dtype=np.float32,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">small_lstm_solver = CaptioningSolver(small_lstm_model, small_data,</span><br><span class="line">           update_rule=&#x27;adam&#x27;,</span><br><span class="line">           num_epochs=50,</span><br><span class="line">           batch_size=25,</span><br><span class="line">           optim_config=&#123;</span><br><span class="line">             &#x27;learning_rate&#x27;: 5e-3,</span><br><span class="line">           &#125;,</span><br><span class="line">           lr_decay=0.995,</span><br><span class="line">           verbose=True, print_every=10,</span><br><span class="line">         )</span><br><span class="line"></span><br><span class="line">small_lstm_solver.train()</span><br><span class="line"></span><br><span class="line"># Plot the training losses</span><br><span class="line">plt.plot(small_lstm_solver.loss_history)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Loss&#x27;)</span><br><span class="line">plt.title(&#x27;Training loss history&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/10.png" alt="10"></p>
<p>后面的内容同RNN需要挂VPN，在下无能为力~~~~</p>
<h1 id="rnn-layers-py"><a href="#rnn-layers-py" class="headerlink" title="rnn_layers.py"></a>rnn_layers.py</h1><p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/3.png" alt="1"></p>
<p>如上图所示，<br>$$<br>a&#x3D;W_x × x_t +W_h × h_t-1 + b<br>$$</p>
<p>$$<br>a_i&#x3D;a[:,0:H]<br>$$</p>
<p>$$<br>i&#x3D;sigmod(a_i)<br>$$</p>
<p>$$<br>a_g&#x3D;a[:,3H:4H]<br>$$</p>
<p>$$<br>g&#x3D;tanh(a_g)<br>$$</p>
<p>$$<br>c_t&#x3D;f<em>c_(t-1) + i</em>g<br>$$</p>
<p>$$<br>h_t&#x3D;o*tanh(c_t)<br>$$</p>
<p>因此比较容易写出代码。</p>
<p>sigmoid的求导如下：</p>
<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/2.png" alt="2"></p>
<p>对tanh的求导如下：</p>
<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/1.png" alt="1"></p>
<p>注意这里的dnext_c是需要累加的，因为是不同的方向获得的dnext_c</p>
<p>LSTM的单步反向传播就是反过来也是比较容易能够写出代码，正文段如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Forward pass for a single timestep of an LSTM.</span><br><span class="line"></span><br><span class="line">    The input data has dimension D, the hidden state has dimension H, and we use</span><br><span class="line">    a minibatch size of N.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data, of shape (N, D)</span><br><span class="line">    - prev_h: Previous hidden state, of shape (N, H)</span><br><span class="line">    - prev_c: previous cell state, of shape (N, H)</span><br><span class="line">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span><br><span class="line">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span><br><span class="line">    - b: Biases, of shape (4H,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - next_h: Next hidden state, of shape (N, H)</span><br><span class="line">    - next_c: Next cell state, of shape (N, H)</span><br><span class="line">    - cache: Tuple of values needed for backward pass.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    next_h, next_c, cache = None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the forward pass for a single timestep of an LSTM.        #</span><br><span class="line">    # You may want to use the numerically stable sigmoid implementation above.  #</span><br><span class="line">    #############################################################################</span><br><span class="line">    next_h, next_c, cache = None, None, None</span><br><span class="line">    # TODO: Implement the forward pass for a single timestep of an LSTM.</span><br><span class="line">    H=prev_h.shape[1]</span><br><span class="line">    a=x.dot(Wx)+prev_h.dot(Wh)+b   #[N,4H]</span><br><span class="line">    i=sigmoid(a[:,0:H])        #[N,H]   </span><br><span class="line">    f=sigmoid(a[:,H:2*H])</span><br><span class="line">    o=sigmoid(a[:,2*H:3*H])</span><br><span class="line">    g=np.tanh(a[:,3*H:4*H])</span><br><span class="line">    next_c=f*prev_c+i*g</span><br><span class="line">    next_h=o*np.tanh(next_c)</span><br><span class="line">    </span><br><span class="line">    cache=next_c,prev_c,g,i,f,o,x,Wh,Wx,prev_h,a</span><br><span class="line"></span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return next_h, next_c, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lstm_step_backward(dnext_h, dnext_c, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Backward pass for a single timestep of an LSTM.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span><br><span class="line">    - dnext_c: Gradients of next cell state, of shape (N, H)</span><br><span class="line">    - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient of input data, of shape (N, D)</span><br><span class="line">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span><br><span class="line">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span><br><span class="line">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span><br><span class="line">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span><br><span class="line">    - db: Gradient of biases, of shape (4H,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dh, dc, dWx, dWh, db = None, None, None, None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the backward pass for a single timestep of an LSTM.       #</span><br><span class="line">    #                                                                           #</span><br><span class="line">    # HINT: For sigmoid and tanh you can compute local derivatives in terms of  #</span><br><span class="line">    # the output value from the nonlinearity.                                   #</span><br><span class="line">    #############################################################################</span><br><span class="line">    H=dnext_h.shape[1]</span><br><span class="line">    next_c,prev_c,g,i,f,o,x,Wh,Wx,prev_h,a=cache</span><br><span class="line">    do=np.tanh(next_c)*dnext_h</span><br><span class="line">    dnext_c+=dnext_h*o*(1-np.tanh(next_c)**2)</span><br><span class="line">    df=dnext_c*prev_c</span><br><span class="line">    di=dnext_c*g</span><br><span class="line">    dg=dnext_c*i</span><br><span class="line">    dprev_c=dnext_c*f</span><br><span class="line">    da=np.zeros_like(a)</span><br><span class="line">    da[:,0:H]=sigmoid(a[:,0:H])*(1-sigmoid(a[:,0:H]))*di</span><br><span class="line">    da[:,H:2*H]=sigmoid(a[:,H:2*H])*(1-sigmoid(a[:,H:2*H]))*df</span><br><span class="line">    da[:,2*H:3*H]=sigmoid(a[:,2*H:3*H])*(1-sigmoid(a[:,2*H:3*H]))*do</span><br><span class="line">    da[:,3*H:4*H]=dg*(1-np.tanh(a[:,3*H:4*H])**2)</span><br><span class="line">    dx=da.dot(Wx.T)   #da[N,4H]  Wx[D,4H]  x[N,D]</span><br><span class="line">    dWx=x.T.dot(da)</span><br><span class="line">    dprev_h=da.dot(Wh.T)</span><br><span class="line">    dWh=prev_h.T.dot(da)</span><br><span class="line">    db=np.sum(da,axis=0)</span><br><span class="line">        </span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return dx, dprev_h, dprev_c, dWx, dWh, db</span><br></pre></td></tr></table></figure>

<p>在LSTM的正反向传播中，正向传播比较容易推导，但是在反向传播中，dh的值一定不能忘记更新！！！</p>
<p>因为这里只求dh0，但是在过程中因为链式规则的原因，会需要更新dh！！！！实际上是与朴素的rnn一样的。</p>
<p>在前向传播中，初始的c_t是为0；在反向传播中，dnext_c同样初始为0！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def lstm_forward(x, h0, Wx, Wh, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span><br><span class="line">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span><br><span class="line">    size of H, and we work over a minibatch containing N sequences. After running</span><br><span class="line">    the LSTM forward, we return the hidden states for all timesteps.</span><br><span class="line"></span><br><span class="line">    Note that the initial cell state is passed as input, but the initial cell</span><br><span class="line">    state is set to zero. Also note that the cell state is not returned; it is</span><br><span class="line">    an internal variable to the LSTM and is not accessed from outside.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data of shape (N, T, D)</span><br><span class="line">    - h0: Initial hidden state of shape (N, H)</span><br><span class="line">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span><br><span class="line">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span><br><span class="line">    - b: Biases of shape (4H,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span><br><span class="line">    - cache: Values needed for the backward pass.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    h, cache = None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the forward pass for an LSTM over an entire timeseries.   #</span><br><span class="line">    # You should use the lstm_step_forward function that you just defined.      #</span><br><span class="line">    #############################################################################</span><br><span class="line">    N,T,D=x.shape</span><br><span class="line">    _,H=h0.shape</span><br><span class="line">    h=np.zeros((N,T,H))</span><br><span class="line">    c=np.zeros_like(h)</span><br><span class="line">    prev_h=h0</span><br><span class="line">    prev_c=np.zeros_like(prev_h)</span><br><span class="line">    cache=[]</span><br><span class="line">    for i in range(T):</span><br><span class="line">        h[:,i,:],c[:,i,:],cac=lstm_step_forward(x[:,i,:],prev_h,prev_c,Wx,Wh,b)</span><br><span class="line">        prev_h=h[:,i,:]</span><br><span class="line">        prev_c=c[:,i,:]</span><br><span class="line">        cache.append(cac)</span><br><span class="line">        </span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lstm_backward(dh, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Backward pass for an LSTM over an entire sequence of data.]</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span><br><span class="line">    - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient of input data of shape (N, T, D)</span><br><span class="line">    - dh0: Gradient of initial hidden state of shape (N, H)</span><br><span class="line">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span><br><span class="line">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span><br><span class="line">    - db: Gradient of biases, of shape (4H,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dh0, dWx, dWh, db = None, None, None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the backward pass for an LSTM over an entire timeseries.  #</span><br><span class="line">    # You should use the lstm_step_backward function that you just defined.     #</span><br><span class="line">    #############################################################################</span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    N,T,H=dh.shape</span><br><span class="line">    D,_=cache[0][0].shape</span><br><span class="line">    dx=np.zeros((N,T,D))</span><br><span class="line">    dh0=np.zeros((N,H))</span><br><span class="line">    dWx=np.zeros((D,4*H))</span><br><span class="line">    dWh=np.zeros((H,4*H))</span><br><span class="line">    db=np.zeros(4*H)</span><br><span class="line">    dnext_c=np.zeros_like(dh0)</span><br><span class="line">    #dprev_c=np.zeros_like(dh0)</span><br><span class="line">    dprev_h=0</span><br><span class="line">    for i in range(T,0,-1):</span><br><span class="line">        dx[:,i-1,:],dprev_h,dnext_c,dWx0,dWh0,db0=lstm_step_backward(dh[:,i-1,:]+dprev_h,dnext_c,cache[i-1])</span><br><span class="line">        dWx+=dWx0</span><br><span class="line">        dWh+=dWh0</span><br><span class="line">        db+=db0</span><br><span class="line">                </span><br><span class="line">    dh0=dprev_h           </span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure>


        </div>
        
<blockquote class="copyright">
    <p><strong>本文链接 : </strong><a class="permalink" href="http://yoursite.com/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/">http://yoursite.com/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/</a></p>
    <p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p>
</blockquote>


    </article>
    
    <section id="comments">
        
    </section>


    

</main>


<aside style="" id="sidebar" class="aside aside-fixture">
    <div class="toc-sidebar">
        <nav id="toc" class="article-toc">
            <h3 class="toc-title">Catalogue</h3>
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">总结：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Image-Captioning-with-LSTMs"><span class="toc-number"></span> <span class="toc-text">Image Captioning with LSTMs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Load-MS-COCO-data"><span class="toc-number">1.</span> <span class="toc-text">Load MS-COCO data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM"><span class="toc-number">2.</span> <span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-step-forward"><span class="toc-number">3.</span> <span class="toc-text">LSTM: step forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-step-backward"><span class="toc-number">4.</span> <span class="toc-text">LSTM: step backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-forward"><span class="toc-number">5.</span> <span class="toc-text">LSTM: forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-backward"><span class="toc-number">6.</span> <span class="toc-text">LSTM: backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-captioning-model"><span class="toc-number">7.</span> <span class="toc-text">LSTM captioning model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overfit-LSTM-captioning-model"><span class="toc-number">8.</span> <span class="toc-text">Overfit LSTM captioning model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn-layers-py"><span class="toc-number"></span> <span class="toc-text">rnn_layers.py</span></a>
        </nav>
    </div>
</aside>





        </section>
        <footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40">
    
    <div class="footer-social-links">
        
            <a target="_blank" rel="noopener" href="https://github.com/fengkx">
                <i class="iconfont icon-github"></i>
            </a>
        
            <a target="_blank" rel="noopener" href="https://t.me/fengkx">
                <i class="iconfont icon-telegram"></i>
            </a>
        
            <a target="_blank" rel="noopener" href="https://twitter.com/example">
                <i class="iconfont icon-twitter"></i>
            </a>
        
            <a href="/atom.xml">
                <i class="iconfont icon-rss"></i>
            </a>
        
    </div>
    
    
</footer>

        <div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div>
        <div id="search-view-container" class="hidden shadow-xl"></div>
        
<script src="/js/dom-event.min.js"></script>



<script src="/js/local-search.min.js"></script>



    <script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script>
    
<script src="/js/light-gallery.min.js"></script>






    </body>
</html>
