<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tukekenulia♥</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="总结：本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tukekenulia♥">
<meta property="og:url" content="http://yoursite.com/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/index.html">
<meta property="og:site_name" content="Tukekenulia♥">
<meta property="og:description" content="总结：本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。">
<meta property="og:locale">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/5.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/6.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/7.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/8.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/9.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/10.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/3.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/2.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/1.png">
<meta property="article:published_time" content="2020-07-11T08:07:42.000Z">
<meta property="article:modified_time" content="2020-07-11T08:07:42.000Z">
<meta property="article:author" content="Tukeke">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/css/scrollUp/image.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<meta name="generator" content="Hexo 6.3.0"></head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <div class="logo">
        <img src="/logo.png" alt="Profile Picture">
      </div>
      <div id="title">Tukekenulia♥</div>
      
        <div id="subtitle">Stay focus,stay humble,stay curiosity.</div>
      
       <ul class="my-socials">
  
  <li>
  	<a href="https://github.com/Tukekehaohaonuli" class="github" target="_blank">
  		<i class="fa fa-github"></i>
  	</a>
  </li>
  
  <li>
  	<a href="YOUR WEIBO HOME PAGE URL" class="weibo" target="_blank">
  		<i class="fa fa-weibo"></i>
  	</a>
  </li>
  
 
</ul>
    </div>
  </div>
  <div id="header-inner" class="">
    <nav id="main-nav">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <!--
        
          
            <a class="main-nav-link" href="/">首页</a>
          
            <a class="main-nav-link" href="/categories/life">生活</a>
          
            <a class="main-nav-link" href="/archives">归档</a>
          
        
      -->
    </nav>
    <nav id="title-nav" style="display:none">
      <a href="/">Tukekenulia♥</a>
      <img src="/logo.png" alt="Profile Picture">
      <!--
      <span id="title-nav-socials">
        
       
     </span>
      -->
    </nav>
    <nav id="sub-nav">
      
      <a id="nav-search-btn" class="nav-icon" title="Search"></a>
    </nav>
    <div id="search-form-wrap">
      <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
        <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
        <input type="submit" value="" class="search-form-submit">
        <input name=tn type=hidden value="bds">
        <input name=cl type=hidden value="3">
        <input name=ct type=hidden value="2097152">
        <input type="hidden" name="si" value="yoursite.com">
      </form>
    </div>
  </div>
  <div class="site-nav" style="display: none;">
    <ul>
      
      
        <li><a href="/">首页</a></li>
      
        <li><a href="/categories/life">生活</a></li>
      
        <li><a href="/archives">归档</a></li>
      
      
    </ul>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CS231n_Assignment3_Image_Captioning_with_LSTMs" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/" class="article-date">
  <time datetime="2020-07-11T08:07:42.000Z" itemprop="datePublished">2020-07-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>本文完成LSTM方式的RNN循环神经网络，他与朴素的RNN方式只是内容更新的方式不同，多了一个隐藏状态c_t，其他与朴素的RNN完全相同。此方法解决了朴素的RNN中会出现的梯度爆炸以及梯度消失的问题。</p>
<span id="more"></span>

<p>后面3个作业都是要用到TensorFlow或Pytorch框架来写，因此暂且搁置，等学完框架可能还会再回来的哈哈哈，目前CS231n assignment作业到此完结，关于CS231N的视频课程，在lecture 14后看的不是很清晰，以后如果还有机会使用到这个部分的内容，可以再回来深究，CS231N的学习历程暂告一段落，未完待续…</p>
<h1 id="Image-Captioning-with-LSTMs"><a href="#Image-Captioning-with-LSTMs" class="headerlink" title="Image Captioning with LSTMs"></a>Image Captioning with LSTMs</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># As usual, a bit of setup</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import time, os, json</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array</span><br><span class="line">from cs231n.rnn_layers import *</span><br><span class="line">from cs231n.captioning_solver import CaptioningSolver</span><br><span class="line">from cs231n.classifiers.rnn import CaptioningRNN</span><br><span class="line">from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions</span><br><span class="line">from cs231n.image_utils import image_from_url</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[&#x27;figure.figsize&#x27;] = (10.0, 8.0) # set default size of plots</span><br><span class="line">plt.rcParams[&#x27;image.interpolation&#x27;] = &#x27;nearest&#x27;</span><br><span class="line">plt.rcParams[&#x27;image.cmap&#x27;] = &#x27;gray&#x27;</span><br><span class="line"></span><br><span class="line"># for auto-reloading external modules</span><br><span class="line"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload 2</span><br><span class="line"></span><br><span class="line">def rel_error(x, y):</span><br><span class="line">    &quot;&quot;&quot; returns relative error &quot;&quot;&quot;</span><br><span class="line">    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))</span><br></pre></td></tr></table></figure>

<h3 id="Load-MS-COCO-data"><a href="#Load-MS-COCO-data" class="headerlink" title="Load MS-COCO data"></a>Load MS-COCO data</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Load COCO data from disk; this returns a dictionary</span><br><span class="line"># We&#x27;ll work with dimensionality-reduced features for this notebook, but feel</span><br><span class="line"># free to experiment with the original features by changing the flag below.</span><br><span class="line">data = load_coco_data(pca_features=True)</span><br><span class="line"></span><br><span class="line"># Print out all the keys and values from the data dictionary</span><br><span class="line">for k, v in data.items():</span><br><span class="line">    if type(v) == np.ndarray:</span><br><span class="line">        print(k, type(v), v.shape, v.dtype)</span><br><span class="line">    else:</span><br><span class="line">        print(k, type(v), len(v))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/4.png" alt="4"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>在rnn_layers.py中完成lstm_step_forward以及反向LSTM的反向单步传播，并在这里检验误差。</p>
<h3 id="LSTM-step-forward"><a href="#LSTM-step-forward" class="headerlink" title="LSTM: step forward"></a>LSTM: step forward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">N, D, H = 3, 4, 5</span><br><span class="line">x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)</span><br><span class="line">prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)</span><br><span class="line">prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)</span><br><span class="line">Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)</span><br><span class="line">Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)</span><br><span class="line">b = np.linspace(0.3, 0.7, num=4*H)</span><br><span class="line"></span><br><span class="line">next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">expected_next_h = np.asarray([</span><br><span class="line">    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],</span><br><span class="line">    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],</span><br><span class="line">    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])</span><br><span class="line">expected_next_c = np.asarray([</span><br><span class="line">    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],</span><br><span class="line">    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],</span><br><span class="line">    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])</span><br><span class="line"></span><br><span class="line">print(&#x27;next_h error: &#x27;, rel_error(expected_next_h, next_h))</span><br><span class="line">print(&#x27;next_c error: &#x27;, rel_error(expected_next_c, next_c))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/5.png" alt="5"></p>
<h3 id="LSTM-step-backward"><a href="#LSTM-step-backward" class="headerlink" title="LSTM: step backward"></a>LSTM: step backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">N, D, H = 4, 5, 6</span><br><span class="line">x = np.random.randn(N, D)</span><br><span class="line">prev_h = np.random.randn(N, H)</span><br><span class="line">prev_c = np.random.randn(N, H)</span><br><span class="line">Wx = np.random.randn(D, 4 * H)</span><br><span class="line">Wh = np.random.randn(H, 4 * H)</span><br><span class="line">b = np.random.randn(4 * H)</span><br><span class="line"></span><br><span class="line">next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">dnext_h = np.random.randn(*next_h.shape)</span><br><span class="line">dnext_c = np.random.randn(*next_c.shape)</span><br><span class="line"></span><br><span class="line">fx_h = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fh_h = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fc_h = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fWx_h = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fWh_h = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line">fb_h = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]</span><br><span class="line"></span><br><span class="line">fx_c = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fh_c = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fc_c = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fWx_c = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fWh_c = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line">fb_c = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]</span><br><span class="line"></span><br><span class="line">num_grad = eval_numerical_gradient_array</span><br><span class="line"></span><br><span class="line">dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)</span><br><span class="line">dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)</span><br><span class="line">dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)</span><br><span class="line">dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)</span><br><span class="line">dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)</span><br><span class="line">db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)</span><br><span class="line"></span><br><span class="line">dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)</span><br><span class="line"></span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dh error: &#x27;, rel_error(dh_num, dh))</span><br><span class="line">print(&#x27;dc error: &#x27;, rel_error(dc_num, dc))</span><br><span class="line">print(&#x27;dWx error: &#x27;, rel_error(dWx_num, dWx))</span><br><span class="line">print(&#x27;dWh error: &#x27;, rel_error(dWh_num, dWh))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/6.png" alt="6"></p>
<h3 id="LSTM-forward"><a href="#LSTM-forward" class="headerlink" title="LSTM: forward"></a>LSTM: forward</h3><p>在rnn_layers.py中完成Lstm的正反向传播，并在此检验误差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N, D, H, T = 2, 5, 4, 3</span><br><span class="line">x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)</span><br><span class="line">h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)</span><br><span class="line">Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)</span><br><span class="line">Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)</span><br><span class="line">b = np.linspace(0.2, 0.7, num=4*H)</span><br><span class="line"></span><br><span class="line">h, cache = lstm_forward(x, h0, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">expected_h = np.asarray([</span><br><span class="line"> [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],</span><br><span class="line">  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],</span><br><span class="line">  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],</span><br><span class="line"> [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],</span><br><span class="line">  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],</span><br><span class="line">  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])</span><br><span class="line"></span><br><span class="line">print(&#x27;h error: &#x27;, rel_error(expected_h, h))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/7.png" alt="7"></p>
<h3 id="LSTM-backward"><a href="#LSTM-backward" class="headerlink" title="LSTM: backward"></a>LSTM: backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.rnn_layers import lstm_forward, lstm_backward</span><br><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">N, D, T, H = 2, 3, 10, 6</span><br><span class="line"></span><br><span class="line">x = np.random.randn(N, T, D)</span><br><span class="line">h0 = np.random.randn(N, H)</span><br><span class="line">Wx = np.random.randn(D, 4 * H)</span><br><span class="line">Wh = np.random.randn(H, 4 * H)</span><br><span class="line">b = np.random.randn(4 * H)</span><br><span class="line"></span><br><span class="line">out, cache = lstm_forward(x, h0, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">dout = np.random.randn(*out.shape)</span><br><span class="line"></span><br><span class="line">dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)</span><br><span class="line"></span><br><span class="line">fx = lambda x: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fh0 = lambda h0: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fWh = lambda Wh: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line">fb = lambda b: lstm_forward(x, h0, Wx, Wh, b)[0]</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(fx, x, dout)</span><br><span class="line">dh0_num = eval_numerical_gradient_array(fh0, h0, dout)</span><br><span class="line">dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)</span><br><span class="line">dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(fb, b, dout)</span><br><span class="line"></span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dh0 error: &#x27;, rel_error(dh0_num, dh0))</span><br><span class="line">print(&#x27;dWx error: &#x27;, rel_error(dWx_num, dWx))</span><br><span class="line">print(&#x27;dWh error: &#x27;, rel_error(dWh_num, dWh))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/8.png" alt="8"></p>
<h3 id="LSTM-captioning-model"><a href="#LSTM-captioning-model" class="headerlink" title="LSTM captioning model"></a>LSTM captioning model</h3><p>完成lstm的loss函数和sample函数，实际上是与朴素的RNN算法完全一致，只不过就是里面的内部隐藏状态多了一个c_t，但是外部的操作完全一致，内部的操作已经体现在lstm_forward中，因此这边只需要在rnn.py中添加一个判断类型的语句即可。这里就不贴代码了~~~~</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">N, D, W, H = 10, 20, 30, 40</span><br><span class="line">word_to_idx = &#123;&#x27;&lt;NULL&gt;&#x27;: 0, &#x27;cat&#x27;: 2, &#x27;dog&#x27;: 3&#125;</span><br><span class="line">V = len(word_to_idx)</span><br><span class="line">T = 13</span><br><span class="line"></span><br><span class="line">model = CaptioningRNN(word_to_idx,</span><br><span class="line">          input_dim=D,</span><br><span class="line">          wordvec_dim=W,</span><br><span class="line">          hidden_dim=H,</span><br><span class="line">          cell_type=&#x27;lstm&#x27;,</span><br><span class="line">          dtype=np.float64)</span><br><span class="line"></span><br><span class="line"># Set all model parameters to fixed values</span><br><span class="line">for k, v in model.params.items():</span><br><span class="line">  model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)</span><br><span class="line"></span><br><span class="line">features = np.linspace(-0.5, 1.7, num=N*D).reshape(N, D)</span><br><span class="line">captions = (np.arange(N * T) % V).reshape(N, T)</span><br><span class="line"></span><br><span class="line">loss, grads = model.loss(features, captions)</span><br><span class="line">expected_loss = 9.82445935443</span><br><span class="line"></span><br><span class="line">print(&#x27;loss: &#x27;, loss)</span><br><span class="line">print(&#x27;expected loss: &#x27;, expected_loss)</span><br><span class="line">print(&#x27;difference: &#x27;, abs(loss - expected_loss))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/9.png" alt="9"></p>
<h3 id="Overfit-LSTM-captioning-model"><a href="#Overfit-LSTM-captioning-model" class="headerlink" title="Overfit LSTM captioning model"></a>Overfit LSTM captioning model</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line"></span><br><span class="line">small_data = load_coco_data(max_train=50)</span><br><span class="line"></span><br><span class="line">small_lstm_model = CaptioningRNN(</span><br><span class="line">          cell_type=&#x27;lstm&#x27;,</span><br><span class="line">          word_to_idx=data[&#x27;word_to_idx&#x27;],</span><br><span class="line">          input_dim=data[&#x27;train_features&#x27;].shape[1],</span><br><span class="line">          hidden_dim=512,</span><br><span class="line">          wordvec_dim=256,</span><br><span class="line">          dtype=np.float32,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">small_lstm_solver = CaptioningSolver(small_lstm_model, small_data,</span><br><span class="line">           update_rule=&#x27;adam&#x27;,</span><br><span class="line">           num_epochs=50,</span><br><span class="line">           batch_size=25,</span><br><span class="line">           optim_config=&#123;</span><br><span class="line">             &#x27;learning_rate&#x27;: 5e-3,</span><br><span class="line">           &#125;,</span><br><span class="line">           lr_decay=0.995,</span><br><span class="line">           verbose=True, print_every=10,</span><br><span class="line">         )</span><br><span class="line"></span><br><span class="line">small_lstm_solver.train()</span><br><span class="line"></span><br><span class="line"># Plot the training losses</span><br><span class="line">plt.plot(small_lstm_solver.loss_history)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Loss&#x27;)</span><br><span class="line">plt.title(&#x27;Training loss history&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/10.png" alt="10"></p>
<p>后面的内容同RNN需要挂VPN，在下无能为力~~~~</p>
<h1 id="rnn-layers-py"><a href="#rnn-layers-py" class="headerlink" title="rnn_layers.py"></a>rnn_layers.py</h1><p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/3.png" alt="1"></p>
<p>如上图所示，<br>$$<br>a&#x3D;W_x × x_t +W_h × h_t-1 + b<br>$$</p>
<p>$$<br>a_i&#x3D;a[:,0:H]<br>$$</p>
<p>$$<br>i&#x3D;sigmod(a_i)<br>$$</p>
<p>$$<br>a_g&#x3D;a[:,3H:4H]<br>$$</p>
<p>$$<br>g&#x3D;tanh(a_g)<br>$$</p>
<p>$$<br>c_t&#x3D;f<em>c_(t-1) + i</em>g<br>$$</p>
<p>$$<br>h_t&#x3D;o*tanh(c_t)<br>$$</p>
<p>因此比较容易写出代码。</p>
<p>sigmoid的求导如下：</p>
<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/2.png" alt="2"></p>
<p>对tanh的求导如下：</p>
<p><img src="/images/CS231n_Assignment3_Image_Captioning_with_LSTMs/1.png" alt="1"></p>
<p>注意这里的dnext_c是需要累加的，因为是不同的方向获得的dnext_c</p>
<p>LSTM的单步反向传播就是反过来也是比较容易能够写出代码，正文段如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Forward pass for a single timestep of an LSTM.</span><br><span class="line"></span><br><span class="line">    The input data has dimension D, the hidden state has dimension H, and we use</span><br><span class="line">    a minibatch size of N.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data, of shape (N, D)</span><br><span class="line">    - prev_h: Previous hidden state, of shape (N, H)</span><br><span class="line">    - prev_c: previous cell state, of shape (N, H)</span><br><span class="line">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span><br><span class="line">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span><br><span class="line">    - b: Biases, of shape (4H,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - next_h: Next hidden state, of shape (N, H)</span><br><span class="line">    - next_c: Next cell state, of shape (N, H)</span><br><span class="line">    - cache: Tuple of values needed for backward pass.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    next_h, next_c, cache = None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the forward pass for a single timestep of an LSTM.        #</span><br><span class="line">    # You may want to use the numerically stable sigmoid implementation above.  #</span><br><span class="line">    #############################################################################</span><br><span class="line">    next_h, next_c, cache = None, None, None</span><br><span class="line">    # TODO: Implement the forward pass for a single timestep of an LSTM.</span><br><span class="line">    H=prev_h.shape[1]</span><br><span class="line">    a=x.dot(Wx)+prev_h.dot(Wh)+b   #[N,4H]</span><br><span class="line">    i=sigmoid(a[:,0:H])        #[N,H]   </span><br><span class="line">    f=sigmoid(a[:,H:2*H])</span><br><span class="line">    o=sigmoid(a[:,2*H:3*H])</span><br><span class="line">    g=np.tanh(a[:,3*H:4*H])</span><br><span class="line">    next_c=f*prev_c+i*g</span><br><span class="line">    next_h=o*np.tanh(next_c)</span><br><span class="line">    </span><br><span class="line">    cache=next_c,prev_c,g,i,f,o,x,Wh,Wx,prev_h,a</span><br><span class="line"></span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return next_h, next_c, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lstm_step_backward(dnext_h, dnext_c, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Backward pass for a single timestep of an LSTM.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span><br><span class="line">    - dnext_c: Gradients of next cell state, of shape (N, H)</span><br><span class="line">    - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient of input data, of shape (N, D)</span><br><span class="line">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span><br><span class="line">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span><br><span class="line">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span><br><span class="line">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span><br><span class="line">    - db: Gradient of biases, of shape (4H,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dh, dc, dWx, dWh, db = None, None, None, None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the backward pass for a single timestep of an LSTM.       #</span><br><span class="line">    #                                                                           #</span><br><span class="line">    # HINT: For sigmoid and tanh you can compute local derivatives in terms of  #</span><br><span class="line">    # the output value from the nonlinearity.                                   #</span><br><span class="line">    #############################################################################</span><br><span class="line">    H=dnext_h.shape[1]</span><br><span class="line">    next_c,prev_c,g,i,f,o,x,Wh,Wx,prev_h,a=cache</span><br><span class="line">    do=np.tanh(next_c)*dnext_h</span><br><span class="line">    dnext_c+=dnext_h*o*(1-np.tanh(next_c)**2)</span><br><span class="line">    df=dnext_c*prev_c</span><br><span class="line">    di=dnext_c*g</span><br><span class="line">    dg=dnext_c*i</span><br><span class="line">    dprev_c=dnext_c*f</span><br><span class="line">    da=np.zeros_like(a)</span><br><span class="line">    da[:,0:H]=sigmoid(a[:,0:H])*(1-sigmoid(a[:,0:H]))*di</span><br><span class="line">    da[:,H:2*H]=sigmoid(a[:,H:2*H])*(1-sigmoid(a[:,H:2*H]))*df</span><br><span class="line">    da[:,2*H:3*H]=sigmoid(a[:,2*H:3*H])*(1-sigmoid(a[:,2*H:3*H]))*do</span><br><span class="line">    da[:,3*H:4*H]=dg*(1-np.tanh(a[:,3*H:4*H])**2)</span><br><span class="line">    dx=da.dot(Wx.T)   #da[N,4H]  Wx[D,4H]  x[N,D]</span><br><span class="line">    dWx=x.T.dot(da)</span><br><span class="line">    dprev_h=da.dot(Wh.T)</span><br><span class="line">    dWh=prev_h.T.dot(da)</span><br><span class="line">    db=np.sum(da,axis=0)</span><br><span class="line">        </span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return dx, dprev_h, dprev_c, dWx, dWh, db</span><br></pre></td></tr></table></figure>

<p>在LSTM的正反向传播中，正向传播比较容易推导，但是在反向传播中，dh的值一定不能忘记更新！！！</p>
<p>因为这里只求dh0，但是在过程中因为链式规则的原因，会需要更新dh！！！！实际上是与朴素的rnn一样的。</p>
<p>在前向传播中，初始的c_t是为0；在反向传播中，dnext_c同样初始为0！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def lstm_forward(x, h0, Wx, Wh, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span><br><span class="line">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span><br><span class="line">    size of H, and we work over a minibatch containing N sequences. After running</span><br><span class="line">    the LSTM forward, we return the hidden states for all timesteps.</span><br><span class="line"></span><br><span class="line">    Note that the initial cell state is passed as input, but the initial cell</span><br><span class="line">    state is set to zero. Also note that the cell state is not returned; it is</span><br><span class="line">    an internal variable to the LSTM and is not accessed from outside.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: Input data of shape (N, T, D)</span><br><span class="line">    - h0: Initial hidden state of shape (N, H)</span><br><span class="line">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span><br><span class="line">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span><br><span class="line">    - b: Biases of shape (4H,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span><br><span class="line">    - cache: Values needed for the backward pass.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    h, cache = None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the forward pass for an LSTM over an entire timeseries.   #</span><br><span class="line">    # You should use the lstm_step_forward function that you just defined.      #</span><br><span class="line">    #############################################################################</span><br><span class="line">    N,T,D=x.shape</span><br><span class="line">    _,H=h0.shape</span><br><span class="line">    h=np.zeros((N,T,H))</span><br><span class="line">    c=np.zeros_like(h)</span><br><span class="line">    prev_h=h0</span><br><span class="line">    prev_c=np.zeros_like(prev_h)</span><br><span class="line">    cache=[]</span><br><span class="line">    for i in range(T):</span><br><span class="line">        h[:,i,:],c[:,i,:],cac=lstm_step_forward(x[:,i,:],prev_h,prev_c,Wx,Wh,b)</span><br><span class="line">        prev_h=h[:,i,:]</span><br><span class="line">        prev_c=c[:,i,:]</span><br><span class="line">        cache.append(cac)</span><br><span class="line">        </span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lstm_backward(dh, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Backward pass for an LSTM over an entire sequence of data.]</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span><br><span class="line">    - cache: Values from the forward pass</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient of input data of shape (N, T, D)</span><br><span class="line">    - dh0: Gradient of initial hidden state of shape (N, H)</span><br><span class="line">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span><br><span class="line">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span><br><span class="line">    - db: Gradient of biases, of shape (4H,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, dh0, dWx, dWh, db = None, None, None, None, None</span><br><span class="line">    #############################################################################</span><br><span class="line">    # TODO: Implement the backward pass for an LSTM over an entire timeseries.  #</span><br><span class="line">    # You should use the lstm_step_backward function that you just defined.     #</span><br><span class="line">    #############################################################################</span><br><span class="line">    pass</span><br><span class="line">    ##############################################################################</span><br><span class="line">    N,T,H=dh.shape</span><br><span class="line">    D,_=cache[0][0].shape</span><br><span class="line">    dx=np.zeros((N,T,D))</span><br><span class="line">    dh0=np.zeros((N,H))</span><br><span class="line">    dWx=np.zeros((D,4*H))</span><br><span class="line">    dWh=np.zeros((H,4*H))</span><br><span class="line">    db=np.zeros(4*H)</span><br><span class="line">    dnext_c=np.zeros_like(dh0)</span><br><span class="line">    #dprev_c=np.zeros_like(dh0)</span><br><span class="line">    dprev_h=0</span><br><span class="line">    for i in range(T,0,-1):</span><br><span class="line">        dx[:,i-1,:],dprev_h,dnext_c,dWx0,dWh0,db0=lstm_step_backward(dh[:,i-1,:]+dprev_h,dnext_c,cache[i-1])</span><br><span class="line">        dWx+=dWx0</span><br><span class="line">        dWh+=dWh0</span><br><span class="line">        db+=db0</span><br><span class="line">                </span><br><span class="line">    dh0=dprev_h           </span><br><span class="line">    #                               END OF YOUR CODE                             #</span><br><span class="line">    ##############################################################################</span><br><span class="line"></span><br><span class="line">    return dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2020/07/11/CS231n_Assignment3_Image_Captioning_with_LSTMs/" data-id="clkwhj3ty000c6s7sbky7hzek" class="article-share-link">Share</a>
      

      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/07/11/CS231n_Assignment1_Softmax/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2020/07/10/CS231n_Assignment3_Image_Captioning_with_RNNs/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Tukeke<br>
      Theme <a href="https://github.com/Tukekehaohaonuli/" target="_blank">Oishi</a>, Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <!--
      <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/categories/life" class="mobile-nav-link">生活</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
</nav>
    -->
    

<!-- 百度分享 start -->

<!-- 百度分享 end -->

<script src="//cdn.bootcss.com/jquery/1.11.1/jquery.min.js"></script>




<script src="/js/jquery.scrollUp.min.js"></script>


<script src="/js/jquery.transform.js"></script>


<script src="/js/menu.js"></script>



<script src="/js/script.js"></script>


<script src="/js/scrollUp.js"></script>


  </div>
</body>
</html>