<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tukekenulia♥</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&#x2F;&#x2F;本章主要讲述模块化的思想，即写成一个个的函数，然后各层整合时调用 &#x2F;&#x2F;完成4种梯度下降算法，并测试表现。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tukekenulia♥">
<meta property="og:url" content="http://yoursite.com/2020/08/05/CS231n_Assignment2_Fully-Connected_Neural_Nets/index.html">
<meta property="og:site_name" content="Tukekenulia♥">
<meta property="og:description" content="&#x2F;&#x2F;本章主要讲述模块化的思想，即写成一个个的函数，然后各层整合时调用 &#x2F;&#x2F;完成4种梯度下降算法，并测试表现。">
<meta property="og:locale">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/1.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/2.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/3.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/4.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/5.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/6.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/7.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/8.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/9.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/10.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/11.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/12.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/13.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/14.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/15.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/16.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/17.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/18.png">
<meta property="og:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/19.png">
<meta property="og:image" content="c:\Users\yuyuyyu\Tukekehaohaonuli.github.io\source\images\CS231n_Assignment2_Fully-Connected_Neural_Nets\20.png">
<meta property="og:image" content="c:\Users\yuyuyyu\Tukekehaohaonuli.github.io\source\images\CS231n_Assignment2_Fully-Connected_Neural_Nets\21.png">
<meta property="article:published_time" content="2020-08-05T01:14:10.000Z">
<meta property="article:modified_time" content="2020-08-05T01:14:10.000Z">
<meta property="article:author" content="Tukeke">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/1.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/css/scrollUp/image.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<meta name="generator" content="Hexo 6.3.0"></head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <div class="logo">
        <img src="/logo.png" alt="Profile Picture">
      </div>
      <div id="title">Tukekenulia♥</div>
      
        <div id="subtitle">Stay focus,stay humble,stay curiosity.</div>
      
       <ul class="my-socials">
  
  <li>
  	<a href="https://github.com/Tukekehaohaonuli" class="github" target="_blank">
  		<i class="fa fa-github"></i>
  	</a>
  </li>
  
  <li>
  	<a href="YOUR WEIBO HOME PAGE URL" class="weibo" target="_blank">
  		<i class="fa fa-weibo"></i>
  	</a>
  </li>
  
 
</ul>
    </div>
  </div>
  <div id="header-inner" class="">
    <nav id="main-nav">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <!--
        
          
            <a class="main-nav-link" href="/">首页</a>
          
            <a class="main-nav-link" href="/categories/life">生活</a>
          
            <a class="main-nav-link" href="/archives">归档</a>
          
        
      -->
    </nav>
    <nav id="title-nav" style="display:none">
      <a href="/">Tukekenulia♥</a>
      <img src="/logo.png" alt="Profile Picture">
      <!--
      <span id="title-nav-socials">
        
       
     </span>
      -->
    </nav>
    <nav id="sub-nav">
      
      <a id="nav-search-btn" class="nav-icon" title="Search"></a>
    </nav>
    <div id="search-form-wrap">
      <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
        <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
        <input type="submit" value="" class="search-form-submit">
        <input name=tn type=hidden value="bds">
        <input name=cl type=hidden value="3">
        <input name=ct type=hidden value="2097152">
        <input type="hidden" name="si" value="yoursite.com">
      </form>
    </div>
  </div>
  <div class="site-nav" style="display: none;">
    <ul>
      
      
        <li><a href="/">首页</a></li>
      
        <li><a href="/categories/life">生活</a></li>
      
        <li><a href="/archives">归档</a></li>
      
      
    </ul>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CS231n_Assignment2_Fully-Connected_Neural_Nets" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/05/CS231n_Assignment2_Fully-Connected_Neural_Nets/" class="article-date">
  <time datetime="2020-08-05T01:14:10.000Z" itemprop="datePublished">2020-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>&#x2F;&#x2F;本章主要讲述模块化的思想，即写成一个个的函数，然后各层整合时调用</p>
<p>&#x2F;&#x2F;完成4种梯度下降算法，并测试表现。</p>
<span id="more"></span>

<h3 id="一些课程上的总结写在前面："><a href="#一些课程上的总结写在前面：" class="headerlink" title="一些课程上的总结写在前面："></a>一些课程上的总结写在前面：</h3><p>1.激活函数如sigmod等，代价函数是所有损失函数的平均值</p>
<p>2.Xavier初始化W，需要W权重初始化合理的原因是因为，如果W初始化时很小，倘若输入的X值也很小，就会导致梯度为0，W权重的更新不会变化；如果W初始化很大时，W*X+b就会很大，在经过激活函数处理时，就会导致会出现饱和状态，这种状态下的Loss[i]值经过激活函数处理就会出现极端值（在tanh中呈现1或-1），此时的梯度值就会趋于0</p>
<p>3.归零化的原因是当归零化之后损失函数对于参数的变化不在那么敏感，稍微变动参数不会有太大的影响，因此需要归零化，归零化之后均值为0方差为1，如果不归零化的情况下，但凡对参数有改动，对于整层的输出都会有较大的改动</p>
<p>4.带动量的随机梯度下降算法换元的原因是因为在增加动量之后对W进行求导之后的dW的关系就会变得非常复杂，不能够很好的应用，在通过换元操作之后，dW的关系就会变的简单明了且不会对原来dW的关系造成改变，即从Sdg_Momentum改进为Nesterov Momentum</p>
<p>5.另一个梯度下降修正的方法是AdaGrad，他的缺点是在接近梯度最小值时，会收敛的越来越慢，在凸函数中表现较好，凹函数表现比较差。因此改进的方法是RMSProp方法.</p>
<p>6.L-BFGS是二阶函数逼近</p>
<p>7.集成技术就是训练多个模型，然后取模型的平均值 可以提高在测试集上的准确率，减少过拟合</p>
<p>8.Dropout会使得训练时消耗更多的时间，但是拥有的模型鲁棒性更佳,dropout是正则化优化的部分</p>
<p>9.迁移学习是当你的数据集很少的时候，改变最后的全连接层从而获得较好的效果</p>
<p>10.加入回归惩罚项使函数逼近期望的函数或者降幂等</p>
<p>11.正则化的概念就是希望在测试集上表现良好而并不关心训练集上的表现如何，在损失函数中添加一个正则项，鼓励模型以某种方式选择更简单的W</p>
<p>12.增加模型规模而仍然采用经典算法同样能获得好的效果</p>
<p>13.神经网络中的层数增加越到后面效果越不明显，但是卷积神经网络不同，他从不同的角度提取了不同的特征值，因此卷积神经网络的深度非常的重要</p>
<p>14.Adam方法是可以作为默认梯度下降算法的良好梯度下降算法</p>
<p>15.softmax的损失已经作为分类问题的标准方案了。</p>
<h1 id="非常重要的错误"><a href="#非常重要的错误" class="headerlink" title="非常重要的错误"></a>非常重要的错误</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">****** X[N,d1,d2,```d_k]  dw[D,M]  db[M,]   N=10 D=6</span><br><span class="line">N = x.shape[0]       //N=N               //x[10,2,3]</span><br><span class="line">x_reshape = x.reshape([N, -1]) # [N, D]  //****x=[10,6]</span><br><span class="line">dx = dout.dot(w.T).reshape(*x.shape)     //dout[N,M] w[D,M] dx=[N,D]</span><br><span class="line">dw = (x_reshape.T).dot(dout)             //dw=[6,10]*[10,M]</span><br><span class="line">db = np.sum(dout, axis=0)           </span><br><span class="line"></span><br><span class="line">以下是错误写法!!!</span><br><span class="line">    N=x.shape[0]           //x[10,2,3]</span><br><span class="line">    dx=dout.dot(w.T)       //dx[N,D]=dout[N,M]*[M,D]     </span><br><span class="line">    dx=dx.reshape(*x.shape)   //dx=[10,2,3]</span><br><span class="line">    dw=(x.T).dot(dout)      //dw=[2,3,10]*[N,M]!!!!!!因此错误</span><br><span class="line">    db=np.sum(dout,axis=0)</span><br></pre></td></tr></table></figure>



<h1 id="Fully-Connected-Neural-Nets"><a href="#Fully-Connected-Neural-Nets" class="headerlink" title="Fully-Connected_Neural_Nets"></a>Fully-Connected_Neural_Nets</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># As usual, a bit of setup</span><br><span class="line">from __future__ import print_function</span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from cs231n.classifiers.fc_net import *</span><br><span class="line">from cs231n.data_utils import get_CIFAR10_data</span><br><span class="line">from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array</span><br><span class="line">from cs231n.solver import Solver</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[&#x27;figure.figsize&#x27;] = (10.0, 8.0) # set default size of plots</span><br><span class="line">plt.rcParams[&#x27;image.interpolation&#x27;] = &#x27;nearest&#x27;</span><br><span class="line">plt.rcParams[&#x27;image.cmap&#x27;] = &#x27;gray&#x27;</span><br><span class="line"></span><br><span class="line"># for auto-reloading external modules</span><br><span class="line"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload 2</span><br><span class="line"></span><br><span class="line">def rel_error(x, y): //输出相对错误</span><br><span class="line">  &quot;&quot;&quot; returns relative error &quot;&quot;&quot;</span><br><span class="line">  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/1.png" alt="1"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Load the (preprocessed) CIFAR10 data.</span><br><span class="line"></span><br><span class="line">data = get_CIFAR10_data()</span><br><span class="line">for k, v in list(data.items()):</span><br><span class="line">  print((&#x27;%s: &#x27; % k, v.shape))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/2.png" alt="2"></p>
<h3 id="Affine-layer-foward"><a href="#Affine-layer-foward" class="headerlink" title="Affine layer: foward"></a>Affine layer: foward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Test the affine_forward function</span><br><span class="line"></span><br><span class="line">num_inputs = 2</span><br><span class="line">input_shape = (4, 5, 6)</span><br><span class="line">output_dim = 3</span><br><span class="line"></span><br><span class="line">input_size = num_inputs * np.prod(input_shape)</span><br><span class="line">weight_size = output_dim * np.prod(input_shape)</span><br><span class="line">#print(input_size)</span><br><span class="line">x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)</span><br><span class="line">w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)</span><br><span class="line">b = np.linspace(-0.3, 0.1, num=output_dim)</span><br><span class="line"></span><br><span class="line">out, _ = affine_forward(x, w, b)</span><br><span class="line">correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],</span><br><span class="line">                        [ 3.25553199,  3.5141327,   3.77273342]])</span><br><span class="line"></span><br><span class="line"># Compare your output with ours. The error should be around 1e-9.</span><br><span class="line">print(&#x27;Testing affine_forward function:&#x27;)</span><br><span class="line">print(&#x27;difference: &#x27;, rel_error(out, correct_out))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/3.png" alt="3"></p>
<h3 id="Affine-layer-backward"><a href="#Affine-layer-backward" class="headerlink" title="Affine layer: backward"></a>Affine layer: backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//数值分析测试误差  </span><br><span class="line">//在矩阵计算的时候千万不要为了节省步骤合并成简单的几行，这样我不知道优先级的情况下反而会出错</span><br><span class="line"># Test the affine_backward function</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(10, 2, 3)</span><br><span class="line">w = np.random.randn(6, 5)</span><br><span class="line">b = np.random.randn(5)</span><br><span class="line">dout = np.random.randn(10, 5)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)</span><br><span class="line">dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)</span><br><span class="line"></span><br><span class="line">_, cache = affine_forward(x, w, b)</span><br><span class="line">dx, dw, db = affine_backward(dout, cache)</span><br><span class="line">print(dw.shape,dx.shape,db.shape)</span><br><span class="line"># The error should be around 1e-10</span><br><span class="line">print(&#x27;Testing affine_backward function:&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dw error: &#x27;, rel_error(dw_num, dw))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<h3 id=""><a href="#" class="headerlink" title=""></a><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/4.png" alt="4"></h3><h3 id="ReLU-layer-forward"><a href="#ReLU-layer-forward" class="headerlink" title="ReLU layer: forward"></a>ReLU layer: forward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Test the relu_forward function</span><br><span class="line"></span><br><span class="line">x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)</span><br><span class="line"></span><br><span class="line">out, _ = relu_forward(x)</span><br><span class="line">correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],</span><br><span class="line">                        [ 0.,          0.,          0.04545455,  0.13636364,],</span><br><span class="line">                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])</span><br><span class="line"></span><br><span class="line"># Compare your output with ours. The error should be around 5e-8</span><br><span class="line">print(&#x27;Testing relu_forward function:&#x27;)</span><br><span class="line">print(&#x27;difference: &#x27;, rel_error(out, correct_out))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/5.png" alt="5"></p>
<h3 id="ReLU-layer-backward"><a href="#ReLU-layer-backward" class="headerlink" title="ReLU layer: backward"></a>ReLU layer: backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(10, 10)</span><br><span class="line">dout = np.random.randn(*x.shape)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)</span><br><span class="line"></span><br><span class="line">_, cache = relu_forward(x)</span><br><span class="line">dx = relu_backward(dout, cache)</span><br><span class="line"></span><br><span class="line"># The error should be around 3e-12</span><br><span class="line">print(&#x27;Testing relu_backward function:&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/6.png" alt="6"></p>
<h3 id="“Sandwich”-layers"><a href="#“Sandwich”-layers" class="headerlink" title="“Sandwich” layers"></a>“Sandwich” layers</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from cs231n.layer_utils import affine_relu_forward, affine_relu_backward</span><br><span class="line">np.random.seed(231)</span><br><span class="line">x = np.random.randn(2, 3, 4)</span><br><span class="line">w = np.random.randn(12, 10)</span><br><span class="line">b = np.random.randn(10)</span><br><span class="line">dout = np.random.randn(2, 10)</span><br><span class="line"></span><br><span class="line">out, cache = affine_relu_forward(x, w, b)</span><br><span class="line">dx, dw, db = affine_relu_backward(dout, cache)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)</span><br><span class="line">dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)</span><br><span class="line">db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing affine_relu_forward:&#x27;)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line">print(&#x27;dw error: &#x27;, rel_error(dw_num, dw))</span><br><span class="line">print(&#x27;db error: &#x27;, rel_error(db_num, db))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/7.png" alt="7"></p>
<h3 id="Loss-layers-Softmax-and-SVM"><a href="#Loss-layers-Softmax-and-SVM" class="headerlink" title="Loss layers: Softmax and SVM"></a>Loss layers: Softmax and SVM</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">num_classes, num_inputs = 10, 50</span><br><span class="line">x = 0.001 * np.random.randn(num_inputs, num_classes)</span><br><span class="line">y = np.random.randint(num_classes, size=num_inputs)</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)</span><br><span class="line">loss, dx = svm_loss(x, y)</span><br><span class="line"></span><br><span class="line"># Test svm_loss function. Loss should be around 9 and dx error should be 1e-9</span><br><span class="line">print(&#x27;Testing svm_loss:&#x27;)</span><br><span class="line">print(&#x27;loss: &#x27;, loss)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br><span class="line"></span><br><span class="line">dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)</span><br><span class="line">loss, dx = softmax_loss(x, y)</span><br><span class="line"></span><br><span class="line"># Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8</span><br><span class="line">print(&#x27;\nTesting softmax_loss:&#x27;)</span><br><span class="line">print(&#x27;loss: &#x27;, loss)</span><br><span class="line">print(&#x27;dx error: &#x27;, rel_error(dx_num, dx))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/8.png" alt="8"></p>
<h3 id="Two-layer-network-模块化设计两层神经网络"><a href="#Two-layer-network-模块化设计两层神经网络" class="headerlink" title="Two-layer network 模块化设计两层神经网络"></a>Two-layer network 模块化设计两层神经网络</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">np.random.seed(231)</span><br><span class="line">N, D, H, C = 3, 5, 50, 7</span><br><span class="line">X = np.random.randn(N, D)</span><br><span class="line">y = np.random.randint(C, size=N)</span><br><span class="line"></span><br><span class="line">std = 1e-3</span><br><span class="line">model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing initialization ... &#x27;)</span><br><span class="line">W1_std = abs(model.params[&#x27;W1&#x27;].std() - std)</span><br><span class="line">b1 = model.params[&#x27;b1&#x27;]</span><br><span class="line">W2_std = abs(model.params[&#x27;W2&#x27;].std() - std)</span><br><span class="line">b2 = model.params[&#x27;b2&#x27;]</span><br><span class="line">assert W1_std &lt; std / 10, &#x27;First layer weights do not seem right&#x27;</span><br><span class="line">assert np.all(b1 == 0), &#x27;First layer biases do not seem right&#x27;</span><br><span class="line">assert W2_std &lt; std / 10, &#x27;Second layer weights do not seem right&#x27;</span><br><span class="line">assert np.all(b2 == 0), &#x27;Second layer biases do not seem right&#x27;</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing test-time forward pass ... &#x27;)</span><br><span class="line">model.params[&#x27;W1&#x27;] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)</span><br><span class="line">model.params[&#x27;b1&#x27;] = np.linspace(-0.1, 0.9, num=H)</span><br><span class="line">model.params[&#x27;W2&#x27;] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)</span><br><span class="line">model.params[&#x27;b2&#x27;] = np.linspace(-0.9, 0.1, num=C)</span><br><span class="line">X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T</span><br><span class="line">scores = model.loss(X)</span><br><span class="line">correct_scores = np.asarray(</span><br><span class="line">  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],</span><br><span class="line">   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],</span><br><span class="line">   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])</span><br><span class="line">scores_diff = np.abs(scores - correct_scores).sum()</span><br><span class="line">assert scores_diff &lt; 1e-6, &#x27;Problem with test-time forward pass&#x27;</span><br><span class="line"></span><br><span class="line">print(&#x27;Testing training loss (no regularization)&#x27;)</span><br><span class="line">y = np.asarray([0, 5, 1])</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = 3.4702243556</span><br><span class="line">assert abs(loss - correct_loss) &lt; 1e-10, &#x27;Problem with training-time loss&#x27;</span><br><span class="line"></span><br><span class="line">model.reg = 1.0</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = 26.5948426952</span><br><span class="line">assert abs(loss - correct_loss) &lt; 1e-10, &#x27;Problem with regularization loss&#x27;</span><br><span class="line"></span><br><span class="line">for reg in [0.0, 0.7]:</span><br><span class="line">  print(&#x27;Running numeric gradient check with reg = &#x27;, reg)</span><br><span class="line">  model.reg = reg</span><br><span class="line">  loss, grads = model.loss(X, y)</span><br><span class="line"></span><br><span class="line">  for name in sorted(grads):</span><br><span class="line">    f = lambda _: model.loss(X, y)[0]</span><br><span class="line">    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)</span><br><span class="line">    print(&#x27;%s relative error: %.2e&#x27; % (name, rel_error(grad_num, grads[name])))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/9.png" alt="9"></p>
<h3 id="Solver-需要自己编写的代码段"><a href="#Solver-需要自己编写的代码段" class="headerlink" title="Solver    &#x2F;&#x2F;需要自己编写的代码段"></a>Solver    &#x2F;&#x2F;需要自己编写的代码段</h3><p>&#x2F;&#x2F;选择神经网络的构成方式，选择梯度下降方式，选择超参数等等，并且测试再验证集上的准确率</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = TwoLayerNet()</span><br><span class="line">solver = None</span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #</span><br><span class="line"># 50% accuracy on the validation set.                                        #</span><br><span class="line">##############################################################################</span><br><span class="line">solver=Solver(model,data,update_rule=&#x27;sgd&#x27;,optim_config=&#123;&#x27;learning_rate&#x27;:1e-3,&#125;,</span><br><span class="line">                                                         lr_decay=0.95,batch_size=100,</span><br><span class="line">             num_epochs=10,num_train_samples=1000,print_every=100)</span><br><span class="line">solver.train()</span><br><span class="line">scores=model.loss(data[&#x27;X_val&#x27;])</span><br><span class="line">y_val_pred=np.argmax(scores,axis=1)</span><br><span class="line">acc=np.mean(y_val_pred==data[&#x27;y_val&#x27;])</span><br><span class="line">print(acc)</span><br><span class="line">pass</span><br><span class="line">##############################################################################</span><br><span class="line">#                             END OF YOUR CODE                               #</span><br><span class="line">##############################################################################</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/10.png" alt="10"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//可视化</span><br><span class="line"># Run this cell to visualize training loss and train / val accuracy</span><br><span class="line">plt.subplot(2, 1, 1)</span><br><span class="line">plt.title(&#x27;Training loss&#x27;)</span><br><span class="line">plt.plot(solver.loss_history, &#x27;o&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(2, 1, 2)</span><br><span class="line">plt.title(&#x27;Accuracy&#x27;)</span><br><span class="line">plt.plot(solver.train_acc_history, &#x27;-o&#x27;, label=&#x27;train&#x27;)</span><br><span class="line">plt.plot(solver.val_acc_history, &#x27;-o&#x27;, label=&#x27;val&#x27;)</span><br><span class="line">plt.plot([0.5] * len(solver.val_acc_history), &#x27;k--&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line">plt.legend(loc=&#x27;lower right&#x27;)</span><br><span class="line">plt.gcf().set_size_inches(15, 12)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/11.png" alt="11"></p>
<h3 id="Multilayer-network"><a href="#Multilayer-network" class="headerlink" title="Multilayer network"></a>Multilayer network</h3><p>完成全连接层，并计算误差应该小于1e-6</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(231)</span><br><span class="line">N, D, H1, H2, C = 2, 15, 20, 30, 10</span><br><span class="line">X = np.random.randn(N, D)</span><br><span class="line">y = np.random.randint(C, size=(N,))</span><br><span class="line"></span><br><span class="line">for reg in [0, 3.14]:</span><br><span class="line">  print(&#x27;Running check with reg = &#x27;, reg)</span><br><span class="line">  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,</span><br><span class="line">                            reg=reg, weight_scale=5e-2, dtype=np.float64)</span><br><span class="line"></span><br><span class="line">  loss, grads = model.loss(X, y)</span><br><span class="line">  print(&#x27;Initial loss: &#x27;, loss)</span><br><span class="line"></span><br><span class="line">  for name in sorted(grads):</span><br><span class="line">    f = lambda _: model.loss(X, y)[0]</span><br><span class="line">    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)</span><br><span class="line">    print(&#x27;%s relative error: %.2e&#x27; % (name, rel_error(grad_num, grads[name])))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/12.png" alt="12"></p>
<h5 id="选择50个数据集，并且调整学习率使得在20ecpochs之后训练的准确率达到100-，并可视化"><a href="#选择50个数据集，并且调整学习率使得在20ecpochs之后训练的准确率达到100-，并可视化" class="headerlink" title="选择50个数据集，并且调整学习率使得在20ecpochs之后训练的准确率达到100%，并可视化"></a>选择50个数据集，并且调整学习率使得在20ecpochs之后训练的准确率达到100%，并可视化</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># TODO: Use a three-layer Net to overfit 50 training examples.</span><br><span class="line"></span><br><span class="line">num_train = 50</span><br><span class="line">small_data = &#123;</span><br><span class="line">  &#x27;X_train&#x27;: data[&#x27;X_train&#x27;][:num_train],</span><br><span class="line">  &#x27;y_train&#x27;: data[&#x27;y_train&#x27;][:num_train],</span><br><span class="line">  &#x27;X_val&#x27;: data[&#x27;X_val&#x27;],</span><br><span class="line">  &#x27;y_val&#x27;: data[&#x27;y_val&#x27;],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">weight_scale = 1e-2</span><br><span class="line">learning_rate = 1e-2</span><br><span class="line">model = FullyConnectedNet([100, 100],</span><br><span class="line">              weight_scale=weight_scale, dtype=np.float64)</span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                print_every=10, num_epochs=20, batch_size=25,</span><br><span class="line">                update_rule=&#x27;sgd&#x27;,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  &#x27;learning_rate&#x27;: learning_rate,</span><br><span class="line">                &#125;</span><br><span class="line">         )</span><br><span class="line">solver.train()</span><br><span class="line"></span><br><span class="line">plt.plot(solver.loss_history, &#x27;o&#x27;)</span><br><span class="line">plt.title(&#x27;Training loss history&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Training loss&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/13.png" alt="13"></p>
<p>与上段代码相似，多调整一个参数weiht_scale,并使得训练的准确率达到100%，并且可视化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># TODO: Use a three-layer Net to overfit 50 training examples.</span><br><span class="line"></span><br><span class="line">num_train = 50</span><br><span class="line">small_data = &#123;</span><br><span class="line">  &#x27;X_train&#x27;: data[&#x27;X_train&#x27;][:num_train],</span><br><span class="line">  &#x27;y_train&#x27;: data[&#x27;y_train&#x27;][:num_train],</span><br><span class="line">  &#x27;X_val&#x27;: data[&#x27;X_val&#x27;],</span><br><span class="line">  &#x27;y_val&#x27;: data[&#x27;y_val&#x27;],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">weight_scale = 1e-2</span><br><span class="line">learning_rate = 1e-2</span><br><span class="line">model = FullyConnectedNet([100, 100],</span><br><span class="line">              weight_scale=weight_scale, dtype=np.float64)</span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                print_every=10, num_epochs=20, batch_size=25,</span><br><span class="line">                update_rule=&#x27;sgd&#x27;,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  &#x27;learning_rate&#x27;: learning_rate,</span><br><span class="line">                &#125;</span><br><span class="line">         )</span><br><span class="line">solver.train()</span><br><span class="line"></span><br><span class="line">plt.plot(solver.loss_history, &#x27;o&#x27;)</span><br><span class="line">plt.title(&#x27;Training loss history&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Training loss&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/14.png" alt="14"></p>
<h3 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">//完成sgd+momentum更新梯度方式</span><br><span class="line">from cs231n.optim import sgd_momentum</span><br><span class="line"></span><br><span class="line">N, D = 4, 5</span><br><span class="line">w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)</span><br><span class="line">dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)</span><br><span class="line">v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)</span><br><span class="line"></span><br><span class="line">config = &#123;&#x27;learning_rate&#x27;: 1e-3, &#x27;velocity&#x27;: v&#125;</span><br><span class="line">next_w, _ = sgd_momentum(w, dw, config=config)</span><br><span class="line"></span><br><span class="line">expected_next_w = np.asarray([</span><br><span class="line">  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],</span><br><span class="line">  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],</span><br><span class="line">  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],</span><br><span class="line">  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])</span><br><span class="line">expected_velocity = np.asarray([</span><br><span class="line">  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],</span><br><span class="line">  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],</span><br><span class="line">  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],</span><br><span class="line">  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])</span><br><span class="line"></span><br><span class="line">print(&#x27;next_w error: &#x27;, rel_error(next_w, expected_next_w))</span><br><span class="line">print(&#x27;velocity error: &#x27;, rel_error(expected_velocity, config[&#x27;velocity&#x27;]))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/15.png" alt="15"></p>
<h5 id="可视化的方式来展示SGD和SGD-Momentum梯度下降方式的差异"><a href="#可视化的方式来展示SGD和SGD-Momentum梯度下降方式的差异" class="headerlink" title="可视化的方式来展示SGD和SGD_Momentum梯度下降方式的差异"></a>可视化的方式来展示SGD和SGD_Momentum梯度下降方式的差异</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">num_train = 4000</span><br><span class="line">small_data = &#123;</span><br><span class="line">  &#x27;X_train&#x27;: data[&#x27;X_train&#x27;][:num_train],</span><br><span class="line">  &#x27;y_train&#x27;: data[&#x27;y_train&#x27;][:num_train],</span><br><span class="line">  &#x27;X_val&#x27;: data[&#x27;X_val&#x27;],</span><br><span class="line">  &#x27;y_val&#x27;: data[&#x27;y_val&#x27;],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line"></span><br><span class="line">for update_rule in [&#x27;sgd&#x27;, &#x27;sgd_momentum&#x27;]:</span><br><span class="line">  print(&#x27;running with &#x27;, update_rule)</span><br><span class="line">  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=5, batch_size=100,</span><br><span class="line">                  update_rule=update_rule,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    &#x27;learning_rate&#x27;: 1e-2,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=True)</span><br><span class="line">  solvers[update_rule] = solver</span><br><span class="line">  solver.train()</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 1)</span><br><span class="line">plt.title(&#x27;Training loss&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 2)</span><br><span class="line">plt.title(&#x27;Training accuracy&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 3)</span><br><span class="line">plt.title(&#x27;Validation accuracy&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line"></span><br><span class="line">for update_rule, solver in list(solvers.items()):</span><br><span class="line">  plt.subplot(3, 1, 1)</span><br><span class="line">  plt.plot(solver.loss_history, &#x27;o&#x27;, label=update_rule)</span><br><span class="line">  </span><br><span class="line">  plt.subplot(3, 1, 2)</span><br><span class="line">  plt.plot(solver.train_acc_history, &#x27;-o&#x27;, label=update_rule)</span><br><span class="line"></span><br><span class="line">  plt.subplot(3, 1, 3)</span><br><span class="line">  plt.plot(solver.val_acc_history, &#x27;-o&#x27;, label=update_rule)</span><br><span class="line">  </span><br><span class="line">for i in [1, 2, 3]:</span><br><span class="line">  plt.subplot(3, 1, i)</span><br><span class="line">  plt.legend(loc=&#x27;upper center&#x27;, ncol=4)</span><br><span class="line">plt.gcf().set_size_inches(15, 15)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/16.png" alt="16"></p>
<h3 id="RMSProp-and-Adam-两种梯度下降算法"><a href="#RMSProp-and-Adam-两种梯度下降算法" class="headerlink" title="RMSProp and Adam 两种梯度下降算法"></a>RMSProp and Adam 两种梯度下降算法</h3><h5 id="RMSProp方式"><a href="#RMSProp方式" class="headerlink" title="&#x2F;&#x2F;RMSProp方式"></a>&#x2F;&#x2F;RMSProp方式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Test RMSProp implementation; you should see errors less than 1e-7</span><br><span class="line">from cs231n.optim import rmsprop</span><br><span class="line"></span><br><span class="line">N, D = 4, 5</span><br><span class="line">w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)</span><br><span class="line">dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)</span><br><span class="line">cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)</span><br><span class="line"></span><br><span class="line">config = &#123;&#x27;learning_rate&#x27;: 1e-2, &#x27;cache&#x27;: cache&#125;</span><br><span class="line">next_w, _ = rmsprop(w, dw, config=config)</span><br><span class="line"></span><br><span class="line">expected_next_w = np.asarray([</span><br><span class="line">  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],</span><br><span class="line">  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],</span><br><span class="line">  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],</span><br><span class="line">  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])</span><br><span class="line">expected_cache = np.asarray([</span><br><span class="line">  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],</span><br><span class="line">  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],</span><br><span class="line">  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],</span><br><span class="line">  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])</span><br><span class="line"></span><br><span class="line">print(&#x27;next_w error: &#x27;, rel_error(expected_next_w, next_w))</span><br><span class="line">print(&#x27;cache error: &#x27;, rel_error(expected_cache, config[&#x27;cache&#x27;]))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/17.png" alt="17"></p>
<h5 id="Adam方式"><a href="#Adam方式" class="headerlink" title="&#x2F;&#x2F;Adam方式"></a>&#x2F;&#x2F;Adam方式</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Test Adam implementation; you should see errors around 1e-7 or less</span><br><span class="line">from cs231n.optim import adam</span><br><span class="line"></span><br><span class="line">N, D = 4, 5</span><br><span class="line">w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)</span><br><span class="line">dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)</span><br><span class="line">m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)</span><br><span class="line">v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)</span><br><span class="line"></span><br><span class="line">config = &#123;&#x27;learning_rate&#x27;: 1e-2, &#x27;m&#x27;: m, &#x27;v&#x27;: v, &#x27;t&#x27;: 5&#125;</span><br><span class="line">next_w, _ = adam(w, dw, config=config)</span><br><span class="line"></span><br><span class="line">expected_next_w = np.asarray([</span><br><span class="line">  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],</span><br><span class="line">  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],</span><br><span class="line">  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],</span><br><span class="line">  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])</span><br><span class="line">expected_v = np.asarray([</span><br><span class="line">  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],</span><br><span class="line">  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],</span><br><span class="line">  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],</span><br><span class="line">  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])</span><br><span class="line">expected_m = np.asarray([</span><br><span class="line">  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],</span><br><span class="line">  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],</span><br><span class="line">  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],</span><br><span class="line">  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])</span><br><span class="line"></span><br><span class="line">print(&#x27;next_w error: &#x27;, rel_error(expected_next_w, next_w))</span><br><span class="line">print(&#x27;v error: &#x27;, rel_error(expected_v, config[&#x27;v&#x27;]))</span><br><span class="line">print(&#x27;m error: &#x27;, rel_error(expected_m, config[&#x27;m&#x27;]))</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/18.png" alt="18"></p>
<h5 id="可视化的方式展示4种梯度下降算法的损失函数以及准确率"><a href="#可视化的方式展示4种梯度下降算法的损失函数以及准确率" class="headerlink" title="可视化的方式展示4种梯度下降算法的损失函数以及准确率"></a>可视化的方式展示4种梯度下降算法的损失函数以及准确率</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = &#123;&#x27;rmsprop&#x27;: 1e-4, &#x27;adam&#x27;: 1e-3&#125;</span><br><span class="line">for update_rule in [&#x27;adam&#x27;, &#x27;rmsprop&#x27;]:</span><br><span class="line">  print(&#x27;running with &#x27;, update_rule)</span><br><span class="line">  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=5, batch_size=100,</span><br><span class="line">                  update_rule=update_rule,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    &#x27;learning_rate&#x27;: learning_rates[update_rule]</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=True)</span><br><span class="line">  solvers[update_rule] = solver</span><br><span class="line">  solver.train()</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 1)</span><br><span class="line">plt.title(&#x27;Training loss&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 2)</span><br><span class="line">plt.title(&#x27;Training accuracy&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 1, 3)</span><br><span class="line">plt.title(&#x27;Validation accuracy&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line"></span><br><span class="line">for update_rule, solver in list(solvers.items()):</span><br><span class="line">  plt.subplot(3, 1, 1)</span><br><span class="line">  plt.plot(solver.loss_history, &#x27;o&#x27;, label=update_rule)</span><br><span class="line">  </span><br><span class="line">  plt.subplot(3, 1, 2)</span><br><span class="line">  plt.plot(solver.train_acc_history, &#x27;-o&#x27;, label=update_rule)</span><br><span class="line"></span><br><span class="line">  plt.subplot(3, 1, 3)</span><br><span class="line">  plt.plot(solver.val_acc_history, &#x27;-o&#x27;, label=update_rule)</span><br><span class="line">  </span><br><span class="line">for i in [1, 2, 3]:</span><br><span class="line">  plt.subplot(3, 1, i)</span><br><span class="line">  plt.legend(loc=&#x27;upper center&#x27;, ncol=4)</span><br><span class="line">plt.gcf().set_size_inches(15, 15)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/CS231n_Assignment2_Fully-Connected_Neural_Nets/19.png" alt="19"></p>
<h1 id="Train-a-good-model"><a href="#Train-a-good-model" class="headerlink" title="Train a good model!"></a>Train a good model!</h1><p>下面中的参数分别代表：</p>
<p>batch_size：梯度每经过多少个训练数据更新一次梯度</p>
<p>num_epochs：比如1000个数据的训练集，迭代多少个周期</p>
<p>iterations_per_epoch&#x3D;num_train&#x2F;&#x2F;batch_size:因此为一个周期更新多少次梯度</p>
<p>num_iterations:梯度更新的次数的总次数</p>
<p>编写如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">best_model = None</span><br><span class="line">best_acc=0</span><br><span class="line">best_drop_value=None</span><br><span class="line">best_lr=None</span><br><span class="line">best_ws=None</span><br><span class="line">best_rule=None</span><br><span class="line"></span><br><span class="line">################################################################################</span><br><span class="line"># TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #</span><br><span class="line"># batch normalization and dropout useful. Store your best model in the         #</span><br><span class="line"># best_model variable.                                                         #</span><br><span class="line">################################################################################</span><br><span class="line">dropout_value=[0,0.25,0.5,0.75]</span><br><span class="line">learning_rates=[1e-3,1e-4,5e-4] #3.1e-4</span><br><span class="line">update_rule=[&#x27;adam&#x27;,&#x27;rmsprop&#x27;,&#x27;sgd_momentum&#x27;]</span><br><span class="line">weight_scale=[5e-2,4e-2,1e-2] #2.5e-2,1e-5</span><br><span class="line">for dp in dropout_value:</span><br><span class="line">    for lr in learning_rates:</span><br><span class="line">        for rules in update_rule:</span><br><span class="line">            for ws in weight_scale:</span><br><span class="line">                model=FullyConnectedNet([100,100,100,100,100,100],dropout=dp,weight_scale=ws)</span><br><span class="line">                solver=Solver(model,small_data,num_epochs=20,batch_size=200,update_rule=rules,</span><br><span class="line">                             optim_config=&#123;&#x27;learning_rate&#x27;:lr&#125;,verbose=False)</span><br><span class="line">                solver.train()</span><br><span class="line">                y_train_pred=np.argmax(model.loss(data[&#x27;X_train&#x27;]),axis=1)</span><br><span class="line">                train_acc=np.mean(y_train_pred==data[&#x27;y_train&#x27;])</span><br><span class="line">                print(&quot;dropout_value:%f,lr:%f,rule:%s,best_acc:%f,ws:%f&quot;%(dp,lr,rules,train_acc,ws))</span><br><span class="line">                if train_acc&gt;best_acc:</span><br><span class="line">                    best_drop_value=dp</span><br><span class="line">                    best_lr=lr</span><br><span class="line">                    best_ws=ws</span><br><span class="line">                    best_model=model</span><br><span class="line">                    best_acc=train_acc</span><br><span class="line">                    best_rule=rules</span><br><span class="line">print(&quot;111dropout_value:%f,lr:%f,rule:%s,best_acc%f,best_ws:%f&quot;%(best_drop_value,best_lr,best_rule,best_acc,best_ws))</span><br><span class="line"></span><br><span class="line">pass</span><br><span class="line">################################################################################</span><br><span class="line">#                              END OF YOUR CODE                                #</span><br><span class="line">################################################################################</span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\yuyuyyu\Tukekehaohaonuli.github.io\source\images\CS231n_Assignment2_Fully-Connected_Neural_Nets\20.png" alt="20"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = np.argmax(best_model.loss(data[&#x27;X_test&#x27;]), axis=1)</span><br><span class="line">y_val_pred = np.argmax(best_model.loss(data[&#x27;X_val&#x27;]), axis=1)</span><br><span class="line">print(&#x27;Validation set accuracy: &#x27;, (y_val_pred == data[&#x27;y_val&#x27;]).mean())</span><br><span class="line">print(&#x27;Test set accuracy: &#x27;, (y_test_pred == data[&#x27;y_test&#x27;]).mean())</span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\yuyuyyu\Tukekehaohaonuli.github.io\source\images\CS231n_Assignment2_Fully-Connected_Neural_Nets\21.png" alt="21"></p>
<h1 id="data-utils-py-获取数据"><a href="#data-utils-py-获取数据" class="headerlink" title="data_utils.py   获取数据"></a>data_utils.py   获取数据</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//读取数据集中的数据，49000张训练集，1000张验证集，1000张测试集，并对所有数据集减去均值化</span><br><span class="line">def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,</span><br><span class="line">                     subtract_mean=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare</span><br><span class="line">    it for classifiers. These are the same steps as we used for the SVM, but</span><br><span class="line">    condensed to a single function.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Load the raw CIFAR-10 data</span><br><span class="line">    cifar10_dir = &#x27;cs231n/datasets/cifar-10-batches-py&#x27;</span><br><span class="line">    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)</span><br><span class="line"></span><br><span class="line">    # Subsample the data</span><br><span class="line">    mask = list(range(num_training, num_training + num_validation))</span><br><span class="line">    X_val = X_train[mask]</span><br><span class="line">    y_val = y_train[mask]</span><br><span class="line">    mask = list(range(num_training))</span><br><span class="line">    X_train = X_train[mask]</span><br><span class="line">    y_train = y_train[mask]</span><br><span class="line">    mask = list(range(num_test))</span><br><span class="line">    X_test = X_test[mask]</span><br><span class="line">    y_test = y_test[mask]</span><br><span class="line"></span><br><span class="line">    # Normalize the data: subtract the mean image</span><br><span class="line">    if subtract_mean:</span><br><span class="line">        mean_image = np.mean(X_train, axis=0)</span><br><span class="line">        X_train -= mean_image</span><br><span class="line">        X_val -= mean_image</span><br><span class="line">        X_test -= mean_image</span><br><span class="line"></span><br><span class="line">    # Transpose so that channels come first</span><br><span class="line">    X_train = X_train.transpose(0, 3, 1, 2).copy()</span><br><span class="line">    X_val = X_val.transpose(0, 3, 1, 2).copy()</span><br><span class="line">    X_test = X_test.transpose(0, 3, 1, 2).copy()</span><br><span class="line"></span><br><span class="line">    # Package data into a dictionary</span><br><span class="line">    return &#123;</span><br><span class="line">      &#x27;X_train&#x27;: X_train, &#x27;y_train&#x27;: y_train,</span><br><span class="line">      &#x27;X_val&#x27;: X_val, &#x27;y_val&#x27;: y_val,</span><br><span class="line">      &#x27;X_test&#x27;: X_test, &#x27;y_test&#x27;: y_test,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<h1 id="layers-py-正向传播和反向传播"><a href="#layers-py-正向传播和反向传播" class="headerlink" title="layers.py   正向传播和反向传播"></a>layers.py   正向传播和反向传播</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">from builtins import range</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def affine_forward(x, w, b)://正向传播的输出值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the forward pass for an affine (fully-connected) layer.</span><br><span class="line"></span><br><span class="line">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span><br><span class="line">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span><br><span class="line">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span><br><span class="line">    then transform it to an output vector of dimension M.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span><br><span class="line">    - w: A numpy array of weights, of shape (D, M)</span><br><span class="line">    - b: A numpy array of biases, of shape (M,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: output, of shape (N, M)</span><br><span class="line">    - cache: (x, w, b)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    out = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the affine forward pass. Store the result in out. You   #</span><br><span class="line">    # will need to reshape the input into rows.                               #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    out=x.reshape(x.shape[0],-1).dot(w)+b</span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    return out, cache</span><br><span class="line">    </span><br><span class="line">def affine_backward(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the backward pass for an affine layer.</span><br><span class="line"></span><br><span class="line">    Inputs:</span><br><span class="line">    - dout: Upstream derivative, of shape (N, M)</span><br><span class="line">    - cache: Tuple of:</span><br><span class="line">      - x: Input data, of shape (N, d_1, ... d_k)</span><br><span class="line">      - w: Weights, of shape (D, M)</span><br><span class="line">      b(M,)</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span><br><span class="line">    - dw: Gradient with respect to w, of shape (D, M)</span><br><span class="line">    - db: Gradient with respect to b, of shape (M,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = None, None, None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the affine backward pass.                               #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    N = x.shape[0]</span><br><span class="line">    x_reshape = x.reshape([N, -1]) # [N, D]</span><br><span class="line">    dx = dout.dot(w.T).reshape(*x.shape)</span><br><span class="line">    dw = (x_reshape.T).dot(dout)</span><br><span class="line">    db = np.sum(dout, axis=0)</span><br><span class="line"></span><br><span class="line">#     N=x.shape[0]         </span><br><span class="line">#     print(x.shape,&#x27;********&#x27;)</span><br><span class="line">#     #x_rsp=x.reshape(N,-1)  </span><br><span class="line">#     dx=dout.dot(w.T)     </span><br><span class="line">#     dx=dx.reshape(*x.shape) </span><br><span class="line">#     #print(dx.shape)</span><br><span class="line">#     print((x.T).shape,&#x27;********&#x27;)</span><br><span class="line">#     print(dout.shape,&#x27;######&#x27;)</span><br><span class="line">#     dw=(x.T).dot(dout)   </span><br><span class="line">#     db=np.sum(dout,axis=0)</span><br><span class="line">    #print(dw.shape,&#x27;~~~~~~~~~~~~~&#x27;)</span><br><span class="line">   # dw=x.reshape(x.shape[0],-1).T.dot(dout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#     dx=dout.dot(w.T).reshape(x.shape[0],-1)</span><br><span class="line">#     dw=x.reshape(x.shape[0],-1).T.dot(dout)</span><br><span class="line">#     db=np.sum(dout,axis=0)</span><br><span class="line">    </span><br><span class="line">#     dx=dout.dot(w.T)</span><br><span class="line">#     dx=dx.reshape(*x.shape)</span><br><span class="line">#     print(dx.shape,&#x27;#####&#x27;)</span><br><span class="line">#     x_rsp=x.reshape(x.shape[0],-1)</span><br><span class="line">#     dw=x_rsp.T.dot(dout)</span><br><span class="line">#     db=np.sum(dout,axis=0)</span><br><span class="line">#     print(&#x27;x1&#x27;)</span><br><span class="line">#     dx=dx.reshape(*x.shape)</span><br><span class="line">#     db=np.sum(dout,axis=0)</span><br><span class="line">    </span><br><span class="line">             </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    return dx, dw, db</span><br><span class="line"></span><br><span class="line">def relu_forward(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the forward pass for a layer of rectified linear units (ReLUs).</span><br><span class="line"></span><br><span class="line">    Input:</span><br><span class="line">    - x: Inputs, of any shape</span><br><span class="line"></span><br><span class="line">    Returns a tuple of:</span><br><span class="line">    - out: Output, of the same shape as x</span><br><span class="line">    - cache: x</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    out = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the ReLU forward pass.                                  #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    out=x*(x&gt;=0)</span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    cache = x</span><br><span class="line">    return out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def relu_backward(dout, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the backward pass for a layer of rectified linear units (ReLUs).</span><br><span class="line"></span><br><span class="line">    Input:</span><br><span class="line">    - dout: Upstream derivatives, of any shape</span><br><span class="line">    - cache: Input x, of same shape as dout</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    - dx: Gradient with respect to x</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dx, x = None, cache</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the ReLU backward pass.                                 #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    dx=(x&gt;=0)*dout</span><br><span class="line"></span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    return dx</span><br></pre></td></tr></table></figure>



<h1 id="gradient-check-py-检查梯度是否有错误"><a href="#gradient-check-py-检查梯度是否有错误" class="headerlink" title="gradient_check.py 检查梯度是否有错误"></a>gradient_check.py 检查梯度是否有错误</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//数值分析的方法计算梯度是否有差异</span><br><span class="line">def eval_numerical_gradient_array(f, x, df, h=1e-5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Evaluate a numeric gradient for a function that accepts a numpy</span><br><span class="line">    array and returns a numpy array.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line">    it = np.nditer(x, flags=[&#x27;multi_index&#x27;], op_flags=[&#x27;readwrite&#x27;])</span><br><span class="line">    while not it.finished:</span><br><span class="line">        ix = it.multi_index</span><br><span class="line"></span><br><span class="line">        oldval = x[ix]</span><br><span class="line">        x[ix] = oldval + h</span><br><span class="line">        pos = f(x).copy()</span><br><span class="line">        x[ix] = oldval - h</span><br><span class="line">        neg = f(x).copy()</span><br><span class="line">        x[ix] = oldval</span><br><span class="line"></span><br><span class="line">        grad[ix] = np.sum((pos - neg) * df) / (2 * h)</span><br><span class="line">        it.iternext()</span><br><span class="line">    return grad</span><br></pre></td></tr></table></figure>

<h1 id="fc-net-py-两层神经网络和全连接层神经网络"><a href="#fc-net-py-两层神经网络和全连接层神经网络" class="headerlink" title="fc_net.py  两层神经网络和全连接层神经网络"></a>fc_net.py  两层神经网络和全连接层神经网络</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br></pre></td><td class="code"><pre><span class="line">from builtins import range</span><br><span class="line">from builtins import object</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from cs231n.layers import *</span><br><span class="line">from cs231n.layer_utils import *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TwoLayerNet(object):  //使用模块化构建两层神经网络</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A two-layer fully-connected neural network with ReLU nonlinearity and</span><br><span class="line">    softmax loss that uses a modular layer design. We assume an input dimension</span><br><span class="line">    of D, a hidden dimension of H, and perform classification over C classes.</span><br><span class="line"></span><br><span class="line">    The architecure should be affine - relu - affine - softmax.</span><br><span class="line"></span><br><span class="line">    Note that this class does not implement gradient descent; instead, it</span><br><span class="line">    will interact with a separate Solver object that is responsible for running</span><br><span class="line">    optimization.</span><br><span class="line"></span><br><span class="line">    The learnable parameters of the model are stored in the dictionary</span><br><span class="line">    self.params that maps parameter names to numpy arrays.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,</span><br><span class="line">                 weight_scale=1e-3, reg=0.0):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Initialize a new network.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - input_dim: An integer giving the size of the input</span><br><span class="line">        - hidden_dim: An integer giving the size of the hidden layer</span><br><span class="line">        - num_classes: An integer giving the number of classes to classify</span><br><span class="line">        - dropout: Scalar between 0 and 1 giving dropout strength.</span><br><span class="line">        - weight_scale: Scalar giving the standard deviation for random</span><br><span class="line">          initialization of the weights.</span><br><span class="line">        - reg: Scalar giving L2 regularization strength.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Initialize the weights and biases of the two-layer net. Weights    #</span><br><span class="line">        # should be initialized from a Gaussian with standard deviation equal to   #</span><br><span class="line">        # weight_scale, and biases should be initialized to zero. All weights and  #</span><br><span class="line">        # biases should be stored in the dictionary self.params, with first layer  #</span><br><span class="line">        # weights and biases using the keys &#x27;W1&#x27; and &#x27;b1&#x27; and second layer weights #</span><br><span class="line">        # and biases using the keys &#x27;W2&#x27; and &#x27;b2&#x27;.          #</span><br><span class="line">        self.params[&#x27;W1&#x27;]=weight_scale*np.random.randn(input_dim,hidden_dim)</span><br><span class="line">        self.params[&#x27;b1&#x27;]=np.zeros(hidden_dim)</span><br><span class="line">        self.params[&#x27;W2&#x27;]=weight_scale*np.random.randn(hidden_dim,num_classes)</span><br><span class="line">        self.params[&#x27;b2&#x27;]=np.zeros(num_classes)</span><br><span class="line">        ############################################################################</span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def loss(self, X, y=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Compute loss and gradient for a minibatch of data.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - X: Array of input data of shape (N, d_1, ..., d_k)</span><br><span class="line">        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">        If y is None, then run a test-time forward pass of the model and return:</span><br><span class="line">        - scores: Array of shape (N, C) giving classification scores, where</span><br><span class="line">          scores[i, c] is the classification score for X[i] and class c.</span><br><span class="line"></span><br><span class="line">        If y is not None, then run a training-time forward and backward pass and</span><br><span class="line">        return a tuple of:</span><br><span class="line">        - loss: Scalar value giving the loss</span><br><span class="line">        - grads: Dictionary with the same keys as self.params, mapping parameter</span><br><span class="line">          names to gradients of the loss with respect to those parameters.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        scores = None</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the forward pass for the two-layer net, computing the    #</span><br><span class="line">        # class scores for X and storing them in the scores variable.              #</span><br><span class="line">        ############################################################################</span><br><span class="line">                                                </span><br><span class="line">        layyer_out1,layyer_cache1=affine_relu_forward(X,self.params[&#x27;W1&#x27;],self.params[&#x27;b1&#x27;])</span><br><span class="line">        layyer_out2,layyer_cache2=affine_forward(layyer_out1,self.params[&#x27;W2&#x27;],self.params[&#x27;b2&#x27;])</span><br><span class="line">        scores=layyer_out2</span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        # If y is None then we are in test mode so just return scores</span><br><span class="line">        if y is None:</span><br><span class="line">            return scores</span><br><span class="line"></span><br><span class="line">        loss, grads = 0, &#123;&#125;</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the backward pass for the two-layer net. Store the loss  #</span><br><span class="line">        # in the loss variable and gradients in the grads dictionary. Compute data #</span><br><span class="line">        # loss using softmax, and make sure that grads[k] holds the gradients for  #</span><br><span class="line">        # self.params[k]. Don&#x27;t forget to add L2 regularization!                   #</span><br><span class="line">        #                                                                          #</span><br><span class="line">        # NOTE: To ensure that your implementation matches ours and you pass the   #</span><br><span class="line">        # automated tests, make sure that your L2 regularization includes a factor #</span><br><span class="line">        # of 0.5 to simplify the expression for the gradient.                      #</span><br><span class="line">        ############################################################################</span><br><span class="line">        </span><br><span class="line">        loss,dscores=softmax_loss(scores,y)</span><br><span class="line">        loss=loss+0.5*self.reg*(np.sum(self.params[&#x27;W1&#x27;]*self.params[&#x27;W1&#x27;])</span><br><span class="line">                                +np.sum(self.params[&#x27;W2&#x27;]*self.params[&#x27;W2&#x27;]))</span><br><span class="line">        </span><br><span class="line">        dx2,dw2,db2=affine_backward(dscores,layyer_cache2)</span><br><span class="line">        grads[&#x27;W2&#x27;]=dw2+self.reg*self.params[&#x27;W2&#x27;]</span><br><span class="line">        grads[&#x27;b2&#x27;]=db2</span><br><span class="line">        </span><br><span class="line">        dx1,dw1,db1=affine_relu_backward(dx2,layyer_cache1)</span><br><span class="line">        grads[&#x27;W1&#x27;]=dw1+self.reg*self.params[&#x27;W1&#x27;]</span><br><span class="line">        grads[&#x27;b1&#x27;]=db1</span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        return loss, grads</span><br><span class="line">class FullyConnectedNet(object):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A fully-connected neural network with an arbitrary number of hidden layers,</span><br><span class="line">    ReLU nonlinearities, and a softmax loss function. This will also implement</span><br><span class="line">    dropout and batch normalization as options. For a network with L layers,</span><br><span class="line">    the architecture will be</span><br><span class="line"></span><br><span class="line">    &#123;affine - [batch norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax</span><br><span class="line"></span><br><span class="line">    where batch normalization and dropout are optional, and the &#123;...&#125; block is</span><br><span class="line">    repeated L - 1 times.</span><br><span class="line"></span><br><span class="line">    Similar to the TwoLayerNet above, learnable parameters are stored in the</span><br><span class="line">    self.params dictionary and will be learned using the Solver class.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,</span><br><span class="line">                 dropout=0, use_batchnorm=False, reg=0.0,</span><br><span class="line">                 weight_scale=1e-2, dtype=np.float32, seed=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Initialize a new FullyConnectedNet.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - hidden_dims: A list of integers giving the size of each hidden layer.</span><br><span class="line">        - input_dim: An integer giving the size of the input.</span><br><span class="line">        - num_classes: An integer giving the number of classes to classify.</span><br><span class="line">        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then</span><br><span class="line">          the network should not use dropout at all.</span><br><span class="line">        - use_batchnorm: Whether or not the network should use batch normalization.</span><br><span class="line">        - reg: Scalar giving L2 regularization strength.</span><br><span class="line">        - weight_scale: Scalar giving the standard deviation for random</span><br><span class="line">          initialization of the weights.</span><br><span class="line">        - dtype: A numpy datatype object; all computations will be performed using</span><br><span class="line">          this datatype. float32 is faster but less accurate, so you should use</span><br><span class="line">          float64 for numeric gradient checking.</span><br><span class="line">        - seed: If not None, then pass this random seed to the dropout layers. This</span><br><span class="line">          will make the dropout layers deteriminstic so we can gradient check the</span><br><span class="line">          model.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.use_batchnorm = use_batchnorm</span><br><span class="line">        self.use_dropout = dropout &gt; 0</span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.num_layers = 1 + len(hidden_dims)</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Initialize the parameters of the network, storing all values in    #</span><br><span class="line">        # the self.params dictionary. Store weights and biases for the first layer #</span><br><span class="line">        # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #</span><br><span class="line">        # initialized from a normal distribution with standard deviation equal to  #</span><br><span class="line">        # weight_scale and biases should be initialized to zero.                   #</span><br><span class="line">        #                                                                          #</span><br><span class="line">        # When using batch normalization, store scale and shift parameters for the #</span><br><span class="line">        # first layer in gamma1 and beta1; for the second layer use gamma2 and     #</span><br><span class="line">        # beta2, etc. Scale parameters should be initialized to one and shift      #</span><br><span class="line">        # parameters should be initialized to zero.              </span><br><span class="line">        #</span><br><span class="line">        ############################################################################</span><br><span class="line">        layyer_out1_dim=input_dim</span><br><span class="line">        for i,hd in enumerate(hidden_dims):</span><br><span class="line">            self.params[&#x27;W%d&#x27;%(i+1)]=weight_scale*np.random.randn(layyer_out1_dim,hd)</span><br><span class="line">            self.params[&#x27;b%d&#x27;%(i+1)]=np.zeros(hd)</span><br><span class="line">            layyer_out1_dim=hd</span><br><span class="line">            #if use_batchnorm:</span><br><span class="line">        self.params[&#x27;W%d&#x27;%self.num_layers]=weight_scale*np.random.randn(layyer_out1_dim,num_classes)</span><br><span class="line">        self.params[&#x27;b%d&#x27;%self.num_layers]=np.zeros(num_classes)</span><br><span class="line">        </span><br><span class="line">                </span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        # When using dropout we need to pass a dropout_param dictionary to each</span><br><span class="line">        # dropout layer so that the layer knows the dropout probability and the mode</span><br><span class="line">        # (train / test). You can pass the same dropout_param to each dropout layer.</span><br><span class="line">        self.dropout_param = &#123;&#125;</span><br><span class="line">        if self.use_dropout:</span><br><span class="line">            self.dropout_param = &#123;&#x27;mode&#x27;: &#x27;train&#x27;, &#x27;p&#x27;: dropout&#125;</span><br><span class="line">            if seed is not None:</span><br><span class="line">                self.dropout_param[&#x27;seed&#x27;] = seed</span><br><span class="line"></span><br><span class="line">        # With batch normalization we need to keep track of running means and</span><br><span class="line">        # variances, so we need to pass a special bn_param object to each batch</span><br><span class="line">        # normalization layer. You should pass self.bn_params[0] to the forward pass</span><br><span class="line">        # of the first batch normalization layer, self.bn_params[1] to the forward</span><br><span class="line">        # pass of the second batch normalization layer, etc.</span><br><span class="line">        self.bn_params = []</span><br><span class="line">        if self.use_batchnorm:</span><br><span class="line">            self.bn_params = [&#123;&#x27;mode&#x27;: &#x27;train&#x27;&#125; for i in range(self.num_layers - 1)]</span><br><span class="line"></span><br><span class="line">        # Cast all parameters to the correct datatype</span><br><span class="line">        for k, v in self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def loss(self, X, y=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Compute loss and gradient for the fully-connected net.</span><br><span class="line"></span><br><span class="line">        Input / output: Same as TwoLayerNet above.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        X = X.astype(self.dtype)</span><br><span class="line">        mode = &#x27;test&#x27; if y is None else &#x27;train&#x27;</span><br><span class="line"></span><br><span class="line">        # Set train/test mode for batchnorm params and dropout param since they</span><br><span class="line">        # behave differently during training and testing.</span><br><span class="line">        if self.use_dropout:</span><br><span class="line">            self.dropout_param[&#x27;mode&#x27;] = mode</span><br><span class="line">        if self.use_batchnorm:</span><br><span class="line">            for bn_param in self.bn_params:</span><br><span class="line">                bn_param[&#x27;mode&#x27;] = mode</span><br><span class="line"></span><br><span class="line">        scores = None</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the forward pass for the fully-connected net, computing  #</span><br><span class="line">        # the class scores for X and storing them in the scores variable.          #</span><br><span class="line">        #                                                                          #</span><br><span class="line">        # When using dropout, you&#x27;ll need to pass self.dropout_param to each       #</span><br><span class="line">        # dropout forward pass.                                                    #</span><br><span class="line">        #                                                                          #</span><br><span class="line">        # When using batch normalization, you&#x27;ll need to pass self.bn_params[0] to #</span><br><span class="line">        # the forward pass for the first batch normalization layer, pass           #</span><br><span class="line">        # self.bn_params[1] to the forward pass for the second batch normalization #</span><br><span class="line">        # layer, etc.                                                              #</span><br><span class="line">        ############################################################################</span><br><span class="line">        ar_cache=&#123;&#125;</span><br><span class="line">        layer_input=X</span><br><span class="line">        for i in range(1,self.num_layers):</span><br><span class="line">            layer_input,ar_cache[i]=affine_relu_forward(layer_input,self.params[&#x27;W%d&#x27;%i],self.params[&#x27;b%d&#x27;%(i)])</span><br><span class="line">           </span><br><span class="line">        #print(&#x27;######&#x27;)</span><br><span class="line">        layer_out,ar_cache[self.num_layers]=affine_forward(layer_input,self.params[&#x27;W%d&#x27;%self.num_layers],</span><br><span class="line">                                                        self.params[&#x27;b%d&#x27;%self.num_layers])</span><br><span class="line">        scores=layer_out                                                    </span><br><span class="line">                                                          </span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        # If test mode return early</span><br><span class="line">                                                          </span><br><span class="line">        if mode == &#x27;test&#x27;:</span><br><span class="line">            return scores</span><br><span class="line"></span><br><span class="line">        loss, grads = 0.0, &#123;&#125;</span><br><span class="line">        ############################################################################</span><br><span class="line">        # TODO: Implement the backward pass for the fully-connected net. Store the #</span><br><span class="line">        # loss in the loss variable and gradients in the grads dictionary. Compute #</span><br><span class="line">        # data loss using softmax, and make sure that grads[k] holds the gradients #</span><br><span class="line">        # for self.params[k]. Don&#x27;t forget to add L2 regularization!               #</span><br><span class="line">        #                                                           </span><br><span class="line">        # When using batch normalization, you don&#x27;t need to regularize the scale   #</span><br><span class="line"></span><br><span class="line">        # NOTE: To ensure that your implementation matches ours and you pass the   #</span><br><span class="line">        # automated tests, make sure that your L2 regularization includes a factor #</span><br><span class="line">        # of 0.5 to simplify the expression for the gradient.                      #</span><br><span class="line">        ############################################################################</span><br><span class="line">        loss,dscores=softmax_loss(scores,y)</span><br><span class="line">        loss+=0.5*self.reg*np.sum(self.params[&#x27;W%d&#x27;%self.num_layers]*self.params[&#x27;W%d&#x27;%self.num_layers])</span><br><span class="line">        dx,dw,db=affine_backward(dscores,ar_cache[self.num_layers])</span><br><span class="line">        grads[&#x27;W%d&#x27;%self.num_layers]=dw+self.reg*self.params[&#x27;W%d&#x27;%self.num_layers]                                             </span><br><span class="line">        grads[&#x27;b%d&#x27;%self.num_layers]=db</span><br><span class="line">        for i in range(1,self.num_layers):</span><br><span class="line">            layer=self.num_layers-i</span><br><span class="line">            loss+=0.5*self.reg*np.sum(self.params[&#x27;W%d&#x27;%layer]**2)</span><br><span class="line">            dx,dw,db=affine_relu_backward(dx,ar_cache[layer])</span><br><span class="line">            grads[&#x27;W%d&#x27;%layer]=dw+self.reg*self.params[&#x27;W%d&#x27;%layer]</span><br><span class="line">            grads[&#x27;b%d&#x27;%layer]=db</span><br><span class="line">                                                        </span><br><span class="line">        pass</span><br><span class="line">        ############################################################################</span><br><span class="line">                                                            </span><br><span class="line">        #                             END OF YOUR CODE                             #</span><br><span class="line">        ############################################################################</span><br><span class="line"></span><br><span class="line">        return loss, grads</span><br></pre></td></tr></table></figure>

<h1 id="solver-py-选择优化的梯度方式等，建立再两层神经网络上"><a href="#solver-py-选择优化的梯度方式等，建立再两层神经网络上" class="headerlink" title="solver.py&#x2F;&#x2F;选择优化的梯度方式等，建立再两层神经网络上"></a>solver.py&#x2F;&#x2F;选择优化的梯度方式等，建立再两层神经网络上</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function, division</span><br><span class="line">from future import standard_library</span><br><span class="line">standard_library.install_aliases()</span><br><span class="line">from builtins import range</span><br><span class="line">from builtins import object</span><br><span class="line">import os</span><br><span class="line">import pickle as pickle</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from cs231n import optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solver(object):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A Solver encapsulates all the logic necessary for training classification</span><br><span class="line">    models. The Solver performs stochastic gradient descent using different</span><br><span class="line">    update rules defined in optim.py.</span><br><span class="line"></span><br><span class="line">    The solver accepts both training and validataion data and labels so it can</span><br><span class="line">    periodically check classification accuracy on both training and validation</span><br><span class="line">    data to watch out for overfitting.</span><br><span class="line"></span><br><span class="line">    To train a model, you will first construct a Solver instance, passing the</span><br><span class="line">    model, dataset, and various optoins (learning rate, batch size, etc) to the</span><br><span class="line">    constructor. You will then call the train() method to run the optimization</span><br><span class="line">    procedure and train the model.</span><br><span class="line"></span><br><span class="line">    After the train() method returns, model.params will contain the parameters</span><br><span class="line">    that performed best on the validation set over the course of training.</span><br><span class="line">    In addition, the instance variable solver.loss_history will contain a list</span><br><span class="line">    of all losses encountered during training and the instance variables</span><br><span class="line">    solver.train_acc_history and solver.val_acc_history will be lists of the</span><br><span class="line">    accuracies of the model on the training and validation set at each epoch.</span><br><span class="line"></span><br><span class="line">    Example usage might look something like this:</span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">      &#x27;X_train&#x27;: # training data</span><br><span class="line">      &#x27;y_train&#x27;: # training labels</span><br><span class="line">      &#x27;X_val&#x27;: # validation data</span><br><span class="line">      &#x27;y_val&#x27;: # validation labels</span><br><span class="line">    &#125;</span><br><span class="line">    model = MyAwesomeModel(hidden_size=100, reg=10)</span><br><span class="line">    solver = Solver(model, data,</span><br><span class="line">                    update_rule=&#x27;sgd&#x27;,</span><br><span class="line">                    optim_config=&#123;</span><br><span class="line">                      &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="line">                    &#125;,</span><br><span class="line">                    lr_decay=0.95,</span><br><span class="line">                    num_epochs=10, batch_size=100,</span><br><span class="line">                    print_every=100)</span><br><span class="line">    solver.train()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    A Solver works on a model object that must conform to the following API:</span><br><span class="line"></span><br><span class="line">    - model.params must be a dictionary mapping string parameter names to numpy</span><br><span class="line">      arrays containing parameter values.</span><br><span class="line"></span><br><span class="line">    - model.loss(X, y) must be a function that computes training-time loss and</span><br><span class="line">      gradients, and test-time classification scores, with the following inputs</span><br><span class="line">      and outputs:</span><br><span class="line"></span><br><span class="line">      Inputs:</span><br><span class="line">      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)</span><br><span class="line">      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the</span><br><span class="line">        label for X[i].</span><br><span class="line"></span><br><span class="line">      Returns:</span><br><span class="line">      If y is None, run a test-time forward pass and return:</span><br><span class="line">      - scores: Array of shape (N, C) giving classification scores for X where</span><br><span class="line">        scores[i, c] gives the score of class c for X[i].</span><br><span class="line"></span><br><span class="line">      If y is not None, run a training time forward and backward pass and</span><br><span class="line">      return a tuple of:</span><br><span class="line">      - loss: Scalar giving the loss</span><br><span class="line">      - grads: Dictionary with the same keys as self.params mapping parameter</span><br><span class="line">        names to gradients of the loss with respect to those parameters.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, model, data, **kwargs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Construct a new Solver instance.</span><br><span class="line"></span><br><span class="line">        Required arguments:</span><br><span class="line">        - model: A model object conforming to the API described above</span><br><span class="line">        - data: A dictionary of training and validation data containing:</span><br><span class="line">          &#x27;X_train&#x27;: Array, shape (N_train, d_1, ..., d_k) of training images</span><br><span class="line">          &#x27;X_val&#x27;: Array, shape (N_val, d_1, ..., d_k) of validation images</span><br><span class="line">          &#x27;y_train&#x27;: Array, shape (N_train,) of labels for training images</span><br><span class="line">          &#x27;y_val&#x27;: Array, shape (N_val,) of labels for validation images</span><br><span class="line"></span><br><span class="line">        Optional arguments:</span><br><span class="line">        - update_rule: A string giving the name of an update rule in optim.py.</span><br><span class="line">          Default is &#x27;sgd&#x27;.</span><br><span class="line">        - optim_config: A dictionary containing hyperparameters that will be</span><br><span class="line">          passed to the chosen update rule. Each update rule requires different</span><br><span class="line">          hyperparameters (see optim.py) but all update rules require a</span><br><span class="line">          &#x27;learning_rate&#x27; parameter so that should always be present.</span><br><span class="line">        - lr_decay: A scalar for learning rate decay; after each epoch the</span><br><span class="line">          learning rate is multiplied by this value.</span><br><span class="line">        - batch_size: Size of minibatches used to compute loss and gradient</span><br><span class="line">          during training.</span><br><span class="line">        - num_epochs: The number of epochs to run for during training.</span><br><span class="line">        - print_every: Integer; training losses will be printed every</span><br><span class="line">          print_every iterations.</span><br><span class="line">        - verbose: Boolean; if set to false then no output will be printed</span><br><span class="line">          during training.</span><br><span class="line">        - num_train_samples: Number of training samples used to check training</span><br><span class="line">          accuracy; default is 1000; set to None to use entire training set.</span><br><span class="line">        - num_val_samples: Number of validation samples to use to check val</span><br><span class="line">          accuracy; default is None, which uses the entire validation set.</span><br><span class="line">        - checkpoint_name: If not None, then save model checkpoints here every</span><br><span class="line">          epoch.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.model = model</span><br><span class="line">        self.X_train = data[&#x27;X_train&#x27;]</span><br><span class="line">        self.y_train = data[&#x27;y_train&#x27;]</span><br><span class="line">        self.X_val = data[&#x27;X_val&#x27;]</span><br><span class="line">        self.y_val = data[&#x27;y_val&#x27;]</span><br><span class="line"></span><br><span class="line">        # Unpack keyword arguments</span><br><span class="line">        self.update_rule = kwargs.pop(&#x27;update_rule&#x27;, &#x27;sgd&#x27;)</span><br><span class="line">        self.optim_config = kwargs.pop(&#x27;optim_config&#x27;, &#123;&#125;)</span><br><span class="line">        self.lr_decay = kwargs.pop(&#x27;lr_decay&#x27;, 1.0)</span><br><span class="line">        self.batch_size = kwargs.pop(&#x27;batch_size&#x27;, 100)</span><br><span class="line">        self.num_epochs = kwargs.pop(&#x27;num_epochs&#x27;, 10)</span><br><span class="line">        self.num_train_samples = kwargs.pop(&#x27;num_train_samples&#x27;, 1000)</span><br><span class="line">        self.num_val_samples = kwargs.pop(&#x27;num_val_samples&#x27;, None)</span><br><span class="line"></span><br><span class="line">        self.checkpoint_name = kwargs.pop(&#x27;checkpoint_name&#x27;, None)</span><br><span class="line">        self.print_every = kwargs.pop(&#x27;print_every&#x27;, 10)</span><br><span class="line">        self.verbose = kwargs.pop(&#x27;verbose&#x27;, True)</span><br><span class="line"></span><br><span class="line">        # Throw an error if there are extra keyword arguments</span><br><span class="line">        if len(kwargs) &gt; 0:</span><br><span class="line">            extra = &#x27;, &#x27;.join(&#x27;&quot;%s&quot;&#x27; % k for k in list(kwargs.keys()))</span><br><span class="line">            raise ValueError(&#x27;Unrecognized arguments %s&#x27; % extra)</span><br><span class="line"></span><br><span class="line">        # Make sure the update rule exists, then replace the string</span><br><span class="line">        # name with the actual function</span><br><span class="line">        if not hasattr(optim, self.update_rule):</span><br><span class="line">            raise ValueError(&#x27;Invalid update_rule &quot;%s&quot;&#x27; % self.update_rule)</span><br><span class="line">        self.update_rule = getattr(optim, self.update_rule)</span><br><span class="line"></span><br><span class="line">        self._reset()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def _reset(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Set up some book-keeping variables for optimization. Don&#x27;t call this</span><br><span class="line">        manually.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # Set up some variables for book-keeping</span><br><span class="line">        self.epoch = 0</span><br><span class="line">        self.best_val_acc = 0</span><br><span class="line">        self.best_params = &#123;&#125;</span><br><span class="line">        self.loss_history = []</span><br><span class="line">        self.train_acc_history = []</span><br><span class="line">        self.val_acc_history = []</span><br><span class="line"></span><br><span class="line">        # Make a deep copy of the optim_config for each parameter</span><br><span class="line">        self.optim_configs = &#123;&#125;</span><br><span class="line">        for p in self.model.params:</span><br><span class="line">            d = &#123;k: v for k, v in self.optim_config.items()&#125;</span><br><span class="line">            self.optim_configs[p] = d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def _step(self): //每次训练更新的方式</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Make a single gradient update. This is called by train() and should not</span><br><span class="line">        be called manually.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # Make a minibatch of training data</span><br><span class="line">        num_train = self.X_train.shape[0]</span><br><span class="line">        batch_mask = np.random.choice(num_train, self.batch_size)</span><br><span class="line">        X_batch = self.X_train[batch_mask]</span><br><span class="line">        y_batch = self.y_train[batch_mask]</span><br><span class="line"></span><br><span class="line">        # Compute loss and gradient</span><br><span class="line">        loss, grads = self.model.loss(X_batch, y_batch) //loss计算的规则</span><br><span class="line">        self.loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">        # Perform a parameter update</span><br><span class="line">        for p, w in self.model.params.items()://遍历字典 //model.params中存放的应该是w的值</span><br><span class="line">            dw = grads[p]</span><br><span class="line">            config = self.optim_configs[p]</span><br><span class="line">            next_w, next_config = self.update_rule(w, dw, config)//梯度下降的方式</span><br><span class="line">            self.model.params[p] = next_w</span><br><span class="line">            self.optim_configs[p] = next_config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def _save_checkpoint(self):</span><br><span class="line">        if self.checkpoint_name is None: return</span><br><span class="line">        checkpoint = &#123;</span><br><span class="line">          &#x27;model&#x27;: self.model,</span><br><span class="line">          &#x27;update_rule&#x27;: self.update_rule,</span><br><span class="line">          &#x27;lr_decay&#x27;: self.lr_decay,</span><br><span class="line">          &#x27;optim_config&#x27;: self.optim_config,</span><br><span class="line">          &#x27;batch_size&#x27;: self.batch_size,</span><br><span class="line">          &#x27;num_train_samples&#x27;: self.num_train_samples,</span><br><span class="line">          &#x27;num_val_samples&#x27;: self.num_val_samples,</span><br><span class="line">          &#x27;epoch&#x27;: self.epoch,</span><br><span class="line">          &#x27;loss_history&#x27;: self.loss_history,</span><br><span class="line">          &#x27;train_acc_history&#x27;: self.train_acc_history,</span><br><span class="line">          &#x27;val_acc_history&#x27;: self.val_acc_history,</span><br><span class="line">        &#125;</span><br><span class="line">        filename = &#x27;%s_epoch_%d.pkl&#x27; % (self.checkpoint_name, self.epoch)</span><br><span class="line">        if self.verbose:</span><br><span class="line">            print(&#x27;Saving checkpoint to &quot;%s&quot;&#x27; % filename)</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            pickle.dump(checkpoint, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def check_accuracy(self, X, y, num_samples=None, batch_size=100):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Check accuracy of the model on the provided data.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - X: Array of data, of shape (N, d_1, ..., d_k)</span><br><span class="line">        - y: Array of labels, of shape (N,)</span><br><span class="line">        - num_samples: If not None, subsample the data and only test the model</span><br><span class="line">          on num_samples datapoints.</span><br><span class="line">        - batch_size: Split X and y into batches of this size to avoid using</span><br><span class="line">          too much memory.</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">        - acc: Scalar giving the fraction of instances that were correctly</span><br><span class="line">          classified by the model.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # Maybe subsample the data</span><br><span class="line">        N = X.shape[0]</span><br><span class="line">        if num_samples is not None and N &gt; num_samples:</span><br><span class="line">            mask = np.random.choice(N, num_samples)</span><br><span class="line">            N = num_samples</span><br><span class="line">            X = X[mask]</span><br><span class="line">            y = y[mask]</span><br><span class="line"></span><br><span class="line">        # Compute predictions in batches</span><br><span class="line">        num_batches = N // batch_size</span><br><span class="line">        if N % batch_size != 0:</span><br><span class="line">            num_batches += 1</span><br><span class="line">        y_pred = []</span><br><span class="line">        for i in range(num_batches):</span><br><span class="line">            start = i * batch_size</span><br><span class="line">            end = (i + 1) * batch_size</span><br><span class="line">            scores = self.model.loss(X[start:end])</span><br><span class="line">            y_pred.append(np.argmax(scores, axis=1))</span><br><span class="line">        y_pred = np.hstack(y_pred)</span><br><span class="line">        acc = np.mean(y_pred == y)</span><br><span class="line"></span><br><span class="line">        return acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def train(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Run optimization to train the model.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        num_train = self.X_train.shape[0]</span><br><span class="line">        iterations_per_epoch = max(num_train // self.batch_size, 1)</span><br><span class="line">        num_iterations = self.num_epochs * iterations_per_epoch</span><br><span class="line"></span><br><span class="line">        for t in range(num_iterations):</span><br><span class="line">            self._step()</span><br><span class="line"></span><br><span class="line">            # Maybe print training loss</span><br><span class="line">            if self.verbose and t % self.print_every == 0:</span><br><span class="line">                print(&#x27;(Iteration %d / %d) loss: %f&#x27; % (</span><br><span class="line">                       t + 1, num_iterations, self.loss_history[-1]))</span><br><span class="line"></span><br><span class="line">            # At the end of every epoch, increment the epoch counter and decay</span><br><span class="line">            # the learning rate.</span><br><span class="line">            epoch_end = (t + 1) % iterations_per_epoch == 0</span><br><span class="line">            if epoch_end:</span><br><span class="line">                self.epoch += 1</span><br><span class="line">                for k in self.optim_configs:</span><br><span class="line">                    self.optim_configs[k][&#x27;learning_rate&#x27;] *= self.lr_decay</span><br><span class="line"></span><br><span class="line">            # Check train and val accuracy on the first iteration, the last</span><br><span class="line">            # iteration, and at the end of each epoch.</span><br><span class="line">            first_it = (t == 0)</span><br><span class="line">            last_it = (t == num_iterations - 1)</span><br><span class="line">            if first_it or last_it or epoch_end:</span><br><span class="line">                train_acc = self.check_accuracy(self.X_train, self.y_train,</span><br><span class="line">                    num_samples=self.num_train_samples)</span><br><span class="line">                val_acc = self.check_accuracy(self.X_val, self.y_val,</span><br><span class="line">                    num_samples=self.num_val_samples)</span><br><span class="line">                self.train_acc_history.append(train_acc)</span><br><span class="line">                self.val_acc_history.append(val_acc)</span><br><span class="line">                self._save_checkpoint()</span><br><span class="line"></span><br><span class="line">                if self.verbose:</span><br><span class="line">                    print(&#x27;(Epoch %d / %d) train acc: %f; val_acc: %f&#x27; % (</span><br><span class="line">                           self.epoch, self.num_epochs, train_acc, val_acc))</span><br><span class="line"></span><br><span class="line">                # Keep track of the best model</span><br><span class="line">                if val_acc &gt; self.best_val_acc:</span><br><span class="line">                    self.best_val_acc = val_acc</span><br><span class="line">                    self.best_params = &#123;&#125;</span><br><span class="line">                    for k, v in self.model.params.items():</span><br><span class="line">                        self.best_params[k] = v.copy()</span><br><span class="line"></span><br><span class="line">        # At the end of training swap the best params into the model</span><br><span class="line">        self.model.params = self.best_params</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="optim-py-梯度更新的方法"><a href="#optim-py-梯度更新的方法" class="headerlink" title="optim.py  梯度更新的方法"></a>optim.py  梯度更新的方法</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">This file implements various first-order update rules that are commonly used</span><br><span class="line">for training neural networks. Each update rule accepts current weights and the</span><br><span class="line">gradient of the loss with respect to those weights and produces the next set of</span><br><span class="line">weights. Each update rule has the same interface:</span><br><span class="line"></span><br><span class="line">def update(w, dw, config=None):</span><br><span class="line"></span><br><span class="line">Inputs:</span><br><span class="line">  - w: A numpy array giving the current weights.</span><br><span class="line">  - dw: A numpy array of the same shape as w giving the gradient of the</span><br><span class="line">    loss with respect to w.</span><br><span class="line">  - config: A dictionary containing hyperparameter values such as learning</span><br><span class="line">    rate, momentum, etc. If the update rule requires caching values over many</span><br><span class="line">    iterations, then config will also hold these cached values.</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">  - next_w: The next point after the update.</span><br><span class="line">  - config: The config dictionary to be passed to the next iteration of the</span><br><span class="line">    update rule.</span><br><span class="line"></span><br><span class="line">NOTE: For most update rules, the default learning rate will probably not</span><br><span class="line">perform well; however the default values of the other hyperparameters should</span><br><span class="line">work well for a variety of different problems.</span><br><span class="line"></span><br><span class="line">For efficiency, update rules may perform in-place updates, mutating w and</span><br><span class="line">setting next_w equal to w.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sgd(w, dw, config=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Performs vanilla stochastic gradient descent.</span><br><span class="line"></span><br><span class="line">    config format:</span><br><span class="line">    - learning_rate: Scalar learning rate.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if config is None: config = &#123;&#125;</span><br><span class="line">    config.setdefault(&#x27;learning_rate&#x27;, 1e-2)</span><br><span class="line"></span><br><span class="line">    w -= config[&#x27;learning_rate&#x27;] * dw</span><br><span class="line">    return w, config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sgd_momentum(w, dw, config=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Performs stochastic gradient descent with momentum.</span><br><span class="line"></span><br><span class="line">    config format:</span><br><span class="line">    - learning_rate: Scalar learning rate.</span><br><span class="line">    - momentum: Scalar between 0 and 1 giving the momentum value.</span><br><span class="line">      Setting momentum = 0 reduces to sgd.</span><br><span class="line">    - velocity: A numpy array of the same shape as w and dw used to store a</span><br><span class="line">      moving average of the gradients.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if config is None: config = &#123;&#125;</span><br><span class="line">    config.setdefault(&#x27;learning_rate&#x27;, 1e-2)</span><br><span class="line">    config.setdefault(&#x27;momentum&#x27;, 0.9)</span><br><span class="line">    v = config.get(&#x27;velocity&#x27;, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the momentum update formula. Store the updated value in #</span><br><span class="line">    # the next_w variable. You should also use and update the velocity v.     #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    </span><br><span class="line">    #这是Nesterov Momentum梯度下降方式</span><br><span class="line">#     old_v=v</span><br><span class="line">#     v=config[&#x27;momentum&#x27;]*v-config[&#x27;learning_rate&#x27;]*dw</span><br><span class="line">#     next_w=w-config[&#x27;momentum&#x27;]*old_v+(1+config[&#x27;momentum&#x27;])*v</span><br><span class="line">    v=config[&#x27;momentum&#x27;]*v-config[&#x27;learning_rate&#x27;]*dw</span><br><span class="line">    next_w=w+v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    config[&#x27;velocity&#x27;] = v</span><br><span class="line"></span><br><span class="line">    return next_w, config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def rmsprop(x, dx, config=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Uses the RMSProp update rule, which uses a moving average of squared</span><br><span class="line">    gradient values to set adaptive per-parameter learning rates.</span><br><span class="line"></span><br><span class="line">    config format:</span><br><span class="line">    - learning_rate: Scalar learning rate.</span><br><span class="line">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span><br><span class="line">      gradient cache.</span><br><span class="line">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span><br><span class="line">    - cache: Moving average of second moments of gradients.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if config is None: config = &#123;&#125;</span><br><span class="line">    config.setdefault(&#x27;learning_rate&#x27;, 1e-2)</span><br><span class="line">    config.setdefault(&#x27;decay_rate&#x27;, 0.99)</span><br><span class="line">    config.setdefault(&#x27;epsilon&#x27;, 1e-8)</span><br><span class="line">    config.setdefault(&#x27;cache&#x27;, np.zeros_like(x))</span><br><span class="line"></span><br><span class="line">    next_x = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the RMSprop update formula, storing the next value of x #</span><br><span class="line">    # in the next_x variable. Don&#x27;t forget to update cache value stored in    #</span><br><span class="line">    # config[&#x27;cache&#x27;].                                                        #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    config[&#x27;cache&#x27;]=config[&#x27;decay_rate&#x27;]*config[&#x27;cache&#x27;]+(1-config[&#x27;decay_rate&#x27;])*dx*dx</span><br><span class="line">    next_x=x-(config[&#x27;learning_rate&#x27;]*dx)/((np.sqrt(config[&#x27;cache&#x27;]))+config[&#x27;epsilon&#x27;])</span><br><span class="line">    </span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line"></span><br><span class="line">    return next_x, config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def adam(x, dx, config=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Uses the Adam update rule, which incorporates moving averages of both the</span><br><span class="line">    gradient and its square and a bias correction term.</span><br><span class="line"></span><br><span class="line">    config format:</span><br><span class="line">    - learning_rate: Scalar learning rate.</span><br><span class="line">    - beta1: Decay rate for moving average of first moment of gradient.</span><br><span class="line">    - beta2: Decay rate for moving average of second moment of gradient.</span><br><span class="line">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span><br><span class="line">    - m: Moving average of gradient.</span><br><span class="line">    - v: Moving average of squared gradient.</span><br><span class="line">    - t: Iteration number.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if config is None: config = &#123;&#125;</span><br><span class="line">    config.setdefault(&#x27;learning_rate&#x27;, 1e-3)</span><br><span class="line">    config.setdefault(&#x27;beta1&#x27;, 0.9)</span><br><span class="line">    config.setdefault(&#x27;beta2&#x27;, 0.999)</span><br><span class="line">    config.setdefault(&#x27;epsilon&#x27;, 1e-8)</span><br><span class="line">    config.setdefault(&#x27;m&#x27;, np.zeros_like(x))</span><br><span class="line">    config.setdefault(&#x27;v&#x27;, np.zeros_like(x))</span><br><span class="line">    config.setdefault(&#x27;t&#x27;, 1)</span><br><span class="line"></span><br><span class="line">    next_x = None</span><br><span class="line">    ###########################################################################</span><br><span class="line">    # TODO: Implement the Adam update formula, storing the next value of x in #</span><br><span class="line">    # the next_x variable. Don&#x27;t forget to update the m, v, and t variables   #</span><br><span class="line">    # stored in config.                                                       #</span><br><span class="line">    ###########################################################################</span><br><span class="line">    config[&#x27;m&#x27;]=config[&#x27;beta1&#x27;]*config[&#x27;m&#x27;]+(1-config[&#x27;beta1&#x27;])*dx</span><br><span class="line">    config[&#x27;v&#x27;]=config[&#x27;beta2&#x27;]*config[&#x27;v&#x27;]+(1-config[&#x27;beta2&#x27;])*dx*dx</span><br><span class="line">    first_unbias=config[&#x27;m&#x27;]/(1-config[&#x27;beta1&#x27;]**config[&#x27;t&#x27;])</span><br><span class="line">    second_unbias=config[&#x27;v&#x27;]/(1-config[&#x27;beta2&#x27;]**config[&#x27;t&#x27;])</span><br><span class="line">    next_x=x-config[&#x27;learning_rate&#x27;]*first_unbias/(np.sqrt(second_unbias)+config[&#x27;epsilon&#x27;])</span><br><span class="line">    config[&#x27;t&#x27;]+=1</span><br><span class="line">    pass</span><br><span class="line">    ###########################################################################</span><br><span class="line">    #                             END OF YOUR CODE                            #</span><br><span class="line">    ###########################################################################</span><br><span class="line"></span><br><span class="line">    return next_x, config</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2020/08/05/CS231n_Assignment2_Fully-Connected_Neural_Nets/" data-id="clkwhj3tx00086s7saasacrfh" class="article-share-link">Share</a>
      

      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/08/05/Windows%E4%B8%8B%E5%AE%89%E8%A3%85Linux%E7%B3%BB%E7%BB%9F%E5%B9%B6%E9%85%8D%E7%BD%AEJava%E7%8E%AF%E5%A2%83%E7%AD%89/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2020/08/04/Java-Stage3-Step2.2-Maven%E5%85%A5%E9%97%A8/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Tukeke<br>
      Theme <a href="https://github.com/Tukekehaohaonuli/" target="_blank">Oishi</a>, Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <!--
      <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/categories/life" class="mobile-nav-link">生活</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
</nav>
    -->
    

<!-- 百度分享 start -->

<!-- 百度分享 end -->

<script src="//cdn.bootcss.com/jquery/1.11.1/jquery.min.js"></script>




<script src="/js/jquery.scrollUp.min.js"></script>


<script src="/js/jquery.transform.js"></script>


<script src="/js/menu.js"></script>



<script src="/js/script.js"></script>


<script src="/js/scrollUp.js"></script>


  </div>
</body>
</html>